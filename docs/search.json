[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "In this blog, I plan to discuss the following topics:\n\nAnything and everything nucleotide recoding RNA-seq (NR-seq). Other metabolic labeling RNA-seq extensions are also potentially free game.\nThe use of statistics in bioinformatics. I developed a course on this topic and will use this setting to post some of my class material in blog format.\n\nI am also excited to use this as a platform to play around with the recently developed Quarto Live!"
  },
  {
    "objectID": "posts/Rintro_day1/index.html",
    "href": "posts/Rintro_day1/index.html",
    "title": "Intro to R: Day 1",
    "section": "",
    "text": "This worksheet will walk you through some basic concepts in R. I would suggest copying code shown here into an R script and running it yourself so that you can play around with the presented examples."
  },
  {
    "objectID": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-simple_calc",
    "href": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-simple_calc",
    "title": "Intro to R: Day 1",
    "section": "Pre-requisite knowledge for simple_calc()",
    "text": "Pre-requisite knowledge for simple_calc()\n\nMath in R\nThe simplest use case of R is using it to do math:\n\n1 + 1 # Addition\n\n[1] 2\n\n10 - 1.5 # Subtraction\n\n[1] 8.5\n\n2 * 3 # Multiplication\n\n[1] 6\n\n10.12 / 17.99 # Division\n\n[1] 0.5625347\n\n5 ^ 2 # Exponentiation\n\n[1] 25\n\n\n\n\nNumeric variables in R\nYou can store numbers in “variables”. This is like a special box in your computer’s memory labeled with a name (like my_number). When you put a number into this box (for example, 10), we say you have assigned the value 10 to the variable my_number.\nIn R, you’d do this by writing:\n\nmy_number &lt;- 10\nmy_number\n\n[1] 10\n\n\n\n\n\n\n\n\nSeeing the value of variables\n\n\n\n\n\nTyping and executing print(my_number) or just my_number will print out the value of the variable to your console.\n\n\n\nHere is what’s happening in this code:\n\nmy_number is the label on the box in memory.\n&lt;- is like an arrow pointing from the value to the box, meaning “put this value in that box”.\n`10 is the actual number you are storing.\n\nYou can then do math like this just like with regular numbers:\n\nmy_number * 2\n\n[1] 20\n\nmy_number ^ 3\n\n[1] 1000\n\nmy_number - 4\n\n[1] 6\n\n\nmy_number does not change value in any of the above lines. To change the value of my_number, you would have to assign it the new value with &lt;- again:\n\nmy_number # my_number is 10\n\n[1] 10\n\nmy_number &lt;- 1001 # my_number is now 1001\nmy_number # Confirm new value of my_number\n\n[1] 1001\n\n\n\n\nStrings in R\nYou can store more than numbers in variables. For example, you can store text, which is referred to as a “string”:\n\nmy_string &lt;- \"Hello\"\nmy_string2 &lt;- 'Bye'\n\nmy_string\n\n[1] \"Hello\"\n\nmy_string2\n\n[1] \"Bye\"\n\n\nYou tell R that you are storing text by wrapping that text in \"\" or ''.\n\n\n\n\n\n\nUseful string tools (aka functions)\n\n\n\n\n\nBelow are some useful tools that R provide you to work with strings. These are called functions, a concept discussed later.\n\npaste(..., sep = \" \"): paste() allows you to stitch together multiple strings, with a chosen separator text between strings (sep argument). Having no separator (sep = \"\") is identical to using a different function paste0():\n\n\nstring1 &lt;- \"Hello\"\nstring2 &lt;- \"friend.\"\nstring3 &lt;- \"It's been too long\"\n\npaste(string1, string2)\n\n[1] \"Hello friend.\"\n\npaste(string1, string2, sep = \"\")\n\n[1] \"Hellofriend.\"\n\npaste0(string1, string2)\n\n[1] \"Hellofriend.\"\n\npaste(string1, string2, string3)\n\n[1] \"Hello friend. It's been too long\"\n\npaste(string1, string2, collapse = \"_\")\n\n[1] \"Hello friend.\"\n\n\n\nnchar(): This will give you the number of individual characters in your text string:\n\n\nstring1 &lt;- \"Hello\"\nnchar(string1)\n\n[1] 5\n\n\n\ngsub(pattern, replacement, x): This allows you to look for the string pattern in the query string x, and replace it with the string replacement:\n\n\ntext &lt;- \"Hello, Hello, Hello!\"\ngsub(\"Hello\", \"Hi\", text)\n\n[1] \"Hi, Hi, Hi!\"\n\n\n\ngrepl(pattern, x): This is similar to gsub() but just searches for string pattern in string x and spits out TRUE if it finds it\n\n\ntext &lt;- \"Hello, Hello, Hello!\"\ngrepl(\"Hello\", text)\n\n[1] TRUE\n\n\nThere is a whole R package called stringr devoted to making working with strings in R easier and more intuitive, so you might want to look into that as well!\n\n\n\n\n\nBooleans in R\nAnother thing that is commonly stored in variables is logical values (TRUE or FALSE), otherwise known as “booleans”:\n\nmy_bool &lt;- TRUE\nmy_bool2 &lt;- FALSE\n\nmy_bool\n\n[1] TRUE\n\nmy_bool2\n\n[1] FALSE\n\n\nYou can do a sort of math with booleans, referred to as “boolean logic”. This takes as input two (in the case of AND and OR) or one (in the case of NOT) boolean variables and outputs a new boolean. The most common examples are:\nAND (&)\n\nBoth of the booleans must be TRUE for the output to be TRUE:\n\n\nTRUE & TRUE # This is TRUE\n\n[1] TRUE\n\nTRUE & FALSE # This is FALSE\n\n[1] FALSE\n\nFALSE & TRUE # This is FALSE\n\n[1] FALSE\n\nFALSE & FALSE # This is FALSE\n\n[1] FALSE\n\n\nOR (|)\n\nAt least one of the booleans must be TRUE for the output of this to be TRUE\n\n\nTRUE | TRUE # This is TRUE\n\n[1] TRUE\n\nTRUE | FALSE # This is TRUE\n\n[1] TRUE\n\nFALSE | TRUE # This is TRUE\n\n[1] TRUE\n\nFALSE | FALSE # This is FALSE\n\n[1] FALSE\n\n\nNOT (!)\n\nUnlike AND and OR, this takes a single boolean value as input\nThis reverses the value of the boolean:\n\n\n!TRUE # This is FALSE\n\n[1] FALSE\n\n!FALSE # This is TRUE\n\n[1] TRUE\n\n\nFinally, you can compare the value of two variables to see if they are the same. If the are variable_1 == variable_2 will return TRUE, otherwise it will return FALSE:\n\n\"Hello\" == \"Hello\" # TRUE\n\n[1] TRUE\n\n\"Hi\" == \"Bye\" # FALSE\n\n[1] FALSE\n\n1 == 1 # TRUE\n\n[1] TRUE\n\nmy_number &lt;- 1\nmy_number2 &lt;- 2\nmy_number == my_number2\n\n[1] FALSE\n\n\n\n\nFunctions in R\nA function in R is like a “recipe” for a mini “machine” that does one specific job. You give it some inputs (called arguments), it follows the steps you’ve defined, and then it gives you a result.\nFunctions help you organize your code so you can reuse it instead of writing the same steps again and again. Here is a simple example:\n\n# Function name: my_function\n# Arguments: x and y\n# Output: x + y\nmy_function &lt;- function(x, y){\n  \n  # 1. Inside the curly braces, write the steps of what you will do with x and y\n  \n  # We will add x and y\n  result &lt;- x + y\n  \n  # 2. Tell the function what to output (i.e., its \"return value\")\n  return(result)\n  \n}\n\n\nmy_function is the name of the function (like a label on the mini machine).\nfunction(x,y) { ... } says “I am creating a function that expects two inputs, called x and y.\nInside the { ... }, you can write as much code as you want; this is the instructions for what you want the function to do with the inputs\nreturn(result) sends the output of the function back to you.\n\nAfter creating my_function, you can call it (computer science lingo meaning “use the function”) by typing:\n\nmy_function(3,5)\n\n[1] 8\n\nmy_new_number &lt;- my_function(2, 2)\nmy_new_number\n\n[1] 4\n\n\nSometimes, you want one (or more) of your function’s inputs to have a “fallback” value if the user doesn’t supply one. That’s where default arguments come in. For example:\n\nmy_new_function &lt;- function(x, y = 10){\n\n  result &lt;- x + y\n  \n  return(result)\n  \n}\n\nmy_new_function now only needs you to supply x. You can supply x and y, but if you don’t supply y, it will give y a default value of 10 by default:\n\nmy_new_function(x = 1)\n\n[1] 11\n\nmy_new_function(x = 2, y = 20)\n\n[1] 22\n\nmy_new_function(2, 20) # Will fill arguments in order, so x = 2 and y = 20 here\n\n[1] 22\n\n\n\n\n\n\n\n\nAdvanced function advice\n\n\n\n\n\n\nTip 1: argument with small set of possible values\nSometimes, one of the arguments of your function may have a set number of possible values that you intend for a user to input. You can specify this as such:\n\nmy_options &lt;- function(a, b, greeting = c(\"Hi\", \"Bye\", \"Huh?\")){\n  \n  # Check to make sure the user supplied one of the valid options\n  greeting &lt;- match.arg(greeting)\n  \n  print(greeting)\n  \n  result &lt;- a + b\n  \n  return(result)\n  \n}\n\n# Uses first option by default\nmy_options(2, 2)\n\n[1] \"Hi\"\n\n\n[1] 4\n\nmy_options(2, 2, \"Huh?\")\n\n[1] \"Huh?\"\n\n\n[1] 4\n\n\n\n\nTip 2: Catching errors\nIn all of our examples so far, we have assumed that the user has supplied a particular kind of data for each argument. Mostly, we have assumed that numbers are being passed to many of our example functions, numbers that we can add. What if they messed up though and passed a string, for example? We can catch this and throw an error message:\n\nmy_valuecheck &lt;- function(a, b){\n  \n  # check if a is a number\n  stopifnot(is.numeric(a))\n  \n  # check if b is a number, but with a slightly different strategy\n  # if-statements are discussed more later.\n  if(!is.numeric(b)){\n    \n    stop(\"b isn't a number\")\n    \n  }\n  \n  result &lt;- a + b\n  \n  return(result)\n}\n\nThis function will work as normal if a and b are numbers, but will throw informative error messages if not. You will also get an error in the old version of this function that didn’t have the stopifnot() lines, but this error might be far more cryptic and hard to understand. You will also get a different error depending on what is wrong about a and/or b, further confusing you or other users of this function.\n\n\n\n\n\n\nControl flow (if-else statements)\nAn if-else statement is one of the most common ways to control the flow of a program. It lets your code make decisions based on whether a condition is TRUE or FALSE.\n\nif checks if something is TRUE\nelse covers what happens if it is not TRUE\nYou can add extra steps in between using else if to handle different possible conditions\n\nThe basic structure looks like:\n\nif (condition1){\n  # This code runs if 'condition1' is TRUE\n}else if(condition2){\n  # This code runs if 'condition2' is TRUE\n}else{\n  # This code runs if both 'condition1' and 'condition2' are FALSE\n}\n\nThink of this code as asking a set of questions:\n\nIf condition1 is TRUE, do something.\nElse if condition2 is TRUE, do something else\nElse, if neither condition1 nor condition2 are TRUE, do a default thing.\n\nA real example might look like:\n\nx &lt;- 5\n\nif(x &gt; 3){\n  print(\"x is greater than 3\")\n}else if(x &lt; 5){\n  print(\"x is between 3 and 5\")\n}else{\n  print(\"x is greater than or equal to 5\")\n}\n\n[1] \"x is greater than 3\"\n\n\nConditions in R must evaluate to a single TRUE or FALSE. Common ways to form conditions are comparison operators:\n\n==: Check if two things are equal (e.g., a == b). a and b can be numbers, strings, booleans, etc.\n!=: Check if two things are not equal 1 &lt;, &gt;, &lt;=, &gt;=: Less than, greater than, less than or equal to, or greater than or equal to, respectively.\n\nHere is an example of how you might use control flow in a function:\n\ngreetUser &lt;- function(user_input){\n  \n  # Check if user_input equals \"Hello\"\n  if (user_input == \"Hello\"){\n    return(\"Hi there! Nice to meet you.\")\n  }else if(user_input == \"Goodbye\"){\n    return(\"See you later! Take care.\")\n  }else{\n    return(\"I'm not sure how to respond to that...\")\n  }\n  \n}\n\ngreetUser(\"Hello\")\n\n[1] \"Hi there! Nice to meet you.\"\n\ngreetUser(\"Comment allez-vous?\")\n\n[1] \"I'm not sure how to respond to that...\""
  },
  {
    "objectID": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-vector_calc",
    "href": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-vector_calc",
    "title": "Intro to R: Day 1",
    "section": "Pre-requisite knowledge for vector_calc()",
    "text": "Pre-requisite knowledge for vector_calc()\n\nVectors\nIn R, a vector is a container that holds multiple values of the same data type (such as numbers, strings, or booleans). You can think of it like a row of boxes, each containing a value of the same kind.\nYou can create a vector with the c() function (short for “combine” or “concatenate”). Here are a few example:\n\n# A numeric vector\nnumbers &lt;- c(10, 20, 30, 40)\n\n# A character (string) vector\nwords &lt;- c(\"cat\", \"dog\", \"bird\")\n\n# A boolean vector\nbools &lt;- c(TRUE, FALSE, TRUE)\n\nOften, you will want to access specific elements or sets of elements of a vector. To do this, you can use square brackets [ ]:\n\n# Get the first element of 'numbers'\nnumbers[1] # 10\n\n[1] 10\n\n# Get the second element of 'words'\nwords[2]\n\n[1] \"dog\"\n\n# Get multiple elements at once\nnumbers[c(1, 3)] # This gives the 1st and 3rd elements: c(10, 30)\n\n[1] 10 30\n\n# Exclude specific elements\nbools[-1] # This gives everything but the 1st element\n\n[1] FALSE  TRUE\n\n\nYou can also change values of specific elements:\n\n# See what 'numbers' is now\nnumbers\n\n[1] 10 20 30 40\n\n# Change a value\nnumbers[2] &lt;- 99\n\n# Check 'numbers' now\nnumbers\n\n[1] 10 99 30 40\n\n\nSometimes, it will be useful to check what kind of data is in a vector. This can be done with the class() function:\n\nclass(numbers) # \"numeric\"\n\n[1] \"numeric\"\n\nclass(words) # \"character\"\n\n[1] \"character\"\n\nclass(bools) # \"logical\" (another word for boolean)\n\n[1] \"logical\"\n\n\nYou can also check the value with functions like is.numeric(). is.character(), or is.logical():\n\nis.numeric(numbers) # TRUE\n\n[1] TRUE\n\nis.numeric(words) # FALSE\n\n[1] FALSE\n\nis.character(numbers) # FALSE\n\n[1] FALSE\n\nis.logical(numbers) # FALSE\n\n[1] FALSE\n\nis.character(words) # TRUE\n\n[1] TRUE\n\nis.logical(bools) # TRUE\n\n[1] TRUE\n\n\n\n\n\n\n\n\nUseful vector functions\n\n\n\n\n\nBelow are some useful functions that allow you to create vectors or lookup some information about a vector:\n\nlength(v): returns the number of elements in the vector v:\n\n\nlength(c(1, 2, 3))\n\n[1] 3\n\n\n\nseq(from, to, length.out) or seq(from, to, by): Creates a vector starting from the number from (default value of 1), to the number to (default value of 1). If you set length.out, then you will get a vector of length.out elements. If you set by, then you specify the distance between adjacent elements:\n\n\nseq(from = 1, to = 5, length.out = 5)\n\n[1] 1 2 3 4 5\n\nseq(from = 1, to = 5, by = 1)\n\n[1] 1 2 3 4 5\n\n\n\nrep(x, times): Creates a vector containing the value x repeated times times:\n\n\nrep(x = 1, times = 10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\n\nstart:end: Same as seq(from = start, to = end, by = 1):\n\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n0.5:2.5\n\n[1] 0.5 1.5 2.5\n\n\n\n\n\n\n\nLoops\nA loop is a way to tell R to “do something multiple times”. This unlocks one of the powerful aspects of computers: their ability to do multiple things quickly.\nThere are two commonly used types of loops: for loops and while loops.\nA for loop in R iterates (or “loops”) over each element of a vector and does something with it. For example, if we want to print every element of a numeric vector:\n\nnumbers &lt;- c(10, 20, 30, 40)\n\n# Loop over the values\nfor(value in numbers){\n  print(value)\n}\n\n[1] 10\n[1] 20\n[1] 30\n[1] 40\n\n# Loop over the vector 1 to the length of the vector\nfor(i in 1:length(numbers)){\n  \n  print(numbers[i])\n \n}\n\n[1] 10\n[1] 20\n[1] 30\n[1] 40\n\n# Fancier alternative to the above code\nfor(i in seq_along(numbers)){\n  \n  print(numbers[i])\n  \n}\n\n[1] 10\n[1] 20\n[1] 30\n[1] 40\n\n\nWhat’s happening here?\n\nfor (value in numbers) means “go through each element of numbers and temporarily call that element value. for(i in 1:length(numbers) creates a vector (1:length(numbers)) which is a vector of whole numbers from 1 to the length of the vector numbers. Each of these whole numbers is then temporarily called i. seq_along(numbers) does pretty much the same things as 1:length(numbers).\nprint(value) means we display the current value on the screen.\nR will do this until it has gone through all elements in numbers.\n\nA while loop keeps going as long as some condition is TRUE. Suppose we want to keep adding numbers from a vector until the total sum exceeds 50:\n\nnumbers &lt;- c(10, 20, 30, 40, 50)\ntotal &lt;- 0 # Start total at 0\ni &lt;- 1 # Start index at 1\n\nwhile(i &lt;= length(numbers) & total &lt;= 50){\n  \n  # Add to total\n  total &lt;- total + numbers[i]\n  \n  # Track which element we are on\n  i &lt;- i + 1\n  \n}\n\nprint(total)\n\n[1] 60\n\nprint(i)\n\n[1] 4\n\n\nWhat’s going on here?\n\nwhile(i &lt;= length(numbers) && total &lt;= 50) - The loop will continue running while two conditions are both TRUE:\n\n\nWe haven’t reached the end of the vector (i &lt;= length(numbers)) and\nThe total hasn’t exceeded 50 (total &lt;= 50).\n\n\nInside the loop, we add the i-th element of numbers to total.\nWe then move i to the next element by adding 1.\nAs soon as one of the conditions in 1. become FALSE, the loop stops."
  },
  {
    "objectID": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-calc_df_stats",
    "href": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-calc_df_stats",
    "title": "Intro to R: Day 1",
    "section": "Pre-requisite knowledge for calc_df_stats()",
    "text": "Pre-requisite knowledge for calc_df_stats()\n\nNavigating directories and file paths\nWhen you work in R, you’ll often deal with files (like CSV files) that sit in folders (directories) on your computer. To load these files into R so that you can work with and analyze them, you need to tell R where they are. In addition, it is important to know where you are while working in R.\nWhen I say “know where you are”, I am referring to your “working directory”. When you open up Rstudio, there is some folder on your computer that R will call your “working directory”. You can see what this folder is at any time with getwd():\n\n# You get to see what my working directory path is\ngetwd()\n\n[1] \"C:/Users/isaac/Documents/Simon_Lab/isaacvock.github.io/posts/Rintro_day1\"\n\n\nIf you want to change this directory, you can switch to a new directory with setwd(\"/path/to/new/working/directory\").\nYou can specify a file path to setwd() in one of two ways:\n\nAs an absolute path. This means specifying exactly where a folder is on your computer (like “C:/Users/YourName/Documents/” on Windows or “/Users/YourName/Documents/ on Macs).\nAs a relative path. This means telling R how to get from the current working directory to the folder. For example, if you are in the directory “Documents” and want to move to a folder called “data” inside of “Documents, you could run setwd(\"data\"), assuming”Documents” is your current working directory.\n\n\n\nReading a file with readr\nThe readr package (part of the tidyverse collection of packages), provides user-friendly functions for reading in data. For example, you can read a csv file like so:\n\nmy_data &lt;- read_csv(\"path/to/mydata.csv\")\n\n\nread_csv(\"path/to/mydata.csv\") reads the CSV file located at the specified path (either a relative or absolute path) and creates a data frame (more on those soon).\nWe’re storing that data frame in a variable called my_data.\n\n\n\nData Frames\nA data frame is a table-like structure with rows and columns, commonly used for storign datasets in R. Each column is usually a vector of a particular type (numeric, character, boolean, etc.), and all columns have the same length.\nTo create a data frame you can run code like this:\n\nages &lt;- c(30, 25, 35)\n\n# You can either specify the vector directly\n# or provide the name of a vector you previously created\npeople_df &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age = ages,\n  Score = c(100, 95, 90)\n)\n\npeople_df\n\n     Name Age Score\n1   Alice  30   100\n2     Bob  25    95\n3 Charlie  35    90\n\n\nHere are some ways you can interact with the data inside of a data frame:\n\nYou can grab an entire column with $ or [[&lt;col name as string&gt;]]:\n\n\n# This will give you a vector\npeople_df$Name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n# This will be the same vector\npeople_df[[\"Name\"]]\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n\n\nYou can grab an entire column with [ , &lt;column number&gt;]:\n\n\n# This will give you a data frame with one column\npeople_df[, 1]\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n# This will give you a data frame with multiple columns\npeople_df[,c(1, 2)]\n\n     Name Age\n1   Alice  30\n2     Bob  25\n3 Charlie  35\n\n\n\nYou can get all columns in a specific row with [&lt;row number&gt;, ]:\n\n\n# This will give you a data frame with one row\npeople_df[1,]\n\n   Name Age Score\n1 Alice  30   100\n\n# This will give you a data frame with multiple rows\npeople_df[c(1, 3), ]\n\n     Name Age Score\n1   Alice  30   100\n3 Charlie  35    90\n\n\n\n\nLists\nA list is like a container in R that can hold a mix of different types of items, like a data frame. Lists are more flexible though, and can hold things of different sizes. A list can hold:\n\nA numeric vector\nA string vector\nA single number\nAn entire data frame\nAnother list\n\nAll at once!\nHere is how you can create a list:\n\nmy_list &lt;- list(\n  name = \"Alice\",\n  age = 30,\n  scores = c(100, 95, 90),\n  is_student = FALSE,\n  df = data.frame(a = c(1, 2, 3), b = c(\"a\", \"b\", \"c\"))\n)\n\nTo access elements of a list, you can:\n\nUse the $ operator (if the elements have names):\n\n\nmy_list$name\n\n[1] \"Alice\"\n\nmy_list$scores\n\n[1] 100  95  90\n\n\n\nThe [[ ]] operator with the element’s name (if it has one), or its position:\n\n\nmy_list[[\"name\"]]\n\n[1] \"Alice\"\n\nmy_list[[1]]\n\n[1] \"Alice\"\n\n\n\n\n\n\n\n\nFancy looping over lists (and data frames)\n\n\n\n\n\nOften, you will want to go element by element of a list and do something with each element. In addition, data frame columns are equivalent to elements of a list (actually, under the hood, a data frame is just a list that forces the list elements to be the same size). You could write a for loop, but there are popular alternatives that can make your code cleaner and easier to read. R has a version of these, but the R package purrr has improved versions of these that I prefer.\n\nmap(): takes a single list as input\n\n\nlibrary(purrr)\n\nnumbers &lt;- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(10, 20, 30, 40, 50)\n)\n\n# Outputs a list, one element for original element of the list\nmap(numbers, function(x) sum(x))\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 150\n\n# Outputs a vector numbers, one element per original list element\n# Also using an alternative notation\nmap_dbl(numbers, ~ sum(.x))\n\n[1]   6  15 150\n\n\n\nmap2(): takes two lists as input\n\n\nnumbers &lt;- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(10, 20, 30, 40, 50)\n)\n\nnumbers2 &lt;- list(\n  c(-1, -2, -3),\n  c(12, 13),\n  c(2, 4, 6, 8, 10, 12)\n)\n\n# Outputs a list, one element for original element of the list\nmap2(numbers, numbers2, ~ sum(.x) + sum(.y))\n\n[[1]]\n[1] 0\n\n[[2]]\n[1] 40\n\n[[3]]\n[1] 192\n\n# Outputs a vector numbers, one element per original list element\nmap2_dbl(numbers, numbers2, function(x, y) sum(x) + sum(y))\n\n[1]   0  40 192\n\n\n\npmap() allows you to provide a named list of inputs:\n\n\n# A list of vectors\nlists_of_inputs &lt;- list(\n  a = c(1, 3, 5),\n  b = c(2, 4, 6),\n  c = c(10, 20, 30)\n)\n\npmap(lists_of_inputs, function(a, b, c) a + b + c)\n\n[[1]]\n[1] 13\n\n[[2]]\n[1] 27\n\n[[3]]\n[1] 41\n\npmap_dbl(lists_of_inputs, function(a, b, c) a + b + c)\n\n[1] 13 27 41"
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html",
    "href": "posts/nrseq_deepdive/index.html",
    "title": "NR-seq: an in-depth review",
    "section": "",
    "text": "This is a reproduction of Chapter 1 of my thesis, an in-depth technical review of NR-seq data. It is intended to be a living document that evolves as the field evolves, and will thus be occasionally updated (edit time scale: ~ monthly)."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#nr-seq-a-review",
    "href": "posts/nrseq_deepdive/index.html#nr-seq-a-review",
    "title": "NR-seq: an in-depth review",
    "section": "",
    "text": "This is a reproduction of Chapter 1 of my thesis, an in-depth technical review of NR-seq data. It is intended to be a living document that evolves as the field evolves, and will thus be occasionally updated (edit time scale: ~ monthly)."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#abstract",
    "href": "posts/nrseq_deepdive/index.html#abstract",
    "title": "NR-seq: an in-depth review",
    "section": "Abstract",
    "text": "Abstract\nThe kinetics of gene expression are largely invisible to methods that only probe steady-state RNA abundances. Nucleotide recoding RNA-seq methods (TimeLapse-seq, SLAM-seq, TUC-seq, etc.) were developed to overcome this limitation. These methods combine metabolic labeling with unique chemistries to track the dynamics of labeled and unlabeled RNA and resolve the kinetic ambiguities of vanilla RNA-seq. Despite their promise, analyzing NR-seq data presents several unique bioinformatic challenges. While software packages exist that implement gold-standard analysis strategies, misconceptions about how to properly analyze and interpret NR-seq data persist. In some cases, this has led to the widespread adoption of potentially flawed analysis paradigms. To address this, I present a detailed overview of NR-seq analyses. I cover best practices, current software implementations, and optimal experimental design. I also discuss the landscape of NR-seq extensions, as these represent exciting areas with unique bioinformatic challenges. I hope that this will be a useful resource to the community of NR-seq users and developers."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#introduction",
    "href": "posts/nrseq_deepdive/index.html#introduction",
    "title": "NR-seq: an in-depth review",
    "section": "Introduction",
    "text": "Introduction\nFrom birth to death, an RNA’s life cycle is tightly regulated. A key aspect of this regulation is fine-tuning of the rate at which each stage progresses, from transcription initiation to RNA degradation. Developing a mechanistic understanding of gene expression regulation requires methods to probe the kinetics of the RNA life cycle (transcription, processing, export, degradation, etc.) (Weake and Workman 2010).\nWhile standard RNA-seq begins to solve this problem, it provides limited information about the kinetics of the processes which determine an RNA’s abundance. Nucleotide recoding RNA-seq (NR-seq; TimeLapse-seq (Schofield et al. 2018), SLAM-seq (Herzog et al. 2017), TUC-seq (Riml et al. 2017), etc.) overcomes these limitations. NR-seq relies on metabolic labeling, which involves feeding cells a nucleotide analog that gets incorporated into RNA synthesized after the start of labeling. The rate at which old, unlabeled RNA degrades and new, labeled RNA accumulates provides information about the kinetics of RNA metabolism (Figure 1). To determine how much of a given population of RNA is labeled, NR-seq relies on novel chemistries that recode the hydrogen bond pattern of the metabolic label to facilitate detection of labeled RNA via chemically induced mutations in sequencing reads (Figure 1). NR-seq is thus a powerful method for resolving the kinetic ambiguities of standard RNA-seq.\n\n\n\n\n\n\nFigure 1: Overview of a standard NR-seq analysis, adapted from (Vock and Simon 2023). NR-seq typically involves labeling cells with s4U and chemically recoding s4U to a cytosine. The data provided by this method is counts of sequencing reads containing a certain number of mutations and a certain number of mutable nucleotides. This data can be used to infer the abundance of labeled and unlabeled RNA, which can be modeled as a function of kinetic parameters (e.g., synthesis and degradation rate constants). An important quantity in this task is the fraction of reads from new RNA, or new-to-total ratio (NTR; denoted θ throughout this chapter) whose estimation is a key part of any NR-seq analysis.\n\n\n\nUnlocking the full potential of NR-seq data requires rigorous and well-founded analysis strategies. While such strategies have been put forth and implemented in a number of bioinformatic tools (Jürges, Dölken, and Erhard 2018; McShane et al. 2024; Vock et al. 2025; Müller et al. 2025), misunderstanding regarding how to best interpret and analyze NR-seq data are common. Therefore, it is crucial to create gold-standard analysis guidelines for users and developers of NR-seq methods.\nTowards that end, I present a comprehensive overview of the analysis of NR-seq data. I will provide a combination of accessible big-picture summaries of my main points, as well as rigorous mathematical formalism to back up key assertions. I also will conclude with a brief overview of existing extensions of the original NR-seq methodology, their applications, and unique aspects of their analyses. Throughout, I will point readers to bioinformatic tools implementing gold-standard analysis strategies while also highlighting fundamental challenges posed by NR-seq analyses. I hope that this serves as a useful resource for the larger community of NR-seq users and promotes best analysis practices in this exciting and growing field."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#summary",
    "href": "posts/nrseq_deepdive/index.html#summary",
    "title": "NR-seq: an in-depth review",
    "section": "Summary",
    "text": "Summary\nBelow I will briefly summarize the main takeaways that are expanded upon throughout the remaining of this post. There are four unique challenges when analyzing NR-seq data: 1) estimating how many reads come from labeled RNA; 2) inferring kinetic parameters from 1); 3) aligning and processing NR-seq reads; 4) optimizing the experimental design for achieving 1) and 2). I thus divide my advice into that pertaining to each of these points:\nInferring the number of reads that come from labeled RNA:\n\nMixture modeling is the most robust and accurate strategy by which to assess the fraction of reads from a given mutational population. This strategy is implemented in tools such as GRAND-SLAM, bakR, Halfpipe, and EZbakR.\nMutation content cutoffs (e.g., classifying reads as “labeled” if they have a certain number of T-to-C mutations) can yield highly biased estimates of labeled RNA abundance.\nMixture models necessarily make assumptions about the distribution of mutations in reads from new and old RNA. It is important to assess these assumptions when interpreting mixture model fits.\nExtending the simplest two-component mixture model is promising but potentially fraught. Carefully validate improvements in model fit and stability of parameter estimates. Consider using regularization strategies to avoid unrealistic parameter estimates.\n\nInferring kinetic parameters:\n\nUnless using a label time significantly shorter than the average half-life of RNAs of interest (for example, this is around 4 hours for mRNAs in mice and humans), the number of reads from labeled RNA is reflective of both transcription and turnover kinetics, not just the former.\nThe relative abundances of reads from labeled and unlabeled RNA is best modeled as a function of both transcription and degradation rate constants. The functional form of this relationship depends on your model for the dynamics of the RNA species you are probing.\nAssuming that the probed RNA populations are at steady-state during labeling simplifies the task of kinetic parameter estimation. This assumption can break down when labeling is done during or following a perturbation. Analysis strategies exist which relax this assumption, but they rely on having measurements of RNA abundances at the start of labeling (i.e., no-label RNA-seq data from that time point or a labeled NR-seq data in which RNA was extracted at that point). grandR and EZbakR implement both of these analysis strategies.\nCombining NR-seq with subcellular fractionation is a powerful way by which to explore the kinetics of processes invisible to whole-cell NR-seq. Analyzing this data requires a strategy to normalize read counts and integrate information across all compartments. EZbakR implements such strategies.\n\nProcessing NR-seq data:\n\nAligning NR-seq reads is difficult due to the chemically induced T-to-C mismatches. While 3-base genome alignment strategies, popular in analyses of bisulfite sequencing data, are a potential solution, they often provide only minimal advantages over standard 4-base alignment approaches while also suffering from their own unique biases and limitations. This is in large part due to the fact that in NR-seq reads, most Ts are not converted to Cs. Thus, the downsides of aligning to a lower complexity genome may often nullify the benefits of not penalizing T-to-C mismatches.\nSpecialized alignment approaches may provide an improvement over either a standard 3- or 4-base genome alignment approach. grandRescue is a recently developed approach that implements 4-base alignment followed by 3-base alignment of reads that fail to align via the 4-base strategy. This approach can help recover high mutation content sequencing reads.\nWhen processing NR-seq data, you have a choice to make about what genomic features you want to assign reads to and perform analyses on. Most tools support performing analyses at the gene- (or 3’-UTR if using 3’-end sequencing) level, but fastq2EZbakR significantly expands the set of features you can analyze to include transcript isoforms, exon-exon junctions, etc.\n\nExperimental design:\n\nLabel-free control samples (a.k.a. no-label controls) are crucial to detect potential biases introduced by labeling (e.g., dropout of labeled RNA).\nNo-label controls can be used to both assess and correct for these biases in some cases. grandR and EZbakR provide strategies for this task.\nBias correction strategies make assumptions like any statistical method, and these assumptions should be assessed when interpreting the output of bias correction.\nWhile unique analysis strategies have been proposed that may be strictly compatible with pulse-chases, pulse-chases suffer from the following serious shortcomings:\n\nProlonged exposure to metabolic label, which can lead to adverse effects\nIncreased variance in kinetic parameter estimation due to having to compare the estimated fraction of reads that are labeled at the end of the pulse to that at the end of the chase. Compare this with a steady-state pulse-label analysis where the only source of variance is that of the fraction labeled estimate for the pulse.\nThe analysis is complicated by the potential for incomplete competition of metabolic label with the chase nucleotide and recycling of metabolic label from degraded RNA.\nHigher cost due to the necessity of more samples (set of pulses and set of chases).\n\nThe best label time for standard NR-seq kinetic parameter inference is around the median half-life of the RNAs you wish to probe the kinetics of. It’s better to undershoot than overshoot this target though to avoid adverse effects of prolonged metabolic labeling.\nAccurate kinetic parameter estimates are obtainable with standard RNA-seq sequencing depth. More depth can significantly improve these analyses though. Sequencing depth is particularly important if wanting to perform analyses on the sub-gene (e.g., exon-exon junction) level.\n\nAdditionally, an exciting advance in the NR-seq field is the number of unique extensions that have been developed to apply nucleotide recoding to the study of several aspects of RNA dynamics. This review concludes with a summary of currently published NR-seq extensions."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#a-brief-history-of-nr-seq",
    "href": "posts/nrseq_deepdive/index.html#a-brief-history-of-nr-seq",
    "title": "NR-seq: an in-depth review",
    "section": "A brief history of NR-seq",
    "text": "A brief history of NR-seq\nOne of the first strategies to assess the kinetics of RNA synthesis and degradation combined global transcriptional inhibition with RNA-seq (Lam et al. 2001; Raghavan and Bohjanen 2004; Spies, Burge, and Bartel 2013). RNA levels following a time course of inhibition could be fit to exponential decay curves to assess turnover kinetics in high-throughput. Combining this with pre-inhibition abundance information allows one to assess synthesis kinetics. While this approach continues to be widely used, it suffers from a number of drawbacks. For one, rigorous normalization is needed to track the decreasing absolute levels of RNA. As RNA-seq only provides a relative measure of RNA abundance, this typically requires exogenous spike-ins to account for the global differences in RNA abundance between inhibition timepoints (Risso et al. 2014; K. Chen et al. 2016). This introduces additional experimental complexity that requires optimization. In addition, global transcription inhibition causes the cells to launch myriad stress responses, many of which affect transcript stability (Bensaude 2011; Sun et al. 2012). This represents a confounder that complicates the interpretation of transcriptional inhibition data. Thus, a strategy to probe the kinetics of RNA without significant perturbation of the system was needed.\nMetabolic labeling with nucleotide analogs offers one such strategy (Dölken et al. 2008; Jao and Salic 2008; Rabani et al. 2011; Tani and Akimitsu 2012). Cells will incorporate these labels into nascent RNA, leading to the existence of two distinct populations of RNA: unlabeled, old RNA that existed at the start of labeling and new RNA that had the potential to incorporate metabolic label. Tracking the dynamics of these two populations yields the information necessary to dissect transcription and degradation kinetics of each RNA in a population of cells. Originally, doing so required biochemically separating the two populations and sequencing each (or more specifically enriching for one and sequencing the enriched and input samples). This either relied on chemistries that conjugated biotin to incorporated labels so that labeled RNA could be separated from unlabeled via a biotin-streptavidin pull down (Cleary et al. 2005; Jao and Salic 2008), or immunoprecipitation via antibodies that specifically recognize the metabolic label (Tani and Akimitsu 2012). While powerful, these enrichment-based techniques require substantial amounts of starting RNA, introduce biochemical biases during enrichment, and cannot distinguish the desired enriched RNAs from nontrivial levels of contamination (Duffy et al. 2015; Wada and Becskei 2017). Labeled and unlabeled spike-ins have been proposed to quantify and account for some of these challenges, but these introduce their own experimental challenges and are unable to account for length biases in the enrichment (Wachutka et al. 2019). Further innovation was thus required to improve the robustness of metabolic labeling strategies.\nSeveral labs (including the Simon lab, where I did my PhD) addressed these shortcomings by developing nucleotide recoding RNA-seq methods (NR-seq). These techniques combine s4U metabolic labeling and nucleotide recoding chemistry (TimeLapse (Schofield et al. 2018), SLAM (Herzog et al. 2017), TUC (Riml et al. 2017), etc.) to either convert or disrupt the hydrogen bonding pattern of incorporated s4U. This yields apparent T-to-C mutations in the RNA-seq data that indicate sites of s4U incorporation and can be used to estimate the fraction of extracted RNA that was synthesized after the introduction of metabolic label. This adds kinetic information to the snapshot provided by RNA-seq while eliminating the need for enrichment of labeled RNA. A simplified schematic of NR-seq data and its analysis is presented in Figure 1. T-to-C mutations in sequencing reads can be used to bioinformatically quantify the levels of labeled and unlabeled RNA, with the Simon lab originally introducing the now gold-standard mixture modeling approach for this task (Schofield et al. 2018). Simple kinetic models of the trajectories of these two species relate the kinetic parameters of interest to the data obtained. This is how NR-seq can quantify the kinetics of gene expression."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#analyzing-nr-seq-data",
    "href": "posts/nrseq_deepdive/index.html#analyzing-nr-seq-data",
    "title": "NR-seq: an in-depth review",
    "section": "Analyzing NR-seq data",
    "text": "Analyzing NR-seq data\nHere I discuss how to analyze processed NR-seq data before discussing the details of NR-seq data processing. This will allow me to better motivate the importance of optimally processed NR-seq data. This means I will assume that you have information about how many instances of a given read-vs-reference mismatch type (e.g., T-to-C mismatches) were in each read, and how many mutable nucleotides were in the reference sequence to which this read aligned (e.g., number of reference T’s). Throughout these and later sections, my discussion will assume a pulse-label (vs. a pule-chase) design was utilized, as the optimality of this approach will be discussed later.\n\nModeling the mutational content of sequencing reads\nIn NR-seq data, two populations of sequencing reads exist: those originating from RNA synthesized prior to metabolic labeling (the unlabeled RNA; old RNA in a pulse-label design), and those originating from RNA synthesized during metabolic labeling (the labeled RNA; new RNA in a pulse-label design). On average, the mutational content of the latter will be higher than that of the former. Analyzing NR-seq data requires making use of this fact to infer the fraction of reads coming from each of these two populations for a given read.\nOriginally, two ideas were proposed. The simplest was to use a mutation content cutoff to classify reads as coming from labeled or unlabeled RNA (Neumann et al. 2019). In other words, all reads with N or more (N usually being 1 or 2) mutations were classified as “labeled”, and all other reads were classified as “unlabeled”. This strategy is intuitive and computationally efficient. Despite this, it suffers from some serious shortcomings. For one, the mutational content of reads from labeled RNA is largely a function of three factors: 1) the U-content of the region of the RNA from which the read is derived, 2) the metabolic label incorporation and chemical recoding efficiencies, and 3) the background mutation rate. While the background mutation rate is often fairly constant across samples and RNAs, the other two factors are subject to large amounts of read-to-read and sample-to-sample variability. For example, different RNAs can have very different average U-contents. In addition, perturbing cellular metabolism often decreases label incorporation rates (Vock and Simon 2023; Mabin et al. 2025). This latter point is an especially concerning batch effect, as it can cause the mutation content of reads from a given feature to vary not because the amount of labeled RNA differs between two conditions, but because the incorporation rate is lower in one condition versus the other. The result is that mutation content cutoffs often provide a simple but biased estimate for the amount of labeled RNA from a given feature (Vock et al. 2025).\nA more robust analysis strategy is mixture modeling (Schofield et al. 2018; Jürges, Dölken, and Erhard 2018). In this strategy, assumptions are made about the distributions that best describe the expected mutational content from labeled and unlabeled RNA. For example, the number of mutations in reads from these two populations could be modeled as following a Poisson distribution with some mean, a mean which is necessarily higher in the labeled RNA reads than the unlabeled RNA reads. Due to the high amounts of read-to-read variance in U-content though, modeling the mutational content as a binomial distribution that takes into account both the incorporation/recoding rate as well as the read’s U-content is optimal (Schofield et al. 2018; Jürges, Dölken, and Erhard 2018). This strategy (or slight variants of it), known as two-component binomial mixture modeling (TCBMM), was thus implemented in analysis software such as GRAND-SLAM, bakR, Halfpipe, and EZbakR. Mixture modeling has been shown to provide unbiased estimates of labeled RNA abundance, even in the face of relatively low incorporation/recoding rates (Figure 2).\n\n\n\n\n\n\nFigure 2: Simple mutation cutoff analysis strategies are less accurate than mixture modeling. From (Vock et al. 2025). Comparison of accuracy of 3 analysis strategies on 3 simulated datasets. Each column represents analysis for a particular dataset. Datasets differ in their simulated plabeled (pnew in formalism presented here; 1%, 2.5%, and 5% mutation rates). Each row represents a particular analysis strategy. Top row: labeled reads are defined as those with at least 1 (&gt; 0) mutation. This is the strategy implemented in SLAMDUNK by default. Middle row: labeled reads are defined as those with at least 2 (&gt; 1) mutations. Represents another commonly used NR-seq analysis cutoff. Bottom row: EZbakR analysis with two-component mixture modeling. Points are colored by density in all plots. \\(\\theta\\) = fraction of reads from labeled RNA. Red dotted lines represent perfect estimation.\n\n\n\nThe likelihood function for TCBMM (Figure 3) can be generalized as such:\n\\[\n\\begin{gather}\nL(\\theta, p_{\\text{new}}, p_{\\text{old}}) = \\theta \\cdot \\text{Binomial}(\\text{nM}, \\text{nN}, p_{\\text{new}}) + (1 - \\theta) \\cdot \\text{Binomial}(\\text{nM}, \\text{nN}, p_{\\text{old}})  \\\\\n\\text{Binomial}(\\text{nM}, \\text{nN}, p_{\\text{old}}) =  \\binom{\\text{nN}}{\\text{nM}}p^{\\text{nM}}(1-p)^{\\text{nN} - \\text{nM}} \\\\\n\\theta = \\text{fraction of reads from labeled RNA (aka the new-to-total ratio, or NTR)} \\\\\n\\text{nM} = \\text{number of mutations (e.g., T-to-C mutations in a standard }\\text{s}^{4}\\text{U} \\text{ NR-seq analysis)} \\\\\n\\text{nN} = \\text{number of mutable nucleotides (e.g., Ts in a standard }\\text{s}^{4}\\text{U} \\text{ NR-seq analysis)} \\\\\np_{\\text{new}}, p_{\\text{old}} = \\text{mutation probability in reads from new (labeled) and old (unlabeled) RNA}\n\\end{gather}\n\\]\n\n\n\n\n\n\nFigure 3: Two-component binomial mixture model. Adapted from (Vock and Simon 2023). The counts of T-to-C mutations, given a number of mutable nucleotides, can be modeled as coming from a mixture of two binomial distribution. One of these (that describing the mutation content of reads from new RNA), has a higher probability of a “success” (mutation) than the other.\n\n\n\n\n\nModifying TCBMM\nThe power of mixture modeling lies in both its robustness as well as its extensibility. TCBMM makes several assumptions about the mutational content of NR-seq reads; namely that:\n\nEvery uridine in an RNA synthesized in the presence of label was equally likely to get replaced with s4U. This is formalized above by there being only one \\(p_{\\text{new}}\\).\nEvery sequenced uridine in an unlabeled RNA was equally likely to give rise to a non-s4U-related mutation due to sequencing errors, alignment errors, etc. This is formalized above by there being only one \\(p_{\\text{old}}\\).\nBy default, all existing tools (GRAND-SLAM, bakR, Halfpipe, and EZbakR) assume that the mutation rate in reads from labeled and unlabeled RNA are sample-wide global parameters. That is, all RNAs transcribed from all genes are assumed to have the same rate of s4U incorporation, and reads from these RNAs are subject to the same background mutation rate. Thus\\(p_{\\text{new}}\\) and \\(p_{\\text{old}}\\) are assumed to be the same for all genes in the above formalism.\n\nIf users find one or more of these assumptions to be violated, they can attempt to modify and extend this model. Towards that end, several modification of standard TCBMM have been proposed. These include:\n\nThree-component mixture modeling, where a second population of reads from unlabeled RNA with a higher mutation rate (presumably due to heightened alignment errors) is modeled.\nOverdisperse mixture modeling where an overdisperse binomial distribution (e.g., a beta-binomial) replaces one or both of the binomial distribution components, or where a different incorporation rate parameter is estimated for fast and slow turnover RNA.\nHierarchical mixture modeling where a sample-wide average incorporation rate is inferred and used as a strongly regularizing prior to estimate feature-specific incorporation rates.\nModeling the transcription process, which at short label times leads to an expected position-dependency in the incorporation rate.\n\nWhile all of these are theoretically promising, the challenge of fitting more complex models is two-fold. 1) Their increased flexibility comes with an increased risk of overfitting. This can lead to estimate instability, where a better model fit yields extreme conclusions about RNA dynamics (e.g., unusually high fraction new and thus unrealistically rapid turnover kinetics). 2) While an alternative model may capture one aspect of the true data generating process unaccounted for by TCBMM, it may amplify biases that arise from not accounting for some other aspect of the data generating process.\nTo illustrate point 1, consider the task of fitting a TCBMM with feature-specific mutation rates. While in theory, it is straightforward to obtain maximum likelihood estimates for the parameters of such a model, model flexibility can make interpretation of maximum likelihood parameters fraught. Intuitively, this is because changing different parameters can have similar expected impacts on your data. A higher fraction new will yield more reads with high mutational content, but so will a low fraction new combined with a higher background mutation rate. While with enough reads these two situations can be accurately deconvolved, this analysis is highly uncertain for low coverage features.\nTo illustrate point 2, consider the idea of three-component mixture modeling. While this can capture certain types of overdispersion in mutation rates from old RNA reads, it can amplify biases from not modeling overdispersion in mutation rates from new RNA reads. A three-component mixture model will classify many moderate mutation rate reads as “old”, when in fact they may represent a preponderance of low mutation rate new reads. This kind of overdispersion is made even more likely by the fact that metabolic label availability will likely ramp up and down over time (Rummel, Sakellaridi, and Erhard 2023). Thus, reads from RNA synthesized at different time points may have different true mutation rates.\nHow can one navigate building more complex models while avoiding some of these problems? Point 1 can be addressed through regularization. From a Bayesian perspective, this means using one’s domain expertise or trends in these high-throughput datasets to craft informative priors that constrain the parameter search space (Morris 1983). For example, to fit a hierarchical mixture model in EZbakR, where each feature is allowed to have its own new read mutation rate (\\(p_{\\text{new}}\\)), I crafted a strategy to infer strongly regularizing priors from sample-wide trends (Vock et al. 2025). These priors were designed to be very conservative to limit estimate variance.\nPoint 2 represents the fundamental challenge of statistical modeling: crafting a model of the data generating process that faithfully captures most of the relevant sources of variance in one’s data. This is difficult, but several strategies exist to navigate this complexity. Information criteria are a popular metric by which to compare fits of a more complex model to that of a simpler model. These criteria are designed to penalize model complexity to avoid rewarding overfit models with better metrics (Dziak et al. 2020; J. Zhang, Yang, and Ding 2023). While simple criteria like the Akaike information criteria (AIC) are popular due to their implementation ease, more robust metrics have been developed since the advent of AIC (Akaike 1998; Watanabe and Opper 2010; Cavanaugh and Neath 2019). For example, in the context of mixture modeling, the widely applicable information criteria (WAIC; a.k.a. the Watanabe-Akaike Information Criteria) may provide a number of advantages over AIC (Gelman et al. 1995; Watanabe and Opper 2010).\nInformation criteria are not panacea though. Information criteria are rigorous ways to assess if added model complexity is capture a significant amount of variance in your data that a simple model fails to account for. Even if a more complex model is succeeding by this metric, it could still be providing biased estimates (Dziak et al. 2020; J. Zhang, Yang, and Ding 2023). Thus, when designing new NR-seq models, it is advisable to use a multi-pronged approach that weighs several metrics when deciding if a more complex model is worth using. If adopting a Bayesian approach, information criteria can be complemented with posterior predictive checks, where data is simulated from the fit model and compared to the analyzed data (Lynch and Western 2004). Serious discrepancies between simulated and real data can reveal model mis-specifications and guide model improvement. We also suggest comparing results given by standard TCBMM with those provided by a more complex model. Extreme sample-wide discrepancies between the two may signify that the more complex model is overfitting or providing unstable estimates. Discrepancies should thus be thoroughly explored and explained. Finally, one should assess model robustness through simulations from a data generating process more complicated than that used for model fitting (Vock and Simon 2023). If the bias introduced by these true vs. assumed data generating process discrepancies is amplified by use of a more complex model relative to TCBMM, we urge caution in adopting the more complex model. The simplicity and robustness of TCBMM makes it an effective baseline with which to compare alternative models.\n\n\nTranscript isoform analyses of short-read data\nTranscript isoforms are the RNA species whose synthesis and degradation kinetics are of biological significance. Despite this, quantifying the NTRs of individual transcript isoforms in a short-read NR-seq experiment is challenging. This is because most short reads cannot be unambiguously assigned to a specific transcript isoform. Strategies have been developed to overcome these challenges in the context of quantifying the abundances of isoforms (Li and Dewey 2011; Bray et al. 2016; Pachter 2011; Patro et al. 2017; C. Zhang et al. 2017). I thus developed a similar approach to estimate isoform-specific NTRs in short read NR-seq data (Mabin et al. 2025).\nThe approach, implemented in the EZbakR suite, combines standard NR-seq TCBMM with transcript isoform quantification. This approach estimates NTRs for each observed transcript equivalence class (TEC; i.e., the set of isoforms with which a read is compatible (Cmero, Davidson, and Oshlack 2019)) and integrates this with estimates of transcript isoform abundances from standard tools for this task. EZbakR is able to estimate isoform-specific NTRs by deconvolving TEC NTRs using a novel beta mixing model. I used this approach while in the Simon lab, and in collaboration with Bobby Hogg’s lab at the NIH, to study the synthesis and degradation kinetics of individual transcript isoforms, and to identify NMD sensitive isoforms (Mabin et al. 2025).\nAccurate transcript isoform analyses require annotations of expressed isoforms (Soneson et al. 2019; Varabyou, Salzberg, and Pertea 2021). In our work presenting the isoform-NTR estimation strategy, we noted that standard off-the-shelf references did not faithfully reflect our particular cell line’s transcriptome. We thus explored strategies using StringTie2 and custom filtering to build more accurate, bespoke annotations (Kovaka et al. 2019). We showed that this approach significantly improved the accuracy of transcript-isoform level analyses. While a powerful approach, the need to build custom annotations adds to the complexity of the workflow. In addition, tools for building such annotations are not without their limitations, as ab initio assembly is a fundamentally difficult task. Having matched, unlabeled, long read data can improve assembly, but presents its own challenges and shortcomings (Kovaka et al. 2019). Thus, while isoform-level analyses represent a powerful new paradigm in analyses of NR-seq data, I urge users to carefully assess the potential of annotation-related biases in their analyses.\n\n\nModeling and correcting for dropout in NR-seq data\nLike all other RNA-seq based methods, NR-seq data can be plagued by various biases. The most prominent example of this is dropout, a phenomenon observed across many distinct datasets (Zimmer et al. 2023; Berg, Lodha, et al. 2024). Dropout is the underrepresentation of reads from labeled RNA. While its origins are not fully understood, it has been proposed to be caused by a combination of disproportionate loss of labeled RNA during RNA extraction and library preparation, loss of high mutation content reads during alignment, and toxicity due to s4U labeling.\nDropout in NR-seq data can be detected and quantified with the help of no-label controls. A no-label control refers to data from samples not fed with a metabolic label. These are important controls that should be included in all NR-seq datasets. A simple model of dropout is that there exists a global rate at which label-containing RNA is lost relative to unlabeled RNA, referred to as the dropout rate (Berg, Lodha, et al. 2024). Thus, rapidly turned over RNA that are more highly labeled will be disproportionately affected compared to more stable, relatively unlabeled RNA. Comparing the estimated turnover rate in the labeled samples to the no-label vs. labeled read counts can thus reveal dropout. More specifically, dropout looks like a strong correlation between these quantities. Plots like those shown in Figure 4 can be easily made with both grandR and EZbakR.\n\n\n\n\n\n\nFigure 4: Visualizing dropout. A simple model of dropout was simulated using EZbakR’s EZSimulate() function, where labeled RNA is lost relative to unlabeled RNA at a rate denoted above each pair of plots (pdo). grandR implements a visualization strategy that correlates the NTR rank (1 = smallest) vs. the log-difference in +label and -label read coverage (dropout). The strength of correlation between these is related to the rate of dropout. bakR implements a visualization strategy that allows for direct assessment of the fit of a simple dropout model on the data. The quantities plotted are similar to those in grandR, just on different model-relevant scales. EZbakR implements both of these visualization strategies.\n\n\n\nIf your NR-seq data suffers from non-trivial amounts of dropout, you can employ strategies to correct for dropout. grandR was the first tool to implement such a strategy. It follows from a model alluded to above and assumes that there is a sample-wide rate (call it \\(\\text{p}_{\\text{do}}\\)) at which labeled RNA is disproportionately lost relative to unlabeled RNA. If this is the case, then then true fraction of reads from labeled RNA (\\(\\theta_{\\text{true}}\\)) is related to the dropout-biased estimate (\\(\\theta_{\\text{do}}\\)) like so:\n\\[\n\\begin{gather}\n\\theta_{\\text{true}} = \\frac{\\theta_{\\text{do}}\\cdot\\text{p}_{\\text{do}}}{\\theta_{\\text{do}}\\cdot\\text{p}_{\\text{do}} + (1-\\theta_{\\text{do}})}\n\\end{gather}\n\\]\nSimilarly, a relationship exists between the true expected read counts from a given feature and the observed, labeled sample read count:\n\\[\n\\begin{gather}\n\\text{R}_{\\text{true}} = \\text{R}_{\\text{do}} \\cdot \\frac{\\theta_{\\text{G}}\\cdot(1-\\text{p}_{\\text{do}}) + (1-\\theta_{\\text{G}})}{\\theta_{\\text{true}}\\cdot(1-\\text{p}_{\\text{do}}) + (1-\\theta_{\\text{G}})} \\\\\n\\theta_{\\text{G}} = \\frac{\\theta_{\\text{G,do}}}{(1-\\text{p}_{\\text{do}}) + \\theta_{\\text{G,do}} \\cdot \\text{p}_{\\text{do}}} \\\\\n\\theta_{\\text{G,do}} = \\frac{\\sum_{\\text{j=1}}^{\\text{NF}} \\theta_{\\text{do,j}} \\cdot \\text{R}_{\\text{j}}}{\\sum_{\\text{j=1}}^{\\text{NF}} \\text{R}_{\\text{j}}}\n\\end{gather}\n\\]\nUsing these theoretical relationships, a dropout rate can be estimated that, after correcting read counts, yields no correlation between the labeled vs. no-label read count ratio and the turnover kinetics of an RNA. This strategy is implemented in grandR. bakR implements a similar strategy by which the relationship between dropout and the NTR is modeled and fit with the method of maximum likelihood. EZbakR implements both of these strategies.\nDropout correction is a powerful addition to the NR-seq analysis toolkit. Despite this, it is not without its limitations. The requirement for matched no-label samples in all conditions tested adds to the experimental burden. In addition, even when no-label data is collected, resource constraints often lead researchers to only collect a single replicate of this data, as it is not itself a useful NR-seq sample. This can make the dropout metric, the ratio of labeled:no-label read counts, noisy.\nTo address these limitations, EZbakR implements a strategy I refer to as dropout normalization. Dropout normalization does not require any no-label data and involves comparing internally normalized NTRs rather than read counts across samples. The strategy starts by identifying the lowest dropout sample (e.g., that which provides the lowest median uncorrected half-life estimate) and estimating dropout in other samples relative to this sample. This estimated relative dropout rate is then used to correct NTRs and read counts in all samples. This strategy has proven particularly useful as dropout rates often correlate with biological conditions, which risks confounding comparative analyses if not properly accounted for. The downside of dropout normalization is that it tends to normalize out global differences in half-life estimates, even if these are biologically real and not solely the result of dropout. Dropout is thus similar in spirit to RNA-seq read count normalization methods such as the median-of-ratios or TMM, as well as a simpler median-kdeg normalization strategy implemented in grandR, which all effectively assume that there are no real global differences in RNA levels/turnover kinetics across samples. Dropout normalization alternatively assumes that any global changes in RNA turnover kinetics are dropout driven. Users should thus be aware of this assumption when using dropout normalization."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#kinetic-parameter-estimation-with-nr-seq-data",
    "href": "posts/nrseq_deepdive/index.html#kinetic-parameter-estimation-with-nr-seq-data",
    "title": "NR-seq: an in-depth review",
    "section": "Kinetic parameter estimation with NR-seq data",
    "text": "Kinetic parameter estimation with NR-seq data\nOne of the original motivations for developing NR-seq was to robustly estimate the kinetics of RNA synthesis and degradation. In this section, I discuss the modeling and assumptions necessary to make this possible.\n\nStandard kinetic analyses\nVanilla, bulk NR-seq is a powerful method by which to quantify the kinetics of RNA synthesis and degradation. A typical kinetic analysis means modeling the NTR of reads from a feature (e.g., the union of exons at a gene) as a function of the RNA’s synthesis and degradation rate constants. The simplest identifiable model of this sort assumes that mature mRNA is synthesized at a rate \\(\\text{k}_{\\text{syn}}\\) and degraded with a rate constant \\(\\text{k}_{\\text{deg}}\\). This model can be formalized via the following analytically tractable differential equation:\n\\[\n\\begin{gather}\n\\frac{\\text{dR}}{\\text{dt}} = \\text{k}_{\\text{syn}} - \\text{k}_{\\text{deg}} \\cdot \\text{R} \\\\\n\\text{Solution (with R(0) = 0)}: \\text{R(t)} = \\frac{\\text{k}_{\\text{syn}}}{\\text{k}_{\\text{deg}}} \\cdot (1 - \\text{e}^{\\text{k}_{\\text{deg}} \\cdot \\text{t}}) \\\\\n\\end{gather}\n\\]\nAt steady-state, the following relationships hold:\n\\[\n\\begin{gather}\n\\frac{\\text{dR}}{\\text{dt}} = 0;\\text{ }\\text{R}_{\\text{ss}} = \\frac{\\text{k}_{\\text{syn}}}{\\text{k}_{\\text{deg}}} \\\\\n\\text{NTR} = \\frac{\\text{R(tl)}}{\\text{R}_{\\text{ss}}} = 1 - \\text{e}^{\\text{k}_{\\text{deg}} \\cdot \\text{t}} \\\\\n\\text{k}_{\\text{deg}} = \\frac{-\\text{ln}(1-\\text{NTR})}{\\text{t}_{\\text{label}}}\n\\end{gather}\n\\] This model makes several explicit assumptions:\n\nSteady-state: this means that during the labeling, RNA levels are not changing. While individual cells may rarely ever be at steady-state, e.g., as transcript levels are regulated throughout the cell cycle, in bulk NR-seq this means assuming that the average RNA levels across all cells assayed are constant.\nZeroth-order synthesis kinetics (i.e., transcription is a Poisson process): this means that there exists a single, constant rate of transcription during the labeling. While once again often violated in single cells due to phenomena such as transcriptional bursting (Tunnacliffe and Chubb 2020), it is likely a decent model of bulk transcriptional behavior (Furlan, Pretis, and Pelizzola 2021).\nFirst-order degradation (i.e., exponential decay): this means that RNAs have a characteristic half-life that does not change over either the course of the labeling or their lifetime. This means assuming that RNAs are “ageless” or that the probability an RNA degrades in the next instance is independent of how long it has been around. This assumption breaks down if multiple rate-limiting steps separate an RNA’s birth from its death (e.g., if RNA export from the nucleus is on a similar time-scale of cytoplasmic degradation), or if there exist multiple sub-populations of RNA with different decay kinetics (e.g., if nuclear mRNA is degraded at a different rate than cytoplasmic mRNA) (Deneke, Lipowsky, and Valleriani 2013; Bedi et al. 2024; Ietswaart et al. 2024). While this assumption is inevitably violated to some extent due to the complexity of the RNA life cycle, it has consistently proven to be a reasonable approximation in a variety of settings, while still providing a useful (if somewhat biased) picture of general turnover kinetics when it is violated.\n\n\n\nNon-steady-state modeling\nThe steady-state assumption makes analyzing and interpreting NR-seq data easy. Despite this, it is often violated in contexts where you are applying a perturbation to cells shortly before or during labeling. Narain et al. proposed a strategy to obtain unbiased estimates even in this setting (Narain et al. 2021). The intuition behind how this approach works is that no matter what, the dynamics of old (unlabeled) RNA is entirely a function of turnover kinetics. Thus, if you know the levels of an RNA at the start of labeling, then that combined with its level at the end of labeling tells you how much RNA decayed in that time frame. Formally:\n\\[\n\\begin{gather}\n\\text{R}_{\\text{old}}(\\text{t}) = \\text{R}_{\\text{init}} \\cdot e^{-\\text{k}_{\\text{deg}} \\cdot \\text{t}} \\\\\n\\text{R}_{\\text{new}}(\\text{t}) = \\frac{\\text{k}_{\\text{syn}}}{\\text{k}_{\\text{deg}}} \\cdot (1 - e^{\\text{k}_{\\text{deg}} \\cdot \\text{t}}) \\\\\n-\\frac{\\text{ln}(\\frac{\\text{R}_{\\text{old}}(\\text{t}_{\\text{label}})}{\\text{R}_{\\text{init}}})}{\\text{t}_{\\text{label}}} = \\text{k}_{\\text{deg}} \\\\\n\\frac{\\text{R}_{\\text{new}}(\\text{t}_{\\text{label}})}{1-e^{\\text{k}_{\\text{deg}} \\cdot \\text{t}}}\\cdot \\text{k}_{\\text{deg}} = \\text{k}_{\\text{syn}}\n\\end{gather}\n\\]\nAn estimate for \\(\\text{R}_{\\text{init}}\\) typically comes from matched RNA-seq data collected at a time point equivalent to that at which labeling was started for a given labeled sample. Note, the stability or synthesis rate of an RNA could be changing during the labeling. The \\(\\text{k}_{\\text{deg}}\\) and \\(\\text{k}_{\\text{syn}}\\) estimates from this strategy should thus be thought of as the time-averaged value of a potentially time-varying \\(\\text{k}_{\\text{deg}}(\\text{t})\\) and \\(\\text{k}_{\\text{syn}}(\\text{t})\\). Both grandR and EZbakR implement this analysis strategy.\nWhile having an approach that yields theoretically unbiased estimates of turnover and synthesis kinetics even in the face of non-steady-state dynamics is powerful, it is not without its limitations. For one, a unique experimental design is required. If you are missing RNA-seq data from the start of any labeling period, this strategy cannot be applied. In addition, even with the proper experimental design, this analysis approach has some weaknesses. Its statistical properties are suboptimal relative to the steady-state analysis. In particular, the variance of the \\(\\text{k}_{\\text{deg}}\\) estimation is typically higher than that of the steady-state estimate, especially at low NTRs (Figure 5). Intuitively, this is because the steady-state estimate is only a function of a single sample’s NTR, while the non-steady-state estimate requires comparing an NTR estimate combined with a normalized read count in one sample to a normalized read count in another sample. Thus, while the estimate is technically unbiased, its mean-squared error is not necessarily lower than that of the biased steady-state analysis. This problem can partially be addressed through greater sequencing depth, label time optimization, and more replicates, but it is a challenge that users should be aware of.\n\n\n\n\n\n\nFigure 5: Comparison of steady-state and non-steady-state (NSS) analysis strategies. Steady-state data was simulated using EZSimulate() at 3 different label times (2, 4, and 6 hours). Both of these analysis strategies are appropriate in this setting; this comparison is meant to show differences in the best-case variance properties of these two approaches. 2 replicates of labeled and 2 replicates of no-label data were simulated. The simulated (true) and estimated degradation rate constants are compared in all plots. Points are colored by their local density. Red dotted line represents perfect estimation.\n\n\n\n\n\nDo you need to account for growth rates?\nA recent preprint from Brian Cleary’s lab made the following claim:\n\nIf growth rate is unmeasured and not accounted for, [the degradation rate constant estimate γ*] captures a mixture of distinct biological processes – RNA degradation and cell growth and division – where effects of the latter are slightly mismodeled. If the variations in γ* across conditions, cell lines, or datasets simply reflected variations in RNA degradation rates, then it would be promising to study those variations to reveal principles or mechanisms of regulation, as some have proposed51. However, such variations can arise from differences in degradation rate, growth rate, experimental efficiency, or experimental design, complicating analysis and interpretation.\n\nCitation 51 is to work I was involved in (RNAdecayCafe), a database of half-lives derived from reprocessing and analyzing bulk metabolic labeling data. Is this true? In short, the answer is technically yes but practically no (in most cases). In more detail:\n\nAs discussed in earlier sections, when an RNA’s levels are at steady-state, the fraction of RNA that is new (i.e., the new-to-total ratio, or NTR) is purely a function of RNA degradation kinetics. If the levels of that RNA are changing during the labeling, the NTR can be a function of both synthesis and degradation kinetics. In either case, growth rates never directly influence these measurements.\nThis is because growth in the absence of kinetic parameter regulation leads to changes in RNA concentrations. The NTR is not a function of absolute RNA concentration. Thus NTR-based kinetic parameter inference is not directly influenced by growth rates.\nCell’s respond to growth by regulating their RNA synthesis and degradation kinetics. This is so that they can maintain RNA concentration homeostasis. Thus, actively dividing cells are rarely ever at steady-state. Thus, growth can indirectly bias kinetic parameter estimates if the ensuing regulation of synthesis and degradation kinetics causes the steady-state assumption to be violated.\nIn addition, as a population of cells grows and divides, the number of cells increase. Thus, if an RNA has a constant per-cell average synthesis rate, the effective bulk synthesis rate ([per-cell rate] x [# of cells]) increases throughout the course of labeling. This introduces a deviation from the steady-state constant synthesis rate assumption, but it is typically very slight. This is because the label time used for a standard pulse-label NR-seq experiment (or the chase time for a pulse-chase) is often much shorter than the doubling time of a given cell, especially in mammalian contexts.\nFailure to account for the non-steady-state increase in synthesis rates over the course of labeling causes the growth rate to get absorbed into degradation rate constant estimates. Since there is one growth rate, estimates for all genes are approximately shifted equally. Thus, the general scale of one’s degradation rate constant estimates may be biased if not accounting for growth. This makes it difficult to compare absolute kinetic parameter values across datasets/conditions, but unless you are directly testing assumptions about true scale changes (i.e., true global increases or decreases in degradation kinetics), then it is often sufficient to normalize any global scale differences out. In many (arguably most) use cases, what matters is that you can accurately distinguish stable from unstable RNA, and identify RNA that go from being stable/unstable to unstable/stable across conditions.\nIf the cells you are probing are actively dividing, growth matters a lot in the single-cell setting. This is because the steady-state assumption is almost always violated in this setting (again, not directly because of growth, but because of what cells do in response to growth). It matters a lot less in the bulk setting if probing an asynchronous population of cells, though it can be a cause of differences in kinetic parameters across cell lines (though again not through direct influence).\n\nIn summary, cell growth can induce non-steady state dynamics that complicates analysis of single-cell NR-seq data, but these concerns are often lessened in the bulk setting. While differences in cell growth can lead to differences in estimated bulk turnover kinetics across cell lines (e.g., if degradation is in part regulated to maintain RNA concentration homeostasis, these differences are real, can be accurately estimated with standard NR-seq analyses. Dissecting the biochemical causes of this regulation would be interesting and feasible from this data.\n\n\nSubcellular NR-seq\nSeveral studies have combined NR-seq with subcellular fractionation to investigate the kinetics of subcellular RNA trafficking. To date, such efforts have included sequencing chromatin-associated RNA (Ietswaart et al. 2024; Yin et al. 2020), nuclear RNA (Ietswaart et al. 2024; Yin et al. 2020; Zuckerman et al. 2020; Müller et al. 2024; Steinbrecht et al. 2024; Williams et al. 2025), cytoplasmic RNA (Ietswaart et al. 2024; Yin et al. 2020; Zuckerman et al. 2020; Müller et al. 2024; Steinbrecht et al. 2024; Williams et al. 2025), ribosome-associated RNA (Ietswaart et al. 2024), and membrane-associated RNA (Steinbrecht et al. 2024). These data necessitate fitting more complex kinetic models to multi-sample datasets. While Halfpipe was the first tool to support analyses of one particular fractionation scheme, EZbakR is the first tool designed to support fitting any such model.\nEZbakR makes use of the fact that the most common identifiable models of RNA dynamics that subcellular NR-seq has been used to fit can be formalized as a linear system of ordinary differential equations (ODEs) of the form:\n\\[\n\\begin{gather}\n\\textbf{R} = \\text{M} \\cdot \\dot{\\textbf{R}} + \\textbf{b} \\\\\n\\textbf{R} = \\text{vector of RNA species abundances of length N} \\\\\n\\text{M} = \\text{NxN matrix of first-order kinetic parameters} \\\\\n\\dot{\\textbf{R}} = \\text{time derivative of } \\textbf{R} \\\\\n\\textbf{b} = \\text{vector of zeroth-order rate constants (e.g., synthesis rate)}\n\\end{gather}\n\\]\nAny such system of ODEs has an analytic solution inferrable from the matrix M and forcing vector \\(\\textbf{b}\\). EZbakR makes use of this fact to efficiently fit these models without needing to rely on numerical integration. It also provides means for uncertainty quantification that is capable of flagging instances of practical unidentifiability, i.e., when specific parameters of a model are outside of the dynamic range of what can be reasonably estimated given experimental details and noise.\nEZbakR also implements a novel strategy by which to normalize certain classes of subcellular NR-seq. This strategy relies on having data from individual fractions, as well as a sufficient set of combinations of fractions (e.g., nuclear, cytoplasmic, and whole cell). It uses the fact that the total fraction of reads in each of the sub-compartments that are new (when adjusted for average length differences in the RNA populations) is related to the same quantity in the combination compartment (Steinbrecht et al. 2024; Vock et al. 2025). The scale factor that relates these quantities is precisely the ratio of absolute abundances between the sub-compartments. For the whole-cell, nuclear, and cytoplasmic example, this relationship looks like:\n\\[\n\\begin{gather}\n\\theta_{\\text{WC}} = \\frac{[\\text{C}]}{[\\text{C}] + [\\text{N}]}\\theta_{\\text{C}} + \\frac{[\\text{N}]}{[\\text{C}] + [\\text{N}]}\\theta_{\\text{N}} = s\\cdot\\theta_{\\text{C}} + (1-s)\\cdot\\theta_{\\text{N}} \\\\\n\\theta = \\text{total fraction of reads from new RNA in a given compartment} \\\\\n\\text{WC = whole-cell; C = cytoplasm; N = nucleus} \\\\\n[\\text{C}] = \\text{absolute levels of cytoplasmic RNA} \\\\\n[\\text{N}] = \\text{absolute levels of nuclear RNA}\n\\end{gather}\n\\]\nAn appropriate RNA-seq read count normalization scale factor can be inferred from the value of \\(s\\). While this strategy requires having NR-seq in all the necessary sub- and combination-compartments, recent work suggests that it may be possible to relax this requirement and instead only require at least standard RNA-seq data for all such compartments (Ntasis and Guigó 2025)."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#processing-nr-seq-data",
    "href": "posts/nrseq_deepdive/index.html#processing-nr-seq-data",
    "title": "NR-seq: an in-depth review",
    "section": "Processing NR-seq data",
    "text": "Processing NR-seq data\nThe previous sections assumed that we had optimally processed NR-seq data. In this section, I discuss how to obtain such data and the challenges involved.\n\nAligning NR-seq data\nThe first data processing step with NR-seq-specific challenges is aligning reads to the genome. The mutations introduced by recoding chemistries make this more challenging than in standard RNA-seq. These mismatches risk yielding inaccurate alignments or causing some reads to be unmappable.\nSLAMDUNK, one of the first bioinformatic pipelines developed for processing NR-seq data, utilized the NextGenMap aligner to overcome this challenge (Sedlazeck, Rescheneder, and Von Haeseler 2013; Neumann et al. 2019). NextGenMap is a non-splice aware aligner that provides an alternative mismatch scoring function designed for NR-seq data. This scoring function penalizes all but T-to-C mismatches, ameliorating the negative impact of these mismatches on alignment accuracy. In addition, SLAMDUNK is optimized for 3’-end sequencing data. Because of this, it is able to reduce the sequence search space by aligning to the set of annotated 3’-UTRs rather than the entire genome. SLAMDUNK also implements a unique multi-mapping read assignment strategy that preferentially assigns ambiguous reads to the more likely 3’-UTR. Despite being designed for 3’-end sequencing, some have used SLAMDUNK to process full-length NR-seq data (Patange et al. 2022). While NextGenMap’s lack of splice awareness makes it impossible to accurately align reads from these libraries to a genome, one can instead opt for alignment to a transcriptome. This has a number of downsides (e.g., improper alignment of intron mapping reads) but can still provide reasonable results (Srivastava et al. 2020).\nFor full-length NR-seq data, standard splice-aware aligners like STAR and HISAT2 were originally preferred (Dobin et al. 2013; Kim et al. 2019). Using these tools typically requires relaxing the default mismatch penalization. Unlike in NextGenMap, no such aligner allows for this to be done in a mismatch type-specific manner (technically BASAL (M. Xu et al. 2025), a recently developed aligner, can also do this, but it cannot provide the unique MD BAM file tag that is required for all existing NR-seq pipelines). This leads to a necessary reduction in alignment accuracy to retain high mutation content reads. To address this challenge, HISAT-3N was developed, which aligns reads to a 3-base genome, i.e. a genome where all Ts are converted to Cs (Y. Zhang et al. 2021). By similarly converting Ts in all reads to Cs, reads could be aligned without penalizing T-to-C mismatches. While this has been useful in recovering high mutation content reads, it is not without its own unique downsides. The lower complexity genome increases the likelihood that a read does not uniquely map to a given locus, and can more generally reduce the accuracy of read alignment. Recently, a rigorous simulation benchmark was performed to compare HISAT-3N and STAR alignment of NR-seq data (Popitsch et al. 2024). It concluded that neither is strictly superior to the other, with each having unique strengths and weaknesses. It remains to be seen if the same holds true for more recent 3N-aligners like rmapalign3N (Muller et al. 2025). As STAR is more widely used and actively maintained aligner than HISAT-3N, we and others typically opt for STAR when aligning full-length RNA NR-seq data.\nIdeally, a strategy would exist to get the best of both 4- and 3-base alignment. Towards that end, grandRescue was developed (Berg, Lodha, et al. 2024). grandRescue first aligns reads with STAR to a 4-base genome. Then, reads that failed to align are aligned with STAR to a 3-base genome (while HISAT-3N makes 3-base genome alignment convenient, it is technically possible with any aligner. grandRescue provides the necessary helper functions to make this user friendly). This allows grandRescue to outperform both STAR and HISAT-3N on the task of accurately aligning high mutation content reads from full-length NR-seq data. It also showed improved alignment accuracy over NextGenMap transcriptome alignment. While grandRescue has not yet reached STAR’s level of bioinformatic maturity, it introduced a powerful paradigm for the accurate alignment of NR-seq reads.\n\n\nCounting mutations\nOnce NR-seq reads have been aligned to a genome, many of the additional processing steps are shared with standard RNA-seq analyses. Despite this, a unique aspect of NR-seq data processing post-alignment is mutation counting.\nNR-seq reads from labeled RNA will on average contain more mutations of a particular type than those from unlabeled reads. Thus, quantifying the number of mutations of the relevant type (e.g., T-to-C mutations in standard s4U NR-seq experiments) is a key component of any NR-seq pipeline. Unfortunately, there are no flexible, general-purpose bioinformatic tools specifically for this purpose. Instead, every NR-seq pipeline typically reimplements a solution to this problem from scratch. SLAMDUNK is the closest exception, as it includes a mutation counting module that in theory could be slotted into other NR-seq pipelines. Despite this, SLAMDUNK’s mutation counting module makes some rigid assumptions (e.g., single-end, forward stranded libraries) that make it difficult to use in many situations. In addition, SLAMDUNK does not currently provide read-specific mutation counts and instead provides feature-wide summarization of this information. This makes it incompatible with optimal downstream analyses (namely mixture modeling; see the earlier section on “Modeling the mutation content of sequencing reads”). Finally, SLAMDUNK can only provide information about T-to-C mutational content, making it incompatible with s6G NR-seq datasets (Kiefer, Schofield, and Simon 2018; Gasser et al. 2020).\nfastq2EZbakR and GRAND-SLAM lack the modularity of SLAMDUNK, but are compatible with a wider array of NR-seq library types. Both can quantify T-to-C and G-to-A mutations. fastq2EZbakR is additionally able to count any combination of all mutation types. This allows these tools to support s6G-based NR-seq assays, and in the case of fastq2EZbakR, even potentially support the wider array of mutation-based chemical probing experiments.\nA general feature of most NR-seq processing tools is the ability to mask single nucleotide polymorphisms (SNPs) when counting mutations. Despite its ubiquity, the relative efficacy of different mutational artifact calling strategies in NR-seq data is understudied. Several approaches have been suggested, including a simple cutoff of the fraction of times a given nucleotide is identified as having a mismatch with the reference, or the use of established variant calling software like VarScan (Koboldt et al. 2012) or BCFtools (Danecek et al. 2021). Also, while SNP calling has been shown to significantly impact analyses relying on simple mutation-content cutoffs for quantifying labeled read abundances, mixture modeling may be more robust to high background mutation rates and uncalled SNPs. Despite this, systematic, non-random mutations may violate the assumptions of independence underlying popular mixture modeling strategies, especially in lower complexity libraries like those from 3’-end sequencing, highlighting the potential importance of SNP-making and its continued optimization.\n\n\nFeature assignment\nNR-seq reads often must be assigned to the annotated features (e.g., genes) to which they belong. Read-by-read feature assignment can then be combined with mismatch calls to estimate the feature’s NTR and metabolic kinetic parameters. While most NR-seq pipelines provide a limited set of features to which this assignment can be done, fastq2EZbakR recently significantly expanded the flexibility of NR-seq feature assignment. fastq2EZbakR can assign reads to genes (exons and introns), exclusively exonic regions of genes, exonic bins, transcript equivalence classes, and exon-exon junctions. While some analyses of NR-seq only require standard gene-level quantification, fastq2EZbakR’s expanded set of feature assignment strategies allows for a higher resolution dissection of RNA dynamics.\n\n\nProcessed data format\nWhat is the best way to store and format processed NR-seq data? For most analyses of vanilla RNA-seq data, the answer to this question is a count matrix (J.-W. Chen et al. 2023). For each feature analyzed and each sample collected, a count matrix notes the number of times a read came from that feature in that sample or tracks some more sophisticated measure of that feature’s abundance. Count matrices are the ideal processed RNA-seq data format for several reasons. Their size largely scales with the number of samples, not the depth of sequencing. This is because the number of annotated features in a given organism (i.e., the number of rows of the count matrix) is mostly fixed, and no matter how many reads you have in a given sample, each feature will end up with a single number for each sample. This format also lends itself to highly efficient linear algebra operations and is widely used in a number of bioinformatic libraries (Robinson, McCarthy, and Smyth 2010; Love, Huber, and Anders 2014; Ritchie et al. 2015; Y. Chen et al. 2025). Thus, one could argue that a similar processed data format should be used for NR-seq.\nTowards that end, several popular NR-seq tools provide processed data to users in a form analogous to a count matrix. Both SLAMDUNK and GRAND-SLAM provide a table with a fixed set of data points for each genes. SLAMDUNK provides users with information about the number of reads with conversions from a given feature, and GRAND-SLAM provides an NTR estimate for each feature from mixture modeling. The former is suboptimal for reasons discussed in the section on “Analyzing NR-seq data” but the latter represents the gold standard in NR-seq analysis output. Thus, NR-seq tools should at least be able to provide NTR estimates and read counts for each feature in each sample collected.\nDespite this, I argue that there are a number of important limitations to providing users with this form of processed data. In my experience, when things go wrong with an NR-seq analysis, the problems start at the mixture modeling step. Thus, by only providing users mixture model output, and thus limited strategies by which to interrogate model fit and identify potential model biases, this form of processed data risks obscuring issues in an NR-seq dataset. In addition, only providing NTR estimates limits innovation in NR-seq analyses. If a user wants to develop their own NTR estimation strategy, they must also develop their own pipeline to generate data in a form compatible with their modeling strategy.\nFor this reason, fastq2EZbakR (and bam2bakR before it) provides user with the processed data necessary to fit state-of-the-art models of NR-seq data. For each feature and sample, fastq2EZbakR reports the number of reads with a given number of mismatches of the relevant type (e.g., T-to-C in a single label, s4U-fed, NR-seq experiment) and a given number of mutable nucleotides (e.g., T’s in the reference). The Simon lab termed this file format a “counts binomial”, or cB, file. The recently developed Halfpipe provides similar data though in a different format. With this information, users can either make use of mixture model fitting strategies implemented in tools like EZbakR or develop their own unique analysis strategy.\nProcessed NR-seq data formats like cB files significantly democratizes the analysis of NR-seq data. Despite this, it is not without its limitations. Unlike with GRAND-SLAM’s output, the size of a cB file scales with the sequencing depth. While the scaling is sub-linear, as data can be compressed by tracking the number of reads with identical mutation and mutable nucleotide content, the output is still often far more unwieldy than GRAND-SLAM’s simple table. To address this limitation, fastq2EZbakR implements an alternative output type that involves creating separate files for separate samples. While the size of these individual files will still scale with the sequencing depth, splitting the files in this manner supports analysis strategies that effectively load only a single sample into RAM at a time. EZbakR implements such an analysis strategy, optimized with the help of the Apache Arrow project and its R frontend, which has significantly increased the scalability of its analyses (Mabin et al. 2025).\nUnique decisions have to be made regarding how to format and distribute NR-seq data. While count matrix-like formats are familiar, scalable, and intuitive, they may significantly limit the types of analyses users can perform. Alternative data formats can address these issues. While more compressed than read-level formats (e.g., BAM files), these alternatives still have to sacrifice RAM scalability. We suggest that all NR-seq pipelines should produce output of this latter type, while optionally providing simpler, count matrix-like output for those looking to avoid the extra hassles."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#optimal-nr-seq-experimental-design",
    "href": "posts/nrseq_deepdive/index.html#optimal-nr-seq-experimental-design",
    "title": "NR-seq: an in-depth review",
    "section": "Optimal NR-seq experimental design",
    "text": "Optimal NR-seq experimental design\nNo matter how optimal your data processing and analysis pipeline is, bad data can fundamentally limit obtainable insights. In this section, I discuss how to best design an NR-seq experiment to avoid these problems.\n\nChoosing the population of RNA to sequence\nIn theory, NR-seq can be applied to a wide array of library preparation strategies to probe the dynamics of your choice of RNA. SLAM-seq originally made use of 3’-end sequencing, and a commercially available kit for this library preparation strategy was developed shortly after its initial publication. Combining NR-seq with 3’-end sequencing has several advantages. For one, it limits the amount of pre-mRNA in your library. Pre-mRNA are typically rapidly turned over and can thus lead to slight overestimation in turnover kinetics if not accounted for in downstream analyses. In addition, 3’-end sequencing allows for some level of isoform deconvolution, allowing analyses to distinguish the kinetics of alternative 3’-UTR isoforms [RN240]. This is useful as 3’-UTRs are well established hubs of post-transcriptional regulation (Caput et al. 1986; Mayr 2017; Siegel et al. 2022).\nAlternatively, NR-seq can be combined with any standard full-length RNA-seq prep. TimeLapse-seq originally made use of this strategy, and it has been used with other recoding chemistries as well (Schofield et al. 2018; Zuckerman et al. 2020; Steinbrecht et al. 2024). While lacking some of the unique advantages of 3’-end NR-seq, full-length NR-seq provides more information. For one, the dynamics of pre-mRNA and mature mRNA can be assessed in the same sample with full-length RNA data (Vock et al. 2025). In addition, I developed strategies using full-length RNA NR-seq data to infer the turnover and synthesis kinetics of individual transcript isoforms (Mabin et al. 2025). I worked with Bobby Hogg’s lab at the NIH to apply this approach to the study of NMD-sensitive isoforms, many of which are not the result of alternative 3’-UTR usage and thus invisible to 3’-end NR-seq (Kurosaki, Popp, and Maquat 2019).\nDespite the common use cases, the power of NR-seq lies in its ability to be combined with myriad library preparation strategies. For example, NR-seq has been combined with small RNA sequencing to probe the dynamics of miRNA. It has also been combined with a number of unique library preparation strategies to assess the dynamics of many different biological processes (see section on extensions at the end) (Reichholf et al. 2019). Thus, users have copious flexibility when deciding what RNA they want to sequence in an NR-seq experiment.\n\n\nLabeling and sequencing depth\nWhen designing and NR-seq experiment, there are 3 major considerations: 1) how deeply to sequence, 2) what concentration of s4U to use, and 3) how long to label with s4U.\nThe answer to the first is somewhat trivial: as deep as you can comfortably afford. Sequencing coverage has a significant impact on the accuracy of NR-seq analyses, and thus maximizing coverage is beneficial (Figure 6). That being said, high quality, reproducible estimates of RNA half-lives have been obtained from a wide range of sequencing depths. If you are looking to analyze specific feature classes though, like splice junctions, exonic bins, or transcript isoforms, depth can be an even more important consideration (Mabin et al. 2025). On the other hand, if performing 3’-end NR-seq, equally confident estimates can be obtained with less depth due to the lower complexity of these libraries.\nOptimization of s4U labeling conditions is a more subtle challenge. First, it is important to assess the extent to which s4U is getting incorporated into the system that you are studying. This can be done through low-throughput assays such as TAMRA dot blots, or small scale sequencing experiments (Moon et al. 2024). We have found there to be a significant amount of system-to-system variability in s4U incorporation rates. A standard s4U concentration of 100 uM has proven effective in a wide array of settings. Despite this, some cell liens may take up s4U far more readily, to the point that it is best to significantly decrease s4U concentrations to avoid cytotoxic effects (Burger et al. 2013). Others may require much higher concentrations to get appreciable incorporation rates. It is thus important to assess the s4U incorporation tendencies in any new system in which you have not previously performed NR-seq.\nThe length of time to label with s4U is the final major consideration for an NR-seq experiment. Statistical arguments have been made in favor of a label time around the median half-life of the RNAs you are interested in studying (Uvarovskii and Dieterich 2017). For human mRNA, this is around 4 hours (Tani and Akimitsu 2012; Wada and Becskei 2017). This argument reflects the fact that the label time establishes the dynamic range of your half-life estimation. RNAs with half-lives much longer than your label time will have nearly undetectable levels of labeling, and those with half-lives much shorter than your label time will be almost completely labeled. A naive estimate of the dynamic range can be made by noting that in the most extreme case, you can claim that you have a single labeled or unlabeled read, making the range of meaningful half-life estimates:\n\\[\n\\begin{gather}\n\\text{Minimum} \\text{ t}_{1/2} \\text{ estimate} = \\frac{\\text{ln}(2)\\cdot \\text{tl}}{-\\text{ln}(1 - \\frac{\\text{N} - 1}{\\text{N}})} \\\\\n\\text{Maximum} \\text{ t}_{1/2} \\text{ estimate} = \\frac{\\text{ln}(2)\\cdot \\text{tl}}{-\\text{ln}(1 - \\frac{\\text{1} }{\\text{N}})} \\\\\n\\text{N} = \\text{number of reads}\n\\end{gather}\n\\]\nIn practice, uncertainties in the half-life estimate get larger the further the true half-life is from the label time (Figure 6).\n\n\n\n\n\n\nFigure 6: Estimate accuracy as a function of NTR. EZbakR’s SimulateOneRep() function was used to simulate data for 10,000 features, all of which were given 100, 1000, or 10000 reads. Features had a range of NTR values, evenly spaced from 0.01 to 0.99. NTRs were estimated with EZbakR. Top: comparison of EZbakR NTR estimates to the simulated truth (both on a logit-scale). Bottom: The logit(NTR) uncertainty as a function of NTR. The uncertainty is computed from the Hessian and can thus be thought of as how steeply the likelihood falls off around the maximum likelihood value. Larger values represent slow decreases in likelihood and high uncertainty (lots of logit(NTR) estimates explain the data reasonably well).\n\n\n\nDespite these theoretical arguments, shorter label times are usually preferable to longer ones. Long-term exposure to s4U can have cytotoxic effects, so minimizing this risk is advisable (Burger et al. 2013; Altieri and Hertel 2021). In addition, dropout may be more likely the higher proportion of your RNA that is labeled. Because of this, we suggest a slight deviation from theoretical statistical optimality and propose that an optimal label time close to, but shorter than, the median half-life of the RNA that you are interested in probing.\nFinally, the best-case scenario is to have multiple different label times (e.g., 1 hour, 2 hour, 4 hours) in each biological condition. Having multiple label times expands the dynamic range of your half-life estimation (Uvarovskii and Dieterich 2017; Rummel, Sakellaridi, and Erhard 2023). In addition, grandR implements a strategy that uses multiple label times to correct for biases that can arise due to gradual ramp up in s4U availability over time (Rummel, Sakellaridi, and Erhard 2023). It assumes that the longest label time is closest to be the ground-truth “effective” label time (this is because the s4U ramp-up time is the smallest percentage of that label time), and identifies adjusted label times for the other time points that yield, on average, estimates in agreement with the longest label time. More generally, having multiple label times is useful for assessing the extent to which models assumed in your NR-seq analysis accurately describe the dynamics of RNAs assayed.\n\n\nPulse-label vs. pulse-chase\nThere are two ways to perform the labeling in an NR-seq experiment: a pulse-label and a pulse-chase (Figure 7). Pulse-labeling refers to treating cells with metabolic label for a certain period of time, after which RNA is extracted, treated with nucleotide recoding chemistry, and sequenced. In a pulse-chase, labeling is typically done for a much longer period of time, followed by a chase with the unmodified nucleotide (e.g., uridine). RNA is extracted after the chase.\nWhile technically equivalent in terms of the information provided, a pulse-label design provides a number of advantages over a pulse-chase design. Pulse-chases necessitate extended exposure of cells to s4U. This can have serious cytotoxic effects that risk biasing your analyses. In addition, the metabolic label can be recycled from degraded RNA during the chase and returned to the NTP pool, complicating analysis of this data (this compounds the orthogonal problem of imperfect washout of metabolic label during the chase) (Neymotin, Athanasiadou, and Gresham 2014; Nikolov and Dabeva 1985). Finally, estimating kinetic parameters in a pulse-chase requires estimating the NTR after the pulse and the chase, and comparing these NTR values. For stable RNA, this can yield ambiguous estimates when the estimated NTR after the chase is lower than that after the pulse (e.g., due to experiment noise, estimation uncertainty, or experimental biases). In contrast, pulse-label designs can limit a cell’s exposure to s4U, aren’t confounded by label recycling, and can provide estimates of RNA turnover kinetics from NTR estimates in a single sample. Because of this, we strongly suggest preferring a pulse-label to a pulse-chase design. This suggestion is further backed up by a recent meta-analysis I performed of published NR-seq datasets, where I found that estimates from pulse-label NR-seq experiments were far more consistent across cell lines, labs, method variations, etc. than those from pulse-chases (Vock et al. 2025).\n\n\n\n\n\n\nFigure 7: Two NR-seq labeling strategies. Pulse-labeling refers to labeling cells for a certain amount of time, after which RNA is extracted. A pulse-chase refers to labeling cells for (typically) a much longer period of time, chasing with the regulate nucleotide (e.g., uridine), and extracting RNA after the chase. These two designs are symmetric (the unlabeled RNA in one behaves like the labeled RNA in another). Despite this, pulse-labels provide a number of technical advantages to a pulse-chase, and thus should be the default NR-seq labeling strategy (see text for discussion)."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#nr-seq-extensions",
    "href": "posts/nrseq_deepdive/index.html#nr-seq-extensions",
    "title": "NR-seq: an in-depth review",
    "section": "NR-seq extensions",
    "text": "NR-seq extensions\nNucleotide recoding has not only been used to augment bulk RNA-seq. A variety of other sequencing methods have been combined with metabolic labeling and nucleotide recoding. These approaches are opening up exciting avenues for studying various aspects of RNA biology. In this section, I briefly review the universe of current NR-seq extensions.\n\nTT-NR-seq\nOriginally, s4U metabolic labeling was combined with biochemical enrichment of labeled RNA (Cleary et al. 2005; Dölken et al. 2008; Schwalb et al. 2016). While the limitations of this approach inspired the development of nucleotide recoding approaches, nucleotide recoding also provides a unique route by which to improve these methods. This is exciting as enrichment-based methods are still useful for studying rapid processes, such as co-transcriptional splicing and cleavage and polyadenylation (Wachutka et al. 2019; Torres-Ulloa, Calvo-Roitberg, and Pai 2024). A major challenge in analyzing these enrichment-based metabolic labeling data though is the inevitable presence of unlabeled RNA contamination in the enriched RNA population. The magnitude of biases introduced by this contamination is greater in settings where very short label times are required, precisely the settings in which enrichment-based strategies are still needed. Applying nucleotide recoding to this data has the potential to allow users to bioinformatically filter out unlabeled RNA contamination (Schofield et al. 2018; Reichholf et al. 2019).\nIn theory, standard mixture modeling can be applied to TT-NR-seq data so as to estimate the amount of reads from a given feature that come from unlabeled RNA contamination. In practice, the extent to which these models are fully appropriate in this setting is underexplored. Concerns about overdispersion of mutation rate distributions are particularly valid in settings with very short label times, as s4U concentrations are likely actively increasing during the labeling. Thus, regions of an RNA produced early in the labeling are likely exposed to lower concentrations than those produced later in the labeling. This means that instead of there being a single new read mutation rate, there is a potentially broad distribution of mutation rates. Strategies to model this distribution could both address this problem and provide interesting orthogonal information about the timing and kinetics of various processes.\n\n\nSTL-seq\nIn metazoans, transcription initiation at protein coding genes is often followed by promoter-proximal pausing, where RNA Pol II halts transcription 20-60 nts downstream of the TSS and awaits further signal to continue into productive elongation or prematurely terminate (Rougvie and Lis 1988; Gilmour and Lis 1986; Core and Adelman 2019). To probe the steady-state levels of paused polymerase, Start-seq was developed (Nechaev et al. 2010). This approach involves sequencing short, capped RNAs housed within the paused polymerase. To assess the kinetics of pause departure and initiation, the Simon lab combined this approach with nucleotide recoding, in a method termed Start-TimeLapse-seq, or STL-seq (Zimmer et al. 2021).\nSTL-seq has provided unique insights regarding the kinetics of promoter-proximal pausing. There are number of analysis challenges and limitations to consider when analyzing STL-seq data though. For one, unlike vanilla NR-seq’s deconvolution of synthesis and degradation kinetics, STL-seq does not completely solve the problem of kinetic ambiguities plaguing steady-state analyses of pausing. The paused polymerase has two distinct potential fates: release into productive elongation and premature termination (Kamieniarz-Gdula and Proudfoot 2019). Thus, the pause site departure kinetics estimated by STL-seq represent a combination of the kinetics of these two processes. Fully resolving the kinetics of pausing requires combining STL-seq with inhibition of promoter-proximal pause release via drugs like flavopiridol. This has the downside of being a harsh perturbation that can affect the kinetics of the processes being studied, the very same downside that partially inspired the development of metabolic labeling methods in the first place.\nSTL-seq also presents some unique analysis challenges. For one, STL-seq reads are fairly short (&lt; 80 nucleotides, as short as 20). Aligning these short reads is made more challenging by the chemically induced T-to-C mismatches. 3-base alignment was originally shown to significantly improve the recovery of high mutation content reads (Zimmer et al. 2021). Despite this, rigorous benchmarks of different alignment strategies for STL-seq data have not yet been developed. Drawing inspiration from SLAMDUNK’s 3’-UTR alignment strategy, we suspect that a strategy using no-label data to infer TSS sequences and aligning s4U labeled data to this constrained sequence space may provide some advantages. In addition, using NextGenMap and its custom T-to-C mismatch penalization function may provide further advantages.\nIn addition, mixture modeling of STL-seq data may suffer from the same challenges faced by mixture modeling of TT-NR-seq data, due to the necessarily short label times (~5 minutes). The discussion in that section about the potential of modeling the distribution of new read mutation rates hold here as well. In summary, further innovation of the processing and analysis of STL-seq data may provide significant dividends.\n\n\nDual-labeling and TILAC\nWhile NR-seq experiments typically make use of s4U, s6G has also shown to be compatible with these methods (Kiefer, Schofield, and Simon 2018; Gasser et al. 2020). This has opened the door for dual-label designs. While EZbakR and GRAND-SLAM implement strategies to fit mixture models to this general class of methods, the potential use cases of dual-labeling are currently underexplored.\nTo my knowledge, the only existing specialized dual-labeling method currently published is TILAC (Courvan et al. 2022). TILAC draws inspiration from the proteomics methods SILAC (Mann 2006) to make use of dual-labeling for rigorous normalization of complex sequencing experiments. TILAC involves mixing s4U labeled cells with s6G labeled cells. The idea is that these two populations represent distinct biological conditions that you are comparing gene expression levels in. The relative abundances of reads from s4U and s6G labeled RNA from a given gene in this experiment provides information about the relative expression of that gene in the two conditions. This allows for rigorous normalization without the need for spike-ins or statistical assumptions.\nTILAC is an exciting paradigm for the normalization of complex sequencing experiments. Despite this, it and other potential dual labeling approaches are not without their unique challenges. s6G is more cytotoxic than s4U, raising the concern of adverse effects due to labeling (Waters and Swann 1997; Yuan and Wang 2008). In addition, s6G may not be readily incorporated into the NTP pool to the same extent that s4U is. Finally, for true dual labeling approaches (i.e., labeling the same population of RNA with both s4U and s6G), we have found that incorporation of these two labels may interfere with one another. This combined with the unique bioinformatic challenge of aligning reads with high levels of both T-to-C and G-to-A conversions makes generating high quality, dual labeled NR-seq data challenging. Despite this, we suspect that strategies to address these challenges could open novel and exciting routes for finer-grained dissection of RNA kinetics (e.g., investigation of RNA aging dynamics).\n\n\nNascentRibo-seq\nApproaches like Ribo-seq are commonly used to infer the efficiency of translation initiation (Ingolia et al. 2009). Despite this, these analyses are fundamentally plagued by kinetic ambiguities that require strong assumptions to interpret in this way. Thus, a method termed Nascent Ribo-seq was developed, which combines nucleotide recoding with Ribo-seq (Schott et al. 2021). This approach allowed the authors to assess the kinetics of polysome assembly. This approach proved particularly useful in settings where cells are actively responding to a perturbation, as the active regulation of mRNA levels would bias standard Ribo-seq analyses of ribosome loading efficiency.\n\n\nscNR-seq\nOne of the most active areas of NR-seq extensions is its application to single-cell RNA-seq. A number of distinct approaches have been developed that combine scRNA-seq approaches with s4U metabolic labeling and nucleotide recoding (Erhard et al. 2019; Hendriks et al. 2019; Cao et al. 2020; Q. Qiu et al. 2020; Lin et al. 2023; Maizels, Snell, and Briscoe 2024). This includes work to further optimize the scalability, ease-of-implementation, and conversion efficiency of these protocols (X. Zhang et al. 2025).\nIn addition, scNR-seq has seen a number of exciting bioinformatic developments. Work from Jonathan Weissman’s showed that metabolic labeling scRNA-seq approaches have the potential to significantly improve analyses of RNA velocity (X. Qiu et al. 2022). This paper also introduced a pipeline for processing scNR-seq data, dynast (GRAND-SLAM is the only other existing tool that provides support for single-cell specific processing tasks). This has spurred several groups to improve upon their initial metabolic labeling-based velocity analyses (Maizels, Snell, and Briscoe 2024; Peng, Qiu, and Li 2024). This route is particularly promising, as it theoretically eliminates the need to rely on low and biased coverage for intronic regions of genes in most scRNA-seq libraries.\nscNR-seq is also opening up brand new classes of analyses. Recently, Florian Erhard’s group developed a combined experimental and bioinformatic approach making use of scNR-seq to infer cell type-specific responses to perturbations (Berg, Sakellaridi, et al. 2024). The idea is to feed cells the metabolic label at the same time that they are treated with a drug or perturbed in some other way. By inferring the degradation kinetics of RNA in a given cell (or group of similar cells) after the perturbation, the starting levels of each RNA (i.e., before application of the perturbation) can be inferred. This allowed them to identify how each cell type present in the starting population uniquely responded to the treatment. Using a non-linear causal inference strategy, they were also able to identify gene’s whose regulation were driving the larger changes seen (Chernozhukov et al. 2018; Berg, Sakellaridi, et al. 2024).\nThis approach, implemented in the new R package HetSeq, showcases how innovative experimental design and data analysis can make use of nucleotide recoding to unlock new avenues for biological discovery. Despite the theoretical promise of this approach, inferring the starting levels of RNA in these experiments is incredibly challenging. The steady-state assumption is almost inevitably violated. While we have discussed approaches to achieve unbiased parameter estimation in non-steady-state contexts, HetSeq requires an experimental design fundamentally incompatible these strategies (HetSeq estimates the quantity that needs to be measured for non-steady-state analyses, the initial RNA levels) (Narain et al. 2021). In addition, single cell data is notably sparse, making accurate estimation of cell type specific NTRs (and thus degradation rate constants) difficult. While GRAND-SLAM and HetSeq implement a number of strategies to mitigate these challenges, further denoising of the analysis may be achievable through collection of matched, unperturbed scRNA-seq data. This data could be used to generate an atlas of initial cell states that the noisy inferred cell states are mapped onto (Lotfollahi et al. 2022; Lotfollahi et al. 2024).\n\n\nPerturbSci-Kinetics\nMany biologists using NR-seq methods are not necessarily interested in how the kinetics of RNA synthesis and degradation change when cells are perturbed. Approaches like Perturb-seq have provided an unprecedented, high-throughput look at the impact of genetic perturbations on gene expression (Dixit et al. 2016). To dissect how regulation of transcription and RNA decay underlies these changes, Perturb-seq was recently combined with nucleotide recoding, in an approach termed PerturbSci-Kinetics (Z. Xu et al. 2024). While the method and its bioinformatic toolkit is in its nascence, it represents a paradigm shift in the scale at which we can investigate the mechanisms of gene expression regulation.\n\n\nTether-seq\nThe Simon lab recently developed an approach that combines a TT-NR-seq-style experiment with single-nucleotide bioinformatic analyses to identify sites of small molecule binding across the transcriptome (Moon et al. 2024). This approach, termed Tether-seq, relies on treating cells with small molecule drugs containing a disulfide moiety, and using this handle to covalently bind the small molecule to RNAs with which it associates. Nucleotide recoding is then used to identify the exact sites to which the small molecule tethered. This approach facilitates the identification of weak binders, while also establishing an exciting paradigm for single-nucleotide NR-seq analyses.\n\n\nLong read NR-seq\nCombining nucleotide recoding with long read sequencing technologies presents a promising avenue by which to dissect the isoform-specific kinetics of gene expression. While it is possible to glean insights about the synthesis and degradation kinetics of individual transcript isoforms from short read NR-seq (Mabin et al. 2025), long read technologies can open up investigation of isoform-specific dynamics largely invisible to short read data (e.g., coupling of TSS and TES usage) (Amarasinghe et al. 2020). There is currently only one described attempt at long read NR-seq, in which TUC chemistry has been combined with PacBio long read sequencing (Rahmanian et al. 2020).\nDespite this, some long read sequencing technologies provide avenues by which to perform enrichment-free metabolic labeling analyses without nucleotide recoding chemistry. In particular, nanopore direct RNA sequencing has been used by several groups to directly distinguish labeled from unlabeled RNA (Maier et al. 2020; Drexler et al. 2021; Jain et al. 2022). These approaches have utilized a range of machine learning architectures, often relying on convolutional neural networks, to achieve this end. While distinguishing labels like s4U and 5-EU in nanopore current signal is challenging, these approaches are promising avenues towards high fidelity dissection of isoform-specific kinetics. This approach was even recently combined with subcellular fractionation to probe the kinetics of more than just bulk RNA synthesis and degradation (Coscujuela Tarrero et al. 2024). Thus, long read NR-seq and direct RNA sequencing of metabolically labeled samples will likely be a powerful tool in dissecting the dynamics of transcript isoforms."
  },
  {
    "objectID": "posts/nrseq_deepdive/index.html#conclusions",
    "href": "posts/nrseq_deepdive/index.html#conclusions",
    "title": "NR-seq: an in-depth review",
    "section": "Conclusions",
    "text": "Conclusions\nNR-seq represents a powerful paradigm shift in the study of RNA dynamics. Making full effective use of these methods requires rigorous analysis strategies robust to technical and biological variance as well as biases that can plague these experiments. While a number of tools have been developed to implement these strategies, new NR-seq extensions often demand unique solutions to these problems. Thus, it is important that users and developers of NR-seq technologies are aware of the dos and don’ts of NR-seq data processing and analysis.\nIn this review, I have laid out the tenets of a solid NR-seq analysis foundation. This includes rigorous modeling of the mutational data in NR-seq reads, awareness regarding biases such as dropout that can plague these data, accurate kinetic modeling of the labeled and unlabeled RNA abundances, proper alignment of conversion containing reads, and experimental design decisions intended to make all of the above easier. I urge NR-seq users and developers to follow the roadmap laid out here so as to avoid misusing and misinterpreting their data."
  },
  {
    "objectID": "posts/growth_and_velocity/index.html",
    "href": "posts/growth_and_velocity/index.html",
    "title": "On ‘Fundamental errors in RNA velocity arising from the omission of cell growth’",
    "section": "",
    "text": "Recently, a preprint made some interesting claims about a widespread flaw in the analysis of metabolic labeling RNA-seq/scRNA-seq data. I’ve worked extensively in this space and have written in-depth reviews of metabolic labeling analysis strategies, and thus my interest was piqued. After reading the preprint a couple times, I have a lot of thoughts."
  },
  {
    "objectID": "posts/growth_and_velocity/index.html#introduction",
    "href": "posts/growth_and_velocity/index.html#introduction",
    "title": "On ‘Fundamental errors in RNA velocity arising from the omission of cell growth’",
    "section": "",
    "text": "Recently, a preprint made some interesting claims about a widespread flaw in the analysis of metabolic labeling RNA-seq/scRNA-seq data. I’ve worked extensively in this space and have written in-depth reviews of metabolic labeling analysis strategies, and thus my interest was piqued. After reading the preprint a couple times, I have a lot of thoughts."
  },
  {
    "objectID": "posts/growth_and_velocity/index.html#tldr",
    "href": "posts/growth_and_velocity/index.html#tldr",
    "title": "On ‘Fundamental errors in RNA velocity arising from the omission of cell growth’",
    "section": "TLDR",
    "text": "TLDR\nIn summary, there are some things I really like about this work, and even more things that I am critical of.\n\nThe Good\n\nThis work accurately points out that cellular growth introduces deviations from steady-state in bulk RNA dynamics. This risks biasing standard analyses of RNA degradation kinetics, but as I discuss in The Bad, this bias will often be pretty small.\nRNA velocity analyses have gotten pretty complicated. There are a litany of ideas of how to take a collection of gene-wise velocity estimates and do things like estimate vector fields, map cell state dynamics, and predict the impacts of perturbations. Instead of tackling all of this complexity head on, the authors focus on a simpler, sometimes neglected, part of the pipeline: actually estimating the gene-wise velocities. I suspect that there are a ton of challenges that need to be intelligently tackled at this first stage, so more work like this is needed.\nThe authors focus on metabolic labeling-based velocity analyses, which promise to overcome some fundamental weaknesses of splicing-based analyses (e.g., reliance on biased or low intronic coverage, in ability to estimate absolute velocities).\n\n\n\nThe Bad\n\nThe authors put forth a flawed analysis workflow that is not accurately reflective of how previous work has formalized estimating velocities with metabolic labeling data. They model absolute molecular content of cells, and assume that UMI counts represent accurate estimates of these quantities.\nThe authors seem unaware of an existing strategy to address the growth-induced non-steady-state dynamics.\nGrowth rates assumed throughout the work are fairly high (12 hours is number they often quote). Growth rates differ from context-to-context, but even the fastest growing cell lines that researchers often work with have doubling times on the order of days. Since these typical growth rates are on much longer time scales than RNA degradation (median half-life &lt; 4 hours), as well as the labeling done in these experiments, the impact of growth-induced non-steady-state dynamics are usually going to be quite limited.\nEven when growth rates matter though, its affect is global. That is, the growth rate is a global parameter that affects all bulk RNA synthesis kinetics equally. Looking beyond RNA velocity analyses for a minute, many investigations of RNA turnover kinetics using metabolic labeling data only need the ordering of RNA degradation kinetic estimates to be accurate. If there are some biases in the global scale of informed RNA degradation, this can be normalized out when comparing estimates across cell lines, training ML models on degradation rate constant estimates, etc. There are plenty other, much larger sources of such bias (dropout, label concentration ramp up), which can be similarly navigated.\nThe discussion of several aspects of metabolic labeling experiments is a bit naive or ill-informed.\n\n\n\nThe Ugly\nNot necessarily worse than anything covered in The Bad, but I couldn’t resist the movie reference.\nIt’s obvious that the analysis framework but forth by the authors is deeply flawed and not representative of how these data are typically analyzed, in that they seemed to have tried to apply it to their own data only for the facade to crumble:\n\nWe ran a pulse-chase experiment with an additional dye allowing us to identify cells that had not divided during the chase (Methods). … However, the scRNA-Seq results do not display a concordant increase in mRNA (UMI) counts per cell, even after correction for sequencing depth (Figure S5B-D; Methods), including in a repeated experiment. We interpretate [sic] this difference as a technical problem, rather than as an accurate representation of the biology. Because the observed counts are not scaling appropriately with true counts, it is not possible to cleanly estimate the isolated degradation rate, as hoped. More work is needed, therefore, to develop technology or methods to isolate degradation and dilution effects."
  },
  {
    "objectID": "posts/growth_and_velocity/index.html#my-criticisms",
    "href": "posts/growth_and_velocity/index.html#my-criticisms",
    "title": "On ‘Fundamental errors in RNA velocity arising from the omission of cell growth’",
    "section": "My criticisms",
    "text": "My criticisms\nHere I go in to more depth about some of the grievances covered above.\n\nA flawed analysis paradigm\nThe authors put forth an analysis framework that they claim is representative of how metabolic labeling data should be analyzed, at least for the purposes of RNA velocity analyses. In summary:\n\nThe authors put forth a reasonable and standard model of labeled and unlabeled RNA abundance. The key observation is that the bulk synthesis rate is a function of the number of cells, and this number increases due to cell growth. Formalized:\n\n\\[\n\\begin{gather}\n\\frac{dR}{dt} = k_{syn}(t) - k_{deg} \\cdot R \\\\\nk_{syn}(t) = n \\cdot k_{syn}^{sc} \\\\\nk_{syn}^{sc} := \\text{Average transcription rate in a single-cell}\n\\end{gather}\n\\]\n\nThe authors then propose to effectively fit said model to data on absolute single-cell molecular abundances.\n\nStep 2 is where some of the challenges arise. First off, how does one even estimate the absolute counts of molecules in a single cell? Given the quote in The Ugly in the above section, it seems that the authors assume that UMI counts are the key to this. This is a flawed assumption. If sequenced to saturation (i.e., you sequence every unique molecule), then UMI counts are a measure of the absolute molecular content of that which made it to the sequencer. Absolute molecular content that got sequenced is not the same as absolute molecular content of the cells. The authors seem to have learned the hard way that UMI counts can vary for entirely technical reasons that prevent their use as truly quantitative metrics of RNA molecule counts in a cell.\nHow are metabolic labeling datasets actually analyzed then? I would argue that there are two “gold-standard” analysis strategies:\n\nSteady-state NTRs: In this approach, the ratio of labeled (new) to total RNA is analyzed (discussed here). At steady-state, there is a clean relationship between this quantity and turnover kinetics. While a single-cell is rarely at steady-state, the ability to rely on an internally normalized ratio computed on a per-sample basis has a number of statistical advantages, as discussed here.\nNon-steady-state NTRs: An NTR is calculated, but using two samples (discussed here). A sample subjected to metabolic labeling is used to estimate the labeled RNA abundance, and a sample collected at the start of labeling is used to estimate the initial total RNA abundance. RNA abundance estimates come from normalized read counts (e.g., using normalization approaches such as median-of-ratios). Such normalization approaches should be robust in this case, as it is unlikely that there are radical changes in RNA content between the start and end of labeling, especially when using a label time of around 2 hours.\n\nThe author’s approach is close in essence to 2. But if properly applied, any of the growth rate-induced biases should be eliminated. Thus, I suspect that their choice to think about things on the scale of absolute molecular abundances is leading to the biases they find in their approach when analyzing simulated data.\nIt is worth noting that many previous studies using metabolic labeling to perform scRNA-seq have relied on something closer to the steady-state NTR approach.\n\n\nFlawed discussions of metabolic labeling data\n\n\nIgnoring the bigger problems"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac Vock",
    "section": "",
    "text": "I am a post-doc at Stanford University in the lab of Anshul Kundaje. I am currently working on developing sequence-to-function models, with a particular focus on models that will help us dissect the mechanisms of post-transcriptional regulation.\nIn my PhD, I developed a number of bioinformatic tools for processing and analyzing nucleotide recoding RNA-seq (NR-seq) experiments. NR-seq is a class of methods for quantifying the kinetics of RNA synthesis and degradation, and includes methods like TimeLapse-seq (developed in the Simon lab), SLAM-seq, TUC-seq, etc. These are powerful tools that are allowing us to study the dynamic lives of RNA at unprecedented resolution, and I have had a lot of fun dreaming up ways in which we can extract exciting biological insights from NR-seq data.\nI am also a statistician, and have developed a number of novel methods for analyzing NR-seq data. One of my passions is helping biologists better understand the statistical methods powering their favorite bioinformatic tools. This led me to develop a course at Yale called “Statistical Intuition for Modern (RNA) Biochemists”, which ran as part of the 2023-2024 cohort of the Associates in Teaching Program. I taught the course in the Spring of 2025; course evals can be found here. It was inspired by Susan Holmes and Wolfgang Huber’s book of a similar title and Richard McCelreath’s cultural phenomenon “Statistical Rethinking”. I wanted the course to be an accessible introduction to statistics for biochemistry majors. We covered the basic machinery of statistical modeling and its use in popular methods like linear modeling and clustering. Rather than relying on mathematical formalism to convey these concepts though, the course made use of simulations and interactive exercises to help students develop an intuition for key concepts. Finally, the course started by introducing student’s to RNA-seq, so as to establish a common data language that we could connect all concepts back to. All of the course material is hosted at this repo."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Isaac Vock",
    "section": "Education",
    "text": "Education\n\n\nYale University | New Haven, CT | August 2019 - August 2025\nPhD in Molecular Biophysics and Biochemistry\n\n\nCentre College | Danville, KY | August 2015 - May 2019\nB.S. in Physics, minor in Math"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Isaac Vock",
    "section": "Experience",
    "text": "Experience\n\n\nProgramming in R (since 2019)\nScripting (example repo)\nR package development (most notably, EZbakR)\nShiny app development (e.g., RNAdecayCafe)\n\n\nSnakemake pipeline development (since 2021)\nfastq2EZbakR: Flexible processing of NR-seq data.\nNRsim: Simulating NR-seq data to test analysis strategies and pipelines.\nAnnotationCleaner: Assembling annotations using StringTie and some custom scripts.\nTHE_Aligner: Aligning almost any kind of RNA-seq data.\nPROseq_etal: PRO-seq/ChIP-seq/ATAC-seq pipeline.\n\n\nOther experience\nPython (since 2021; used throughout Snakemake pipelines listed above)\nC (since 2023; example repo)\nPytorch (since 2023; example repo)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "My main projects",
    "section": "",
    "text": "Presents a first-of-its-kind database of high quality RNA stability estimates from pulse-label NR-seq experiments. Data can be accessed here.\nPaper presents a meta-analysis that shows how pulse-label NR-seq data is often highly consistent across labs, cell lines, method variations, etc. In contrast, we found pulse-chase NR-seq data to be batch effect-ridden and difficult to consistently interpret.\nWe used this data to highlight the sometimes underappreciated role of RNA stability in tuning gene expression levels within a given cell line and in establishing differences across cell lines.\n\nLink to preprint"
  },
  {
    "objectID": "about.html#rnadecaycafe",
    "href": "about.html#rnadecaycafe",
    "title": "My main projects",
    "section": "",
    "text": "Presents a first-of-its-kind database of high quality RNA stability estimates from pulse-label NR-seq experiments. Data can be accessed here.\nPaper presents a meta-analysis that shows how pulse-label NR-seq data is often highly consistent across labs, cell lines, method variations, etc. In contrast, we found pulse-chase NR-seq data to be batch effect-ridden and difficult to consistently interpret.\nWe used this data to highlight the sometimes underappreciated role of RNA stability in tuning gene expression levels within a given cell line and in establishing differences across cell lines.\n\nLink to preprint"
  },
  {
    "objectID": "about.html#isoform-stability-quantification",
    "href": "about.html#isoform-stability-quantification",
    "title": "My main projects",
    "section": "Isoform stability quantification",
    "text": "Isoform stability quantification\n\nSummary\n\n\n\nDeveloped method to quantify the stabilities of individual transcript isoforms from full-length (as opposed to 3’-end) NR-seq data.\nUsed this method to rigorously identify and study the properties of NMD-sensitive transcript isoforms.\nDeveloped and interpreted a simple classification model that identified features which appeared to be major drivers of NMD sensitivity.\nIdentified a unique class of alternative splicing coupled NMD that did not involve premature termination codons.\n\nLink to preprint"
  },
  {
    "objectID": "about.html#the-ezbakr-suite",
    "href": "about.html#the-ezbakr-suite",
    "title": "My main projects",
    "section": "The EZbakR-suite",
    "text": "The EZbakR-suite\n\nSummary\n\n\n\nIntroduced Snakemake pipeline (fastq2EZbakR) and R package (EZbakR)\nGeneralizes feature assignment: the set of annotated genomic features that you can perform analyses on.\nGeneralizes mixture modeling: how the fraction of reads from labeled RNA are estimated.\nGeneralizes dynamical systems modeling: how kinetic parameters are estimated from NR-seq data.\nGeneralizes comparative analyses: how kinetic parameters are compared across biological conditions.\n\nLink to paper"
  },
  {
    "objectID": "about.html#bakr",
    "href": "about.html#bakr",
    "title": "My main projects",
    "section": "bakR",
    "text": "bakR\n\nSummary\n\n\n\nIntroduced an R package (bakR) designed to compare RNA synthesis and degradation rate constants across biological conditions.\nAlso introduced a hierarchical modeling strategy to increase statistical power of these comparisons.\n\nLink to paper"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Isaac’s blog",
    "section": "",
    "text": "On ‘Fundamental errors in RNA velocity arising from the omission of cell growth’\n\n\n\nnrseq\n\n\n\n\n\n\n\n\n\nDec 30, 2025\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nThe future of the EZbakR-suite\n\n\n\nnrseq\n\n\n\n\n\n\n\n\n\nDec 11, 2025\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nNR-seq: an in-depth review\n\n\n\nnrseq\n\n\n\n\n\n\n\n\n\nNov 18, 2025\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to ggplot, dplyr, and pivoting\n\n\n\nstats\n\n\n\n\n\n\n\n\n\nJan 23, 2025\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to R: Day 1\n\n\n\nstats\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing NR-seq data: the basics\n\n\n\nnrseq\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing NR-seq by simulating NR-seq\n\n\n\nnrseq\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnrseq\n\nstats\n\n\n\n\n\n\n\n\n\nDec 13, 2024\n\n\nIsaac Vock\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ezbakR_update/index.html",
    "href": "posts/ezbakR_update/index.html",
    "title": "The future of the EZbakR-suite",
    "section": "",
    "text": "With the first official CRAN release of EZbakR (version 0.1.0) and the (unrelated) release of fastq2EZbakR version 0.8.0, I decided to write this brief blog to discuss the present and future of the EZbakR suite.\nEZbakR version 0.1.0 isn’t an update as much as it is a more official roll out. This is EZbakR’s first CRAN release, and the first release I have assigned an official version to. Thus, in this post I will mainly discuss my future plans for EZbakR. fastq2EZbakR version 0.8.0 introduces an exciting piece of novel functionality, so I will discuss this change as well as future functionality.\n\n\nAs mentioned above, the first official CRAN release of EZbakR is more symbolic than it is an actual major release. There have not been major expansions to EZbakR’s core functionality for several months. The CRAN submission process was just meant to force me to clean up various technical aspects of the code base and settle into an official versioning scheme, both of which are signals for a more consistent development cycle to come.\nTo keep this brief, the following is a nearly exhaustive list of functionalities that I would currently like to implement in EZbakR over the next year:\nExtensive model fit assessment. This will include assessing mixture model fits (i.e., how well does a two-component binomial mixture model fit your data; fit assessed for each +label sample) as well as dynamical systems model fits (i.e., how well does your assumed model of RNA dynamics fit your data; fit assessed across multiple different label times).\n\nSignificance: This will hopefully facilitate filtering out problematic features plagued by various artifacts.\n\nAlternative mixture models. It is currently unclear what this will specifically look like, but I will be exploring various iterations of models of overdispersion and other deviations from binomially distributed mutational data.\n\nSignificance: These alternative models will likely be useful for applications involving shorter label times, where overdispersion can become a bigger problem.\n\nExpanded data visualization capabilities.\n\nSignificance: NR-seq data is rich and EZbakR’s current visualization toolkit is limited.\n\nRefinement of existing functionality. Namely, I will be exploring potential improvements to the generalized linear modeling, generalized dynamical systems modeling, and transcript isoform modeling performed by EZbakR.\n\nSignificance: Small but potentially impactful improvements to the unique analysis strategies currently implemented by no other tool but EZbakR.\n\nSingle-cell NR-seq compatibility, paired with updates to fastq2EZbakR.\n\nSignificance: The last couple years has seen a relative explosion of single-cell NR-seq methods. I think EZbakR has a lot to offer users in terms of the insights they could be extracting from this data.\n\nBetter support for single-nucleotide NR-seq analyses.\n\nSignificance: I’ve been invovlved in several collaborations that use the EZbakR-suite to analyze single-nucleotide NR-seq data (that is, they are interested in mutation rates at specific nucleotides). In fact, there is a whole universe of methods that you could lump under the NR-seq umbrella, that while not technically using metabolic labeling and nucleotide recoding chemistry, could be processed and analyzed by the EZbakR-suite (structure probing methods like DMS-MaP-seq, RNA modification mapping methods like DART-seq, RBP interaction mapping methods like TRIBE-ID, etc.)\n\nBetter support for time series analyses.\n\n\n\nfastq2EZbakR version 0.8.0 (released 12/4/2025) brought a promising brand new piece of functionality: the ability to assign reads to 3’-ends. You can read more about this strategy here.\nOver the next year, I will strive to implement the following improvements/changes:\n\nFaster and improved SNP calling. This can be the major bottleneck in some datasets. I will also be inspecting the SNP calling strategy more closely in hopes of refining several aspects of it.\nImproved TEC assignment. There are currently a couple quirky edge cases where RSEM assigns reads to the wrong transcript isoform; I would like to get rid of these mistakes. I would also like to expand compatibility of this assignment strategy to aligners other than STAR, which will require incorporating tools like mudskipper into the workflow.\nSingle-cell NR-seq compatibility. We have seen a relative explosion of single-cell NR-seq methods. While tools like GRAND-SLAM can support analyses of this data, and tools like dynast are specifically designed for this one class of NR-seq experiment, I believe that fastq2EZbakR has a lot to offer in this space. In particular, I think giving users the power to explore their mutational data more deeply and giving bioinformaticians a simple starting point for model development could prove very useful to the community.\nBetter single-nucleotide support with perbase. fastq2EZbakR can provide nucleotide-specific mutational data, but this is currently done with some old custom Python/Shell scripts that are a bit rough around the edges. I would like to clean this up, and perbase will be key to that.\n\n\n\n\nI completed my PhD in Matt Simon’s lab (where I developed the EZbakR suite) in August of 2025. I am now a post-doc in Anshul Kundaje’s lab. Despite this, maintaining the EZbakR suite has not been completely relegated to “side project” status. I am still an active user of the EZbakR suite for some of the work I have planned during my post-doc. Because of this, I am planning to stick to a roughly monthly update schedule, with new minor versions of both EZbakR and fastq2EZbakR released around the end of every month.\nA very hesitant rough draft of what the schedule of changes might look like is below:\nEnd of January:\n\nfastq2EZbakR single-cell NR-seq support\nEZbakR single-cell NR-seq support\nExpanded EZbakR data vis\n\nEnd of February:\n\nImproved EZbakR core functionality\nExpanded EZbakR data vis\nfastq2EZbakR perbase refactor\n\nEnd of March:\n\nEZbakR model fit assessment\nfastq2EZbakR improve SNP calling\n\nEnd of April:\n\nEZbakR single-nucleotide support\nfastq2EZbakR improve TEC assignment\n\nEnd of May:\n\nEZbakR time series analyses\nEZbakR alternative mixture models\n\nSuggestions for changes/new features or prioritization of features can be made on the relevant Github Issues page (EZbakR’s or fastq2EZbakR’s)."
  },
  {
    "objectID": "posts/ezbakR_update/index.html#the-ezbakr-suite-an-update",
    "href": "posts/ezbakR_update/index.html#the-ezbakr-suite-an-update",
    "title": "The future of the EZbakR-suite",
    "section": "",
    "text": "With the first official CRAN release of EZbakR (version 0.1.0) and the (unrelated) release of fastq2EZbakR version 0.8.0, I decided to write this brief blog to discuss the present and future of the EZbakR suite.\nEZbakR version 0.1.0 isn’t an update as much as it is a more official roll out. This is EZbakR’s first CRAN release, and the first release I have assigned an official version to. Thus, in this post I will mainly discuss my future plans for EZbakR. fastq2EZbakR version 0.8.0 introduces an exciting piece of novel functionality, so I will discuss this change as well as future functionality.\n\n\nAs mentioned above, the first official CRAN release of EZbakR is more symbolic than it is an actual major release. There have not been major expansions to EZbakR’s core functionality for several months. The CRAN submission process was just meant to force me to clean up various technical aspects of the code base and settle into an official versioning scheme, both of which are signals for a more consistent development cycle to come.\nTo keep this brief, the following is a nearly exhaustive list of functionalities that I would currently like to implement in EZbakR over the next year:\nExtensive model fit assessment. This will include assessing mixture model fits (i.e., how well does a two-component binomial mixture model fit your data; fit assessed for each +label sample) as well as dynamical systems model fits (i.e., how well does your assumed model of RNA dynamics fit your data; fit assessed across multiple different label times).\n\nSignificance: This will hopefully facilitate filtering out problematic features plagued by various artifacts.\n\nAlternative mixture models. It is currently unclear what this will specifically look like, but I will be exploring various iterations of models of overdispersion and other deviations from binomially distributed mutational data.\n\nSignificance: These alternative models will likely be useful for applications involving shorter label times, where overdispersion can become a bigger problem.\n\nExpanded data visualization capabilities.\n\nSignificance: NR-seq data is rich and EZbakR’s current visualization toolkit is limited.\n\nRefinement of existing functionality. Namely, I will be exploring potential improvements to the generalized linear modeling, generalized dynamical systems modeling, and transcript isoform modeling performed by EZbakR.\n\nSignificance: Small but potentially impactful improvements to the unique analysis strategies currently implemented by no other tool but EZbakR.\n\nSingle-cell NR-seq compatibility, paired with updates to fastq2EZbakR.\n\nSignificance: The last couple years has seen a relative explosion of single-cell NR-seq methods. I think EZbakR has a lot to offer users in terms of the insights they could be extracting from this data.\n\nBetter support for single-nucleotide NR-seq analyses.\n\nSignificance: I’ve been invovlved in several collaborations that use the EZbakR-suite to analyze single-nucleotide NR-seq data (that is, they are interested in mutation rates at specific nucleotides). In fact, there is a whole universe of methods that you could lump under the NR-seq umbrella, that while not technically using metabolic labeling and nucleotide recoding chemistry, could be processed and analyzed by the EZbakR-suite (structure probing methods like DMS-MaP-seq, RNA modification mapping methods like DART-seq, RBP interaction mapping methods like TRIBE-ID, etc.)\n\nBetter support for time series analyses.\n\n\n\nfastq2EZbakR version 0.8.0 (released 12/4/2025) brought a promising brand new piece of functionality: the ability to assign reads to 3’-ends. You can read more about this strategy here.\nOver the next year, I will strive to implement the following improvements/changes:\n\nFaster and improved SNP calling. This can be the major bottleneck in some datasets. I will also be inspecting the SNP calling strategy more closely in hopes of refining several aspects of it.\nImproved TEC assignment. There are currently a couple quirky edge cases where RSEM assigns reads to the wrong transcript isoform; I would like to get rid of these mistakes. I would also like to expand compatibility of this assignment strategy to aligners other than STAR, which will require incorporating tools like mudskipper into the workflow.\nSingle-cell NR-seq compatibility. We have seen a relative explosion of single-cell NR-seq methods. While tools like GRAND-SLAM can support analyses of this data, and tools like dynast are specifically designed for this one class of NR-seq experiment, I believe that fastq2EZbakR has a lot to offer in this space. In particular, I think giving users the power to explore their mutational data more deeply and giving bioinformaticians a simple starting point for model development could prove very useful to the community.\nBetter single-nucleotide support with perbase. fastq2EZbakR can provide nucleotide-specific mutational data, but this is currently done with some old custom Python/Shell scripts that are a bit rough around the edges. I would like to clean this up, and perbase will be key to that.\n\n\n\n\nI completed my PhD in Matt Simon’s lab (where I developed the EZbakR suite) in August of 2025. I am now a post-doc in Anshul Kundaje’s lab. Despite this, maintaining the EZbakR suite has not been completely relegated to “side project” status. I am still an active user of the EZbakR suite for some of the work I have planned during my post-doc. Because of this, I am planning to stick to a roughly monthly update schedule, with new minor versions of both EZbakR and fastq2EZbakR released around the end of every month.\nA very hesitant rough draft of what the schedule of changes might look like is below:\nEnd of January:\n\nfastq2EZbakR single-cell NR-seq support\nEZbakR single-cell NR-seq support\nExpanded EZbakR data vis\n\nEnd of February:\n\nImproved EZbakR core functionality\nExpanded EZbakR data vis\nfastq2EZbakR perbase refactor\n\nEnd of March:\n\nEZbakR model fit assessment\nfastq2EZbakR improve SNP calling\n\nEnd of April:\n\nEZbakR single-nucleotide support\nfastq2EZbakR improve TEC assignment\n\nEnd of May:\n\nEZbakR time series analyses\nEZbakR alternative mixture models\n\nSuggestions for changes/new features or prioritization of features can be made on the relevant Github Issues page (EZbakR’s or fastq2EZbakR’s)."
  },
  {
    "objectID": "posts/nrseq_analysis/index.html",
    "href": "posts/nrseq_analysis/index.html",
    "title": "Analyzing NR-seq data: the basics",
    "section": "",
    "text": "In my last post, I introduced NR-seq by walking through the development of an NR-seq simulator. That post implicitly introduced some of the complexities of interpreting NR-seq data. In this post, we will tackle these challenges head-on and build up a rigorous strategy by which to analyze NR-seq data. We will do this in a piece-meal fashion, first developing a simple but flawed strategy, until eventually working up to mixture modeling (the current gold-standard for NR-seq analyses). No statistical model is perfect though, so we will finish with a discussion and exploration of the limitations of this gold-standard."
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#nr-seq-a-reminder",
    "href": "posts/nrseq_analysis/index.html#nr-seq-a-reminder",
    "title": "Analyzing NR-seq data: the basics",
    "section": "NR-seq: a reminder",
    "text": "NR-seq: a reminder\nIn an NR-seq experiment, there are two populations of RNA: those synthesized in the presence of label (a.k.a. labeled, or new, RNA) and those which were synthesized prior to metabolic labeling (a.k.a unlabeled, or old, RNA). The first task of any NR-seq analysis is for a given species of RNA (e.g., RNA transcribed from a particular gene), quantify the relative amounts of these two populations. This is referred to as that species’ “fraction new” or “new-to-total ratio (NTR)”. Downstream analyses are then aimed at interpreting these fraction news/NTRs. This post will only concern itself with fraction new estimation. I will use the term “fraction new” for the remainder of this post.\nTo estimate the fraction new, we rely on the mutational content of mapped sequencing reads. NR-seq involves chemically recoding metabolic label (e.g., s4U) so that reverse transcriptase reads it as a different nucleotide (e.g., a cytosine). Thus, reads from new RNA will have, on average, more mutations than reads from old RNA. This observation is the key to analyzing NR-seq data.\nTo test the strategies discussed, we will use simulated data. This allows us to know the ground truth and explore the robustness of any approach. Here is the function that we will use to simulate data, as well as some helper functions we can use to assess analysis strategies:"
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#a-simple-approach-mutational-cutoffs",
    "href": "posts/nrseq_analysis/index.html#a-simple-approach-mutational-cutoffs",
    "title": "Analyzing NR-seq data: the basics",
    "section": "A simple approach: mutational cutoffs",
    "text": "A simple approach: mutational cutoffs\nIf reads from new RNA have more mutations on average than those from old RNA, maybe we can just use a simple mutational cutoff to classify individual reads as from old or new RNA. The fraction of reads that come from the latter is then our estimate for the fraction new. This approach has been popular since the advent of NR-seq, and is implemented in popular bioinformatic tools for analyzing NR-seq data like SLAMDUNK. Let’s simulate some data and test out this approach\n\n\n\n\n\n\n\n\nIf you run this code with the default simulation parameters, you’ll see that the estimates are decent. The 1+ mutation cutoff for newness looks better than the 2+ cutoff, with the former yielding estimates that consistently correlate pretty well with the simulated ground truth.\nSo that’s all it takes to analyze NR-seq data? Not so fast. In our simulation, there is a default metabolic label incorporation + conversion rate of 5%. While this is a standard “good” incorporation rate, if you analyze as many NR-seq datasets as I have you will quickly notice that there is a lot of dataset-to-dataset variation in the incorporation rate. For example, there is a tremendous amount of cell line-to-cell line variation in the readiness of s4U incorporation, with some cell lines (e.g., HEK293 and HeLa) uptaking s4U with great tenacity and others (e.g., neuronal cell lines) having typically much lower s4U incorporation rates. In addition, incorporation rates also can correlate with biological condition. For example, knocking out key factors in RNA metabolism (e.g., degradation factors) can significantly impact incorporation rates. In general, incorporation rates seem to correlate strongly with general metabolic rates, and anything that perturbs these rates will likely affect incorporation rates.\nThis lattermost observation is particularly dangerous when it comes to applying the simple mutation content cutoff analysis strategy. Often, we don’t just care about what an RNA’s dynamics look like in one biological condition, but rather how it differs between two more different conditions (e.g., WT vs. KO of your favorite gene, untreated vs. drug treated, etc.). If an analysis method is not robust to variation in incorporation rates, it risks making technical variability look like biological signal.\nThus, what happens if we simulate a different incorporation rate? If you tweak the simulation above (set phigh in simulate_nrseq() to a different value than its default of 0.05 and rerun code):\n\n\n\nAccuracy of cutoff approach for range of phighs\n\n\nThe key takeaway from this investigation is that the accuracy of the cutoff-based approach is heavily reliant on the incorporation rate. Since incorporation rate often correlates with biology, this represents a dangerous confounder for mutation cutoff analyses. We need a more robust analysis strategy."
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#a-better-idea-statistical-modeling",
    "href": "posts/nrseq_analysis/index.html#a-better-idea-statistical-modeling",
    "title": "Analyzing NR-seq data: the basics",
    "section": "A better idea: statistical modeling",
    "text": "A better idea: statistical modeling\nThe problem with the cutoff based approach is two-fold:\n\nIt’s possible for reads from labeled RNA to have no mutations. This is because the metabolic label has to compete with the regular nucleotide for incorporation, which is what keeps incorporation rates relatively low in most NR-seq experiments.\nIt’s possible for reads from unlabeled RNA to have mutations. This can be due to RT errors, sequencing errors, alignment errors, unmasked SNPs, etc.\n\nThus, a mutation in a read does not make it definitively from new RNA, and a lack of mutations does not make it definitively from old RNA. How can we navigate this inherent uncertainty? This is exactly what statistical modeling was built for.\nStatistical modeling first means coming up with a model that specifies how likely every possible data point is. If you tell me the number of mutable nucleotides, the number of mutations in a read, whether it came from old or new RNA, and whatever can be specified about the process by which mutations arise in reads, I should be able to use this model to calculate a likelihood for that piece of data.\n\n\n\n\n\n\nWhat is a data point’s “likelihood”?\n\n\n\n\n\nThe likelihood of a data point is the probability of seeing that data, given all of the information you provided, often written as P(data | parameters). In this case, we are dealing with discrete data (integer mutation counts), meaning that this likelihood can also be interpreted as the probability of getting that data point given all of the specified parameters. In a continuous setting, interpreting this is a bit more complicated, as the probability of any specific continuous outcome is 0.\n\n\n\nIn practice, this often involves specifying a convenient to work with probability distribution that describes the variability in your data. To do this, you need to make some assumptions about your data. For NR-seq data, it is common to assume:\n\nFor reads from new RNA, there is a set probability (call it phigh) that a given mutable nucleotide (e.g., uridines in an s4U labeling NR-seq experiment) is mutated. This phigh is the same for all such reads, regardless of the RNA species of origin.\nFor reads from old RNA, there is also a set probability of mutation (call it pold) for all such reads.\nAll nucleotides are independent. Whether or not a given nucleotide is mutated has no impact on the probability that other nucleotides in that read are also mutated (given the status of the read as coming from old or new RNA).\n\nThese are actually the exact assumptions that we used to simulate data above and in the introduction to NR-seq blog. These assumptions lend themselves to a particular model: a two-component binomial mixture model.\n\nTwo-component binomial mixture model\n“Two-component binomial mixture model” is a mouthful, so let’s break it down.\n“Two-component” = the model supposes that there are two populations in your data. In our case, this is reads from old RNA and reads from new RNA.\n“binomial” = data from each of the populations is modeled as following a binomial distribution. We’ve seen this distribution in the intro to NR-seq post. It describes a situation where you have a certain number of independent “trials” (e.g., mutable nucleotides), with a set probability of “success” (e.g., mutation of the nucleotide) for each trial.\n“mixture model” = you don’t know which population any given data point comes from. This is known as a “latent-variable model”, which can pose some computational challenges when trying to estimate the parameters of such a model. These challenges will turn out to be fairly easy to navigate in this setting, but will limit our efforts to extend and improve this model in future sections.\nTo summarize, we are assuming that each sequencing read comes from one of two populations: old RNA or new RNA. The mutational content of both types of reads is well modeled as following a binomial distribution. The parameters of these binomial distributions are the number of mutable nucleotides and the probability that each of these nucleotides gets mutated. We don’t need to estimate the number of mutable nucleotides (this is just more data), but we do not know a priori the two mutation rates. Thus, we need to estimate these two parameters, as well as the quantity of primary interest: the fraction new. We can schematize this model as such:\n\n\n\nTwo-component binomial mixture model"
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#fitting-a-two-component-binomial-mixture-model",
    "href": "posts/nrseq_analysis/index.html#fitting-a-two-component-binomial-mixture-model",
    "title": "Analyzing NR-seq data: the basics",
    "section": "Fitting a two-component binomial mixture model",
    "text": "Fitting a two-component binomial mixture model\n\nThe basics\nA TCBMM has three parameters that need to be estimated for each RNA feature:\n\nThe fraction new\nThe probability of a mutation in reads from new RNA (\\(p_{\\text{high,TC}}\\) in the TCBMM figure above)\nThe probability of a mutation in reads from old RNA (\\(p_{\\text{low,TC}}\\) in the TCBMM figure above)\n\n\n\n\n\n\n\nWhat is an “RNA feature”?\n\n\n\n\n\nOur goal is to estimate the fraction of RNA molecules that are new/labeled for a given species of RNA. Our definition of “species” is technically flexible, and is what I refer to as an “RNA feature”. The most common choice for a feature is a gene. That is, we estimate the fraction of RNA molecules produced from a given gene that are new. In practice though, there are a lot more features we may be interested in analyzing. See the EZbakR paper for a description of some other options.\n\n\n\nIn this post, we will estimate these via the method of maximum likelihood. That means we will find parameter estimates that maximize the likelihood of our data. In theory, this is simple: just write a function to calculate the likelihood for any combination of parameter values and data, and use the optimization algorithm of your choice. Here’s what that might look like for the TCBMM:\n\n\n\n\n\n\n\n\nAlready, this quickly whipped up strategy is working pretty well. For one, we have largely solved the problem of phigh/plow dependence on estimate accuracy. Play around with different values of plow/phigh in the simulation and prove this for yourself, but that is the main advantage of the TCBMM approach. You can also see that the more data we have, the better our estimates get (on average). This is a nice trend, and means that paying for more sequencing depth can have a significant positive impact on the quality of our estimates. Technically, a similar trend holds for the mutation content cutoff strategy, but because we can’t be sure if our estimates are biased or not, more reads could just yield higher confidence wrong estimates.\n\n\nComplete pooling to improve estimate stability\nOne thing you should note though is that there are a handful of highly inaccurate estimates. The frequency of these increases if you decrease the simulated s4U incorporation rate (e.g., try phigh = 0.02 for instance). These are usually lower coverage features, but can we do better? I will argue yes, thanks to a two-step fitting approach that is implemented in tools like bakR/EZbakR and GRAND-SLAM.\nHaving to estimate both the labeled/unlabeled read mutation rates as well as the fraction of reads from each population is fundamentally challenging. While it is technically identifiable (ignoring label flipping, which is easy to deal with in this setting), low coverage features typically have too little information to accurately estimate all of these parameters. Does a feature have only a few high mutation content reads because the fraction new is low, or because the mutation rate in reads from new RNA (phigh) low? Tough to distinguish these two if you only have 10s of reads. The common solution to this problem is “complete pooling of the mutation rate estimates”.\nComplete pooling refers to a spectrum of model archetypes possible in multi-group analyses (e.g., estimating the fraction new for multiple different RNAs, like RNAs produced from different genes). Instead of estimating a separate phigh and plow for each feature, how about we use all of the data to calculate a single phigh and plow for all features? Doing this means assuming that there is very little feature-to-feature phigh or plow variation, but there is decent evidence that this assumption often holds, especially if analyzing data with standard label times (e.g., multi-hour label times in human cell lines).\nThe modified strategy might look like:\n\n\n\n\n\n\n\n\nThis should look notably better. For example, here is what I get from a 200 feature simulation for a range of phighs:\n\n\n\nAccuracy of cutoff approach for range of phighs\n\n\nOf course, it will always be more difficult to estimate the fraction new for a low coverage feature vs. a high coverage one. Despite this, complete pooling of the mutation rate estimates has significantly stabilized low coverage estimates, making them far more accurate than in the no pooling case."
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#going-beyond-tcbmm",
    "href": "posts/nrseq_analysis/index.html#going-beyond-tcbmm",
    "title": "Analyzing NR-seq data: the basics",
    "section": "Going beyond TCBMM",
    "text": "Going beyond TCBMM\nThe power of mixture modeling lies both in its robustness as well as its extensibility. TCBMM makes several assumptions about the mutational content of NR-seq reads. Namely that:\n\nEvery uridine in a labeled RNA was equally likely to get replaced with s4U\nEvery uridine captured by a sequencing read was equally likely to give rise to a non-s4U-related mutation due to sequencing errors, alignment errors, etc.\n\nIn future posts, I will discuss a number of ideas for how to extend and improve NR-seq TCBMM’s. These include:\n\nThree-component mixture modeling, where a second population of reads from unlabeled RNA with a higher mutation rate (presumably due to heightened alignment errors) is modeled.\nOverdisperse mixture modeling where either an overdisperse binomial distribution (e.g., a beta-binomial) replaces one or both of the binomial distribution components, or where a different incorporation rate parameter is estimated for fast and slow turnover RNA.\nHierarchical mixture modeling where a sample-wide average incorporation rate is inferred and used as a strongly regularizing prior to estimate feature-specific incorporation rates.\nModeling the transcription process, which at short label times leads to an expected position-dependency in the incorporation rate (more 5’ nucleotides will be on average less well labeled than more 3’ nucleotides).\n\nWhile all of these are theoretically promising, the challenge of fitting more complex models is two-fold:\n\nTheir increased flexibility comes with an increased risk of overfitting. In this setting, this leads to estimate instability, where a better model fit yields extreme conclusions about RNA dynamics (i.e., unusually high fraction new and thus unrealistically rapid turnover kinetics).\nWhile an alternative model may capture one aspect of the true data generating process unaccounted for by TCBMM, it may amplify biases that arise from not accounting for some other aspect of the data generating process.\n\nMore on this in later posts!"
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#summary",
    "href": "posts/nrseq_analysis/index.html#summary",
    "title": "Analyzing NR-seq data: the basics",
    "section": "Summary",
    "text": "Summary\nIn this post, we introduced two common NR-seq analysis strategies: mutational content cutoffs and two-component binomial mixture models (TCBMM’s). We saw how while the former is easy to implement, efficient, and intuitive, it risks providing biased estimates. In addition, the magnitude of these biases is a function of technical details that can vary between biological conditoins. This is why TCBMM’s are typically superior for estimating the fraction of reads that come from labeled RNA in an NR-seq experiment. We also explored how complete pooling of phigh and plow estimates can improve the accuracy of fraction new estimates, especially for low coverage features."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Introducing NR-seq by simulating NR-seq",
    "section": "",
    "text": "Nucleotide recoding RNA-seq (NR-seq; SLAM-seq, TimeLapse-seq, TUC-seq, etc.), is a set of methods to probe the dynamics of RNA. These methods use metabolic labeling. In this case, metabolic labeling means treating cells with a molecule that looks like a regular nucleotide (e.g., 4-thiouridine, or s4U), which cells incorporate into nascent RNA. NR-seq then employs a chemistry to modify the metabolic label so that a reverse transcriptase identifies it as a nucleotide different from what it originally mimiced (e.g., recoding s4U as a cytosine analog). This allows label incorporation events to be bioinformatically detected as apparent mutations in aligned sequencing reads.\nThis post introduces the main ideas behind the modeling of NR-seq data."
  },
  {
    "objectID": "posts/post-with-code/index.html#a-brief-introduction-to-nr-seq",
    "href": "posts/post-with-code/index.html#a-brief-introduction-to-nr-seq",
    "title": "Introducing NR-seq by simulating NR-seq",
    "section": "A brief introduction to NR-seq",
    "text": "A brief introduction to NR-seq\n\n\n\nSchematic of an NR-seq experiment. Adapted from Schofield et al. 2018.\n\n\nDeveloping a mechanistic understanding of gene expression regulation requires methods to probe the kinetics of RNA synthesis, processing, and degradation. While standard RNA-seq begins to solve this problem, it provides limited information about the kinetics of the processes which determine an RNA’s abundance. Nucleotide recoding RNA-seq (NR-seq; TimeLapse-seq, SLAM-seq, TUC-seq, etc.) overcomes these limitations. NR-seq combines metabolic labeling with novel chemistries that recode the hydrogen bonding pattern of a metabolic label so as to facilitate detection of labeled RNA via these chemically induced mutations in sequencing reads, absolving the need for biochemical enrichment of labeled RNA. By providing information about both overall RNA abundance and the dynamics of nascent and pre-existing RNA, NR-seq resolves the kinetic ambiguities of standard RNA-seq.\nOne way to intuit how metabolic labeling provides information about the dynamics of RNA is to consider one of the populations being tracked: the old, unlabeled RNA. Since this RNA can only degrade, its dynamics are entirely determined by its turnover kinetics. Combining this with the abundance information provided by standard RNA-seq provides information about the RNA’s synthesis kinetics (transcription + processing)."
  },
  {
    "objectID": "posts/post-with-code/index.html#understanding-nr-seq-by-simulating-nr-seq",
    "href": "posts/post-with-code/index.html#understanding-nr-seq-by-simulating-nr-seq",
    "title": "Introducing NR-seq by simulating NR-seq",
    "section": "Understanding NR-seq by simulating NR-seq",
    "text": "Understanding NR-seq by simulating NR-seq\nHere, we will walk through a basic simulation of NR-seq data to give you a sense as to how to think about and interpret NR-seq data.\n\nThe full shebang\nBelow is a summary of the entire simulation which we will walk through piece by piece in the following sections.\n\n\n\n\n\n\n\n\n\n\nMutable nucleotide content\nDifferent RNAs have different nucleotide content. Sequencing reads in standard short read sequencing experiments typically only sample a small portion of the entire RNA, and thus reads from a given species of RNA will also vary in their nucleotide content. In a standard NR-seq experiments, uridines in an RNA synthesized in the presence of metabolic label have an opportunity to be replaced with s4U. This, we need to simulate the number of Us in each read (prior to nucleotide recoding). Since we typically sequence cDNA, and this is also best thought as the number of Ts in the genomic sequence to which a given read aligned, we will refer to this as the number of T’s.\nLet’s consider simulating sequencing reads from a specific RNA species (e.g., a particular transcript isoform) first. In this case, the RNA will have a particular U-content, i.e., the fraction of nucleotides in its exonic sequence that are U’s. This is a simulation parameter, along with the number of reads we want to simulate, and the length of each read. It’s simplest to assume that each nucleotide in a read has some set probability of being a U, and that this probability is the same for all nucleotides. In this case, we can draw the number of U’s (or cDNA T’s) from a binomial distribution:\n\n\n\n\n\n\n\n\n\n\nMutational content\nThe number of mutations in an NR-seq sequencing read provides the information necessary to classify it as having come from either labeled or unlabeled RNA. If a read is from labeled RNA (or RNA that was synthesized during the label time; a given RNA molecule may not incorporate any metabolic label even if it could have), the mutational content of that read is a function of the following things:\n\nHow many mutable nucleotides are contained in the read, which we simulated above.\nHow often the metabolic label is incorporated in place of the standard nucleotide. We will also lump into this term the chemical efficiency of recoding, which is usually pretty high (&gt; 80%).\nThe background mutation rate, due to sequencing/RT/PCR errors, alignment errors, SNPs, etc.\n\nWe’ve already simulated the first factor. The other two are parameters that we will include in our simulation. Incorporation rates in successful NR-seq experiments are typically around 5%, and background mutation rates can range from nearly 0% to around 0.4%, depending on a number of factors. 0.2% is a fairly typical background mutation rate in my experience.\nFinally though, we need to determine the “newness” status of each sequencing read. Each species of RNA will have a characteristic “fraction new”, which is the fraction of RNA molecules present at RNA extraction time that were exposed to metabolic label. We can then model sequencing reads as being randomly drawn from this pool, with fraction new probability of sampling a labeled RNA. These processess are all well modeled with binomial distributions, so simulating all of this looks like:\n\n\n\n\n\n\n\n\nSome observations from the above visualizations:\n\nSome reads from unlabeled RNA have mutations. This is due to the non-zero background mutation rate.\nSome reads from labeled RNA have no mutations. This is due to the incorporation rate being much less than 100%. Experimentally, this is due to the metabolic label having to compete with the regular nucleotide for incorporation into nascent RNA.\n\nThese observations are what make analyzing NR-seq data challenging, and what will motivate analysis strategies discussed in other posts.\n\n\nRNA kinetics and NR-seq\nDifferent transcript isoforms can have very different properties. Of importance to an NR-seq experiment, different isoforms can differ in their nucleotide content and turnover kinetics. The latter’s impact is obvious, but my reason for bringing up the former may not be. In short, the rate at which a given isoform is degraded will determine its fraction new.\nTo see why turnover kinetics of RNA influences NR-seq data, consider the following model:\nIn it, RNA’s are synthesized at some rate ksyn, and degraded with a rate constant kdeg. kdeg is related to the average lifetime of a given RNA. When the rate at which a given species of RNA is degraded (kdeg * the amount of RNA that exists to be degraded) is equal to the rate at which it is synthesized (ksyn), that RNA is said to be at steady-state. A bit of algebra reveals that this occurs when the levels of RNA are equal to ratio of the synthesis and degradation rate constants:\n\\[\n\\begin{aligned}\n\\text{ksyn} & = \\text{kdeg}*[\\text{RNA}]_{\\text{ss}} \\\\\n\\frac{\\text{ksyn}}{\\text{kdeg}} & = [\\text{RNA}]_{\\text{ss}}\n\\end{aligned}\n\\]\nWhen you add metabolic label, you effectively create two species of RNA with distinct dynamics:\n\nOld RNA that existed at the time of labeling. These can only degrade and are no longer synthesized. In this model, this means that they exponentially degrade with rate constant kdeg.\nNew RNA that is synthesized during labeling. These will slowly accumulate to steady-state levels as they are both synthesized and degraded.\n\nAssuming that the synthesis and degradation rate constants are constant throughout the label time, the dynamics of old and new RNA looks like:\n\n\n\n\n\n\n\n\nBecause the rate constants are unchanging, so is the total amount of RNA at any given time. Thus, the amount of Old RNA + New RNA = steady-state RNA level. Because of this, there is a simple relationship between the turnover kinetics of an RNA and the fraction new (abbreviated fn) for that RNA:\n\\[\n\\begin{aligned}\n\\text{fn} &= \\frac{[\\text{New RNA}]}{[\\text{RNA}]} \\\\\n\\text{fn} &= \\frac{[\\text{RNA}]_{\\text{ss}}\\ast(1 - e^{-\\text{kdeg}\\ast \\text{tl}})}{[\\text{RNA}]_{\\text{ss}}} \\\\\n\\text{fn} &= 1 - e^{-\\text{kdeg}\\ast \\text{tl}}\n\\end{aligned}\n\\] where tl is the amount of time for which the cells were labeled. This simple relationship reveals some of the power of NR-seq. NR-seq provides information about RNA kinetics inaccessible to analyses of standard RNA-seq data (i.e., read counts). We can also use this relationship to modify our simulation and set the more biologically interpretable degradation rate constant rather than the fraction new:\n\n\n\n\n\n\n\n\n\n\nSimulating multiple transcripts\nSo far, we have focused on simulating data for a single transcript. In actuality, NR-seq is a high throughput method that provides information about all of the appreciably expressed transcripts in the cells from which you extracted RNA. Thus, we can set the number of transcripts we would like to simulate, and then draw kinetic parameters from a chosen distribution.\nOne thing we need to consider though is how the rate constants give rise to expected read counts for each transcript. We have discussed how the steady-state levels of a given RNA are a function of its synthesis and degradation rate constants, but is an RNA’s abundance related to its RNA-seq coverage?\nThe answer comes from realizing that RNA-seq is a measure of relative RNA abundance, not absolute abundance. That is, the number of reads you get from an RNA is a function of how abundant that RNA is, relative to all other RNAs in the sequenced pool. In addition, abundance in this setting is a function not only of the molecular abundance of an RNA (i.e., the number of molecules of that RNA present in the average cell), but also of the length of the RNA. This is because we are sequencing short fragments of an RNA, and thus the probability that we sequence a fragment from a given RNA depends on how many fragments in our pool come from that RNA. This is roughly \\([\\text{RNA}]_{\\text{ss}} \\ast \\text{Length}\\). In this simulation, we will make the simplifying assumption that all transcripts are the same length, and will thus only need to consider the relative steady-state abundances of each RNA. Thus, the parameters that we need to set are:\n\nThose determining the rate constant distributions from which we sample transcript-specific ksyn’s and kdeg’s.\nThe total number of reads in our library. These will be randomly divided among the various simulated transcripts.\nThose determining the U-content distribution from which we sapmle transcript-specific U-content’s.\n\nDeciding 1) and 3) is often best done through comparing candidate distributions to real data, and choosing parameters that make the simulated data look as close to the real data as possible. The result of this may look like:"
  },
  {
    "objectID": "posts/post-with-code/index.html#summary",
    "href": "posts/post-with-code/index.html#summary",
    "title": "Introducing NR-seq by simulating NR-seq",
    "section": "Summary",
    "text": "Summary\nIn this post, we introduced NR-seq, and built a simulation of NR-seq data to explore several aspects of NR-seq data. In the next post, we will discuss various strategies for analyzing this data, using the simulation we built here to test our strategies."
  },
  {
    "objectID": "posts/Rintro_day2/index.html",
    "href": "posts/Rintro_day2/index.html",
    "title": "Intro to ggplot, dplyr, and pivoting",
    "section": "",
    "text": "Much of this class will center on manipulating and visualizing data. This section gets into how to do that in R. Further details about these topics can be found in the amazing and totally free R for Data Science (R4DS). Chapters 1 and 3 will be especially useful, but all of the content in chapters 1-20 (sounds like a lot, but each chapter is fairly short) may be worth checking out.\nTo the code in this walkthrough, you will need to load the tidyverse libraries (technically we will primarily use ggplot, dplyr, and tidyr, with a tiny bit of tibble):\n# tidyverse packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nThroughout, I will use the mtcars dataset, which is a built-in R data frame (meaning it’s always available to you within any R installation; run print(mtcars) on your console to see what I mean) that contains automobile data extracted from the 1974 Motor Trend magazine. The dataset has 32 rows (each representing a different car model, with the name of that model being the row names of the data frame), and 11 columns (variables). These variables include:\nWhy this data? Because it’s an R education classic."
  },
  {
    "objectID": "posts/Rintro_day2/index.html#an-introduction-to-ggplot",
    "href": "posts/Rintro_day2/index.html#an-introduction-to-ggplot",
    "title": "Intro to ggplot, dplyr, and pivoting",
    "section": "An introduction to ggplot",
    "text": "An introduction to ggplot\nThe ggplot2 package is part of the tidyverse collection. It implements the Grammar of Graphics, which allows you to build plots layer by layer. The general structure is:\n\nggplot(data = &lt;DATA&gt;,\n       mapping = aes(x = &lt;X-VARIABLE&gt;,\n                     y = &lt;Y-VARIABLE&gt;)) +\n  &lt;GEOM_FUNCTION&gt;()\n\nwhere things in &lt;...&gt; are general placeholders that you would have to edit to get working code. The general features are:\n\nggplot(data = ...): specifies the dataset.\naes(): specifies which columns map to the x-axis, y-axis, color, size, etc. of the plot\n&lt;GEOM_FUNCTION&gt;, which in practice often looks like geom_*(): specifies the geometry or type of plot (points, bars, lines, tiles, etc.)\n\n\nMaking scatter plots with geom_point()\nScatter plots are used to visualize the relationship between two numeric variables.\nAs an example, consider plotting mpg (miles per gallon) against wt (weight of the car) from the mtcars dataset to see if mileage is correlated with weight:\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nWe are mapping wt on the x-axis and mpg on the y-axis\ngeom_point() draws the scatter plot by making each data points x and y a point (circular by default).\n\nYou can add more aesthetics, for example coloring the points by the number of cylinders:\n\nggplot(data = mtcars,\n       aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point()\n\n\n\n\n\n\n\n\nNote that cyl is numeric in mtcars, so we can convert it to a factor (factor(cyl)) when using it as a categorical variable (a variable with a finite set of values).\n\n\nJittered scatter plots with geom_jitter()\nA jitter plot helps when data points overlap (i.e., have identical x or y values). It adds a small random noise to the position of each point, preventing them from lying exactly on top of each other:\n\nggplot(\n  data = mtcars, aes(x = factor(cyl),\n                     y = mpg)\n) +\n  geom_jitter(\n    width = 0.2,\n    height = 0,\n    alpha = 0.7\n  )\n\n\n\n\n\n\n\n\n\nHere, x is factor(cyl), turning cylinders into discrete categories.\nWe add jitter on the x-axis (using width = 0.2), and none on the y-axis (height = 0). This makes sense as the x-axis is not numeric, so points being more to the left or right than they “should” doesn’t effect interpretation. The y-axis is a continuous numeric variable though, so adding jitter in that direction distorts the data and could mislead someone looking at the plot.\nalpha = 0.7 makes the points slightly transparent to give you a sense of point density.\n\n\n\nHistograms with geom_histogram()\nA histogram is used to visualize the distribution of a single numeric variable. Let’s look at the distribution of miles per gallon (mpg):\n\nggplot(data = mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2,\n                 fill = \"skyblue\",\n                 color = \"black\")\n\n\n\n\n\n\n\n\n\nbinwidth = 2 sets the width of the histogram bins.\nfill sets the color of the bars, color sets the outline color.\n\nSomtimes, data can have a large range of frequencies, making it helpful to display the counts on a log scale:\n\nggplot(data = mtcars, aes(x = mpg)) +\n  geom_histogram(color = 'black') +\n  scale_y_log10()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\n\n\nWarning: Removed 12 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\nHeatmaps with geom_tile()\nA heatmap is useful for visualizing a matrix of values or the relationship between two categorical variables, colored by a numeric value. One common example is a correlation matrix among numeric variables.\nA somewhat silly example of this is below; all you need to know is that data ends up being a data frame with columns X, Y, and Z, where X and Y are categorical variables (i.e., they take on a finite set of values) and Z is a continuous numeric variable created using a particular random number generator (runif()):\n\n# Dummy data\nx &lt;- LETTERS[1:20]\ny &lt;- paste0(\"var\", seq(1,20))\ndata &lt;- expand.grid(X=x, Y=y)\ndata$Z &lt;- runif(400, 0, 5)\n \n# Heatmap \nggplot(data, aes(X, Y, fill= Z)) + \n  geom_tile()\n\n\n\n\n\n\n\n\n\ngeom_tile() draws the heatmap squares, one for each X and Y combo. fill = Z colors the tiles by the Z-value.\n\nFor a more hardcore example, let’s create a correlation matrix among select columns in mtcars (e.g., mpg, wt, hp, disp) and plot it as a heatmap:\n\n# Compute correlations\ncor_mat &lt;- cor(mtcars[, c(\"mpg\", \"wt\", \"hp\", \"disp\")])\n\n# Convert to a long format dataframe for plotting\ncor_df &lt;- as.data.frame(as.table(cor_mat))\ncolnames(cor_df) &lt;- c(\"Var1\", \"Var2\", \"Correlation\")\n\n# Plot with geom_tile()\nggplot(data = cor_df, aes(x = Var1, y = Var2, fill = Correlation)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\n  theme_minimal() +\n  coord_fixed() +\n  labs(title = \"Correlation Heatmap\")\n\n\n\n\n\n\n\n\n\ncor() calculates the correlation matrix, this has rows and columns “mpg”, “wt”, “hp”, and “disp”, with entries being the correlation between of these variables and all of the other varialbes.\nas.table() and then as.data.frame() converts the matrix into a long format with columns for what was the rowname of the correlation matrix (renamed to “Var1”) and what was the colname of the correlation matrix (renamed to “Var2”), as well as the correlation value between “Var1” and “Var2”.\nscale_fill_gradient2() helps us visualize positive vs. negative correlations using a diverging color scale.\ncoord_fixed() ensures each tile is square\n\n\n\nggplot themes\nSo far, we have discussed the basics of how to plot data with ggplot. One thing that might stand out about this plot is various aspects of its aesthetics:\n\nThe gray checkerboard background\nThe font sizes\nText on axes and color legends\netc.\n\nTo change these aspects, we can use the concept of “themes” in ggplot2. This is done through the theme() function.\nStarting with the simple scatter plot we made earlier, we can update its look with a number of built-in themes that ggplot provides. For example, I am a fan of the “classic” theme:\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) + \n  geom_point() +\n  theme_classic()\n\n\n\n\n\n\n\n\nOther popular themes include:\n\ntheme_minimal()\ntheme_dark()\ntheme_void()\n\nYou can also customize every aspect of your plot with the use of theme():\n\nggplot(data = mtcars, aes(x = wt, y = mpg)) + \n  geom_point() +\n  theme(\n      # Plot background\n  plot.background = element_rect(fill = \"ivory\", color = NA),\n  \n  # Panel (plot area) settings\n  panel.background = element_rect(fill = \"ghostwhite\"),\n  panel.grid.major = element_line(color = \"gray90\", linetype = \"dashed\"),\n  panel.grid.minor = element_line(color = \"gray95\", linetype = \"dotted\"),\n  \n  # Axis customization\n  axis.title = element_text(face = \"bold\", color = \"darkblue\", size = 12),\n  axis.text = element_text(color = \"navy\", size = 10),\n  axis.line = element_line(color = \"navy\", linewidth = 0.5),\n  )\n\n\n\n\n\n\n\n\nCheck out the documentation (e.g., by running ?ggplot2::theme or going to this link) to learn more. There is also a nice article on the topic of customizing themes here."
  },
  {
    "objectID": "posts/Rintro_day2/index.html#an-introduction-to-dplyr-and-tidyr",
    "href": "posts/Rintro_day2/index.html#an-introduction-to-dplyr-and-tidyr",
    "title": "Intro to ggplot, dplyr, and pivoting",
    "section": "An introduction to dplyr and tidyr",
    "text": "An introduction to dplyr and tidyr\ndplyr and tidyr are both part of the tidyverse collection of packages:\n\ndplyr focuses on data manipulation and transformation. It provides a set of “verbs” that correspond to common data manipulation tasks (e.g., filter, select, mutate, summarise, arrange). These functions are often used together in a pipeline (with the %&gt;% operator) to create clean, readable code that closely expresses the steps of your data processing.\ntidyr specializes in reshaping data between wide and long (the latter called “tidy”) formats. In a tidy dataset, each variable is its own column, each observation is its own row, and each value is in its own cell. By using functions like pivot_longer() and pivot_wider(), tidyr helps you reorganize your data so that it’s consistent with these tidy data principles, facilitating analysis and visualization later on.\n\n\nCombining commands with %&gt;%\nThroughout this tutorial, I will use the so-called “magrittr pipe” (%&gt;%). This allows you to pass the result of one function as the first argument of the next function. It can make your code a lot cleaner and easier to read. A simple example of the pipe is:\n\nmyvect &lt;- c(1, 2, 3)\n\n# Sum the elements of the vector\nsum(myvect)\n\n[1] 6\n\n# Same thing, but with a pipe\nmyvect %&gt;% sum()\n\n[1] 6\n\n\nThese are two ways of doing the same thing. Either you can provide myvect to the function sum() as normal, or you can pipe it in; these are equivalent. The real power of %&gt;% comes from how it allows you to stitch together multiple operations:\n\nmtcars %&gt;%\n  subset(mpg &gt; 20) %&gt;%\n  ggplot(aes(x = wt, y = mpg)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nFirst, we pipe our data into subset(), which is a base R function that takes a data frame as input and returns only the rows that match a certain condition (mpg &gt; 20 in this case).\nWe are then piping the output of subset() into ggplot(), which is the equivalent of writing ggplot(data = subset(mtcars, mpg &gt; 20), aes(x = wt, y = mpg)) + ...\n\nTechnically, newer versions of R (version 4.1 and later) have a base R version of the pipe, known as the native pipe (|&gt;). They work pretty similarly, but I am an old hat and thus like to stick with the trusty magrittr pipe. You should feel free to use either though.\n\n\nSelecting columns with dplyr::select()\nselect() can be used to choose (or exclude) specific columns in a data frame. The simplest usage of select() is to specify the columns you want to keep by name:\n\nmtcars %&gt;%\n  dplyr::select(mpg, cyl, hp) %&gt;%\n  head()\n\n                   mpg cyl  hp\nMazda RX4         21.0   6 110\nMazda RX4 Wag     21.0   6 110\nDatsun 710        22.8   4  93\nHornet 4 Drive    21.4   6 110\nHornet Sportabout 18.7   8 175\nValiant           18.1   6 105\n\n\nThis returns a data frame with just those three columns. You can also exclude columns by using a - sign:\n\nmtcars %&gt;%\n  dplyr::select(-hp) %&gt;%\n  head()\n\n                   mpg cyl disp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 2.76 3.460 20.22  1  0    3    1\n\n\nThis returns all columns except hp.\nIf you want to get fancy, sometimes you don’t know the column names beforehand – maybe they come from user input to a function. In those cases, you can store column names in a charater variable and use !! (the “bang-bang” operator) to unquote them (convert them from strings to as if you were typing them in yourself without the \" \"):\n\ncols_to_select &lt;- c(\"mpg\", \"wt\")\n\nmtcars %&gt;%\n  select(!!cols_to_select) %&gt;%\n  head()\n\n                   mpg    wt\nMazda RX4         21.0 2.620\nMazda RX4 Wag     21.0 2.875\nDatsun 710        22.8 2.320\nHornet 4 Drive    21.4 3.215\nHornet Sportabout 18.7 3.440\nValiant           18.1 3.460\n\n\n\n\nAdding columns to a data frame with dplyr::mutate()\nmutate() allows you to add new columns (variables) or modify existing columns.\nFor instance, suppose you want to create a new column named mpg_level, categorizing mpg into “high” or “low” mileage based on whether a car gets more than 20 mpg:\n\nmtcars_new &lt;- mtcars %&gt;%\n  mutate(mpg_level = ifelse(mpg &gt; 20, \"high\", \"low\"))\n\n\nWe take the mtcars data frame and pipe (%&gt;%) it to mutate(). This means we are passing mtcars as the first argument of mutate, which needs to be a data frame.\nifelse() assigns a value of “high” to rows for which mpg &gt; 20, and “low” otherwise.\nThe resulting new column is called mpg_level, and it is only present in the new data frame mtcars_new\n\n\n\nGrouping and summarizing with dplyr::summarise() and dplyr::group_by()\nWhen working with data grouped by categories, you can compute summary statistics per group.\nExample: find the average miles per gallon (mpg) for each number of cylinders, and also track how many car models there are in each group:\n\nmtcars %&gt;%\n  group_by(cyl) %&gt;% \n  summarise(mean_mpg = mean(mpg), n = n())\n\n# A tibble: 3 × 3\n    cyl mean_mpg     n\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1     4     26.7    11\n2     6     19.7     7\n3     8     15.1    14\n\n\n\ngroup_by(cyl) splits the data by the cyl variable. Whatever happens next will be done on each group separately.\nsummarise() calculates summary statistics. Here, we calculate the average value (mean()) of the mpg column, and the number of data points (count of rows, n()).\n\n\n\nFiltering data frames with dplyr::filter()\nUse filter() to only keep rows that pass a certain set of conditions. For example, to select cars with more than 20 mpg:\n\nmtcars %&gt;%\n  filter(mpg &gt; 20)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\nYou can also combine filtering with grouping and only keep entire groups that pass a certain filter (e.g., by using any() and all()). For example, to keep only the groups of cars with the same cylinder that have at least one member of the group with more than 150 horsepower:\n\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  filter(any(hp &gt; 150))\n\n# A tibble: 21 × 11\n# Groups:   cyl [2]\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 4  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 5  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 6  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 7  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n 8  17.8     6  168.   123  3.92  3.44  18.9     1     0     4     4\n 9  16.4     8  276.   180  3.07  4.07  17.4     0     0     3     3\n10  17.3     8  276.   180  3.07  3.73  17.6     0     0     3     3\n# ℹ 11 more rows\n\n\n\nany(hp &gt; 150) is evaluated within each cyl group.\nIf any car in that group has hp &gt; 150, all rows of that group are kept.\n\nBy contrast, if you wanted to keep only the groups where all cars exceeded 150 horsepower:\n\nmtcars %&gt;%\n  group_by(cyl) %&gt;%\n  filter(all(hp &gt; 150))\n\n# A tibble: 0 × 11\n# Groups:   cyl [0]\n# ℹ 11 variables: mpg &lt;dbl&gt;, cyl &lt;dbl&gt;, disp &lt;dbl&gt;, hp &lt;dbl&gt;, drat &lt;dbl&gt;,\n#   wt &lt;dbl&gt;, qsec &lt;dbl&gt;, vs &lt;dbl&gt;, am &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt;\n\n\n\nall(hp &gt; 150) means every car in the group must have hp greater than 150.\n\n\n\nCombining tables with joins\nWhen you have two data frames that share at least one common column, you can join them together. For example, you might have:\n\nsample: The name of a sample that data came from\ngene_id: An identifier for a gene\n\nThere are 4 major types of joins that you can consider doing in this situation (df1 and df2 refer to the data frames being joined):\n\ninner_join(df1, df2): Combines the tables so that only rows with matching values in the join columns of both tables are kept. In the example below (moving away from mtcars for just the join examples), we are joining by the shared column “sample”, and “sampleC” is unique to one data frame while “sampleD” and “sampleZ” are unique to the other. Thus, the rows for these three cases are dropped:\n\n\ndfleft &lt;- tibble(\n  sample = c(\"sampleA\", \"sampleB\", \"sampleC\"),\n  reads = c(10, 100, 1000)\n)\n\ndfright &lt;- tibble(\n  sample = c(\"sampleA\", \"sampleB\", \"sampleD\", \"sampleZ\"),\n  treatment = c(\"drug\", \"nodrug\", \"drug\", \"nodrug\")\n)\n\ndf_joined &lt;- inner_join(\n  dfleft,\n  dfright,\n  by = \"sample\"\n)\n\nprint(df_joined)\n\n# A tibble: 2 × 3\n  sample  reads treatment\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    \n1 sampleA    10 drug     \n2 sampleB   100 nodrug   \n\n\n\nleft_join(df1, df2): Keeps all rows from the “left table” (df1, as it is the argument on the left). If any of the entries in join columns in the left table are not found in the right table, NA will be placed in the df2 column values in the new table. If an entry is found in df2 but not df1, it will not be in the output. For example, below, “sampleC” is present in the left data frame but not the right, so the column unique to the right data frame (treatment) is given a value of NA for that row:\n\n\ndfleft &lt;- tibble(\n  sample = c(\"sampleA\", \"sampleB\", \"sampleC\"),\n  reads = c(10, 100, 1000)\n)\n\ndfright &lt;- tibble(\n  sample = c(\"sampleA\", \"sampleB\", \"sampleD\", \"sampleZ\"),\n  treatment = c(\"drug\", \"nodrug\", \"drug\", \"nodrug\")\n)\n\ndf_joined &lt;- left_join(\n  dfleft,\n  dfright,\n  by = \"sample\"\n)\n\nprint(df_joined)\n\n# A tibble: 3 × 3\n  sample  reads treatment\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    \n1 sampleA    10 drug     \n2 sampleB   100 nodrug   \n3 sampleC  1000 &lt;NA&gt;     \n\n\n\nright_join(df1, df2): Same as left join but now the “right table” (df2 in this case) keeps all of its rows and the “left table” columns will be replaced with NA when a right table entry is not present in the left table:\n\n\ndfleft &lt;- tibble(\n  sample = c(\"sampleA\", \"sampleB\", \"sampleC\"),\n  reads = c(10, 100, 1000)\n)\n\ndfright &lt;- tibble(\n  sample = c(\"sampleA\", \"sampleB\", \"sampleD\", \"sampleZ\"),\n  treatment = c(\"drug\", \"nodrug\", \"drug\", \"nodrug\")\n)\n\ndf_joined &lt;- right_join(\n  dfleft,\n  dfright,\n  by = \"sample\"\n)\n\nprint(df_joined)\n\n# A tibble: 4 × 3\n  sample  reads treatment\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    \n1 sampleA    10 drug     \n2 sampleB   100 nodrug   \n3 sampleD    NA drug     \n4 sampleZ    NA nodrug   \n\n\n\nfull_join(df1, df2): Everything from both tables is kept, and NA fills any entry not present in the original table:\n\n\ndfleft &lt;- tibble(\n  sample = c(\"sampleA\", \"sampleB\", \"sampleC\"),\n  reads = c(10, 100, 1000)\n)\n\ndfright &lt;- tibble(\n  sample = c(\"sampleA\", \"sampleB\", \"sampleD\", \"sampleZ\"),\n  treatment = c(\"drug\", \"nodrug\", \"drug\", \"nodrug\")\n)\n\ndf_joined &lt;- full_join(\n  dfleft,\n  dfright,\n  by = \"sample\"\n)\n\nprint(df_joined)\n\n# A tibble: 5 × 3\n  sample  reads treatment\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    \n1 sampleA    10 drug     \n2 sampleB   100 nodrug   \n3 sampleC  1000 &lt;NA&gt;     \n4 sampleD    NA drug     \n5 sampleZ    NA nodrug   \n\n\nThere is a really nice visualization for all of the join strategies in R4DS here. Other kinds of joins are discussed in R4DS, but aren’t necessary for the exercises in this unit.\n\n\nPivoting data longer and wider with tidyr\nThe tidyr package provides convenient functions to reshape data:\n\npivot_longer(): Makes wide data longer, gathering columns into key-value pairs. This can make untidy data tidy.\npivot_wider(): Spreads long (tidy) data into wider format, creating new columns. Sometimes, untidy data is better for certain visualizations and analyses.\n\nLet’s create a simplified data frame with a few columns:\n\ncar_data &lt;- mtcars %&gt;%\n  dplyr::select(mpg, cyl, hp, wt) %&gt;%\n  rownames_to_column(var = \"car_name\")\n\nhead(car_data)\n\n           car_name  mpg cyl  hp    wt\n1         Mazda RX4 21.0   6 110 2.620\n2     Mazda RX4 Wag 21.0   6 110 2.875\n3        Datsun 710 22.8   4  93 2.320\n4    Hornet 4 Drive 21.4   6 110 3.215\n5 Hornet Sportabout 18.7   8 175 3.440\n6           Valiant 18.1   6 105 3.460\n\n\n\nrow_names_to_column converts the row names of the data frame into a column. This is generally good practice. It is part of the tibble package, included in the tidyverse.\nhead() prints the first 6 rows of a data frame to give you an easy to parse look at its contents\n\nSuppose we want to pivot this data so that mpg, hp, and wt become rows under a single “measurement” column, with their values in another column:\n\ncar_data_long &lt;- car_data %&gt;%\n  pivot_longer(\n    cols = c(mpg, hp, wt),\n    names_to = \"measurement\",\n    values_to = \"value\"\n  )\n\nhead(car_data_long)\n\n# A tibble: 6 × 4\n  car_name        cyl measurement  value\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n1 Mazda RX4         6 mpg          21   \n2 Mazda RX4         6 hp          110   \n3 Mazda RX4         6 wt            2.62\n4 Mazda RX4 Wag     6 mpg          21   \n5 Mazda RX4 Wag     6 hp          110   \n6 Mazda RX4 Wag     6 wt            2.88\n\n\n\ncols denotes the set of columns you want to “pivot”\nnames_to is the name of the new columns that will store the names of the columns in cols. We are “pivoting longer”, so these three columns in cols will no longer exist, with their content spread throughout the data frame. This new column (“measurement”) will track which rows correspond to information originally contained in these columns\nvalues_to is the name of the new column that will store the values of the original columns.\n\nWe can go back to the wide format by “pivoting wider”:\n\ncar_data_wide &lt;- car_data_long %&gt;%\n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n\nhead(car_data_wide)\n\n# A tibble: 6 × 5\n  car_name            cyl   mpg    hp    wt\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Mazda RX4             6  21     110  2.62\n2 Mazda RX4 Wag         6  21     110  2.88\n3 Datsun 710            4  22.8    93  2.32\n4 Hornet 4 Drive        6  21.4   110  3.22\n5 Hornet Sportabout     8  18.7   175  3.44\n6 Valiant               6  18.1   105  3.46\n\n\n\nnames_from specifies a column from which new column names will be derived.\nvalues_from will specify which column to get values that will be put int the new columns."
  }
]