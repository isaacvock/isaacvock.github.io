[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "In this blog, I plan to discuss the following topics:\n\nAnything and everything nucleotide recoding RNA-seq (NR-seq). Other metabolic labeling RNA-seq extensions are also potentially free game.\nThe use of statistics in bioinformatics. I developed a course on this topic and will use this setting to post some of my class material in blog format.\n\nI am also excited to use this as a platform to play around with the recently developed Quarto Live!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Introducing NR-seq by simulating NR-seq",
    "section": "",
    "text": "Nucleotide recoding RNA-seq (NR-seq; SLAM-seq, TimeLapse-seq, TUC-seq, etc.), is a set of methods to probe the dynamics of RNA. These methods use metabolic labeling. In this case, metabolic labeling means treating cells with a molecule that looks like a regular nucleotide (e.g., 4-thiouridine, or s4U), which cells incorporate into nascent RNA. NR-seq then employs a chemistry to modify the metabolic label so that a reverse transcriptase identifies it as a nucleotide different from what it originally mimiced (e.g., recoding s4U as a cytosine analog). This allows label incorporation events to be bioinformatically detected as apparent mutations in aligned sequencing reads.\nThis post introduces the main ideas behind the modeling of NR-seq data."
  },
  {
    "objectID": "posts/post-with-code/index.html#a-brief-introduction-to-nr-seq",
    "href": "posts/post-with-code/index.html#a-brief-introduction-to-nr-seq",
    "title": "Introducing NR-seq by simulating NR-seq",
    "section": "A brief introduction to NR-seq",
    "text": "A brief introduction to NR-seq\n\n\n\nSchematic of an NR-seq experiment. Adapted from Schofield et al. 2018.\n\n\nDeveloping a mechanistic understanding of gene expression regulation requires methods to probe the kinetics of RNA synthesis, processing, and degradation. While standard RNA-seq begins to solve this problem, it provides limited information about the kinetics of the processes which determine an RNA’s abundance. Nucleotide recoding RNA-seq (NR-seq; TimeLapse-seq, SLAM-seq, TUC-seq, etc.) overcomes these limitations. NR-seq combines metabolic labeling with novel chemistries that recode the hydrogen bonding pattern of a metabolic label so as to facilitate detection of labeled RNA via these chemically induced mutations in sequencing reads, absolving the need for biochemical enrichment of labeled RNA. By providing information about both overall RNA abundance and the dynamics of nascent and pre-existing RNA, NR-seq resolves the kinetic ambiguities of standard RNA-seq.\nOne way to intuit how metabolic labeling provides information about the dynamics of RNA is to consider one of the populations being tracked: the old, unlabeled RNA. Since this RNA can only degrade, its dynamics are entirely determined by its turnover kinetics. Combining this with the abundance information provided by standard RNA-seq provides information about the RNA’s synthesis kinetics (transcription + processing)."
  },
  {
    "objectID": "posts/post-with-code/index.html#understanding-nr-seq-by-simulating-nr-seq",
    "href": "posts/post-with-code/index.html#understanding-nr-seq-by-simulating-nr-seq",
    "title": "Introducing NR-seq by simulating NR-seq",
    "section": "Understanding NR-seq by simulating NR-seq",
    "text": "Understanding NR-seq by simulating NR-seq\nHere, we will walk through a basic simulation of NR-seq data to give you a sense as to how to think about and interpret NR-seq data.\n\nThe full shebang\nBelow is a summary of the entire simulation which we will walk through piece by piece in the following sections.\n\n\n\n\n\n\n\n\n\n\nMutable nucleotide content\nDifferent RNAs have different nucleotide content. Sequencing reads in standard short read sequencing experiments typically only sample a small portion of the entire RNA, and thus reads from a given species of RNA will also vary in their nucleotide content. In a standard NR-seq experiments, uridines in an RNA synthesized in the presence of metabolic label have an opportunity to be replaced with s4U. This, we need to simulate the number of Us in each read (prior to nucleotide recoding). Since we typically sequence cDNA, and this is also best thought as the number of Ts in the genomic sequence to which a given read aligned, we will refer to this as the number of T’s.\nLet’s consider simulating sequencing reads from a specific RNA species (e.g., a particular transcript isoform) first. In this case, the RNA will have a particular U-content, i.e., the fraction of nucleotides in its exonic sequence that are U’s. This is a simulation parameter, along with the number of reads we want to simulate, and the length of each read. It’s simplest to assume that each nucleotide in a read has some set probability of being a U, and that this probability is the same for all nucleotides. In this case, we can draw the number of U’s (or cDNA T’s) from a binomial distribution:\n\n\n\n\n\n\n\n\n\n\nMutational content\nThe number of mutations in an NR-seq sequencing read provides the information necessary to classify it as having come from either labeled or unlabeled RNA. If a read is from labeled RNA (or RNA that was synthesized during the label time; a given RNA molecule may not incorporate any metabolic label even if it could have), the mutational content of that read is a function of the following things:\n\nHow many mutable nucleotides are contained in the read, which we simulated above.\nHow often the metabolic label is incorporated in place of the standard nucleotide. We will also lump into this term the chemical efficiency of recoding, which is usually pretty high (&gt; 80%).\nThe background mutation rate, due to sequencing/RT/PCR errors, alignment errors, SNPs, etc.\n\nWe’ve already simulated the first factor. The other two are parameters that we will include in our simulation. Incorporation rates in successful NR-seq experiments are typically around 5%, and background mutation rates can range from nearly 0% to around 0.4%, depending on a number of factors. 0.2% is a fairly typical background mutation rate in my experience.\nFinally though, we need to determine the “newness” status of each sequencing read. Each species of RNA will have a characteristic “fraction new”, which is the fraction of RNA molecules present at RNA extraction time that were exposed to metabolic label. We can then model sequencing reads as being randomly drawn from this pool, with fraction new probability of sampling a labeled RNA. These processess are all well modeled with binomial distributions, so simulating all of this looks like:\n\n\n\n\n\n\n\n\nSome observations from the above visualizations:\n\nSome reads from unlabeled RNA have mutations. This is due to the non-zero background mutation rate.\nSome reads from labeled RNA have no mutations. This is due to the incorporation rate being much less than 100%. Experimentally, this is due to the metabolic label having to compete with the regular nucleotide for incorporation into nascent RNA.\n\nThese observations are what make analyzing NR-seq data challenging, and what will motivate analysis strategies discussed in other posts.\n\n\nRNA kinetics and NR-seq\nDifferent transcript isoforms can have very different properties. Of importance to an NR-seq experiment, different isoforms can differ in their nucleotide content and turnover kinetics. The latter’s impact is obvious, but my reason for bringing up the former may not be. In short, the rate at which a given isoform is degraded will determine its fraction new.\nTo see why turnover kinetics of RNA influences NR-seq data, consider the following model:\nIn it, RNA’s are synthesized at some rate ksyn, and degraded with a rate constant kdeg. kdeg is related to the average lifetime of a given RNA. When the rate at which a given species of RNA is degraded (kdeg * the amount of RNA that exists to be degraded) is equal to the rate at which it is synthesized (ksyn), that RNA is said to be at steady-state. A bit of algebra reveals that this occurs when the levels of RNA are equal to ratio of the synthesis and degradation rate constants:\n\\[\n\\begin{aligned}\n\\text{ksyn} & = \\text{kdeg}*[\\text{RNA}]_{\\text{ss}} \\\\\n\\frac{\\text{ksyn}}{\\text{kdeg}} & = [\\text{RNA}]_{\\text{ss}}\n\\end{aligned}\n\\]\nWhen you add metabolic label, you effectively create two species of RNA with distinct dynamics:\n\nOld RNA that existed at the time of labeling. These can only degrade and are no longer synthesized. In this model, this means that they exponentially degrade with rate constant kdeg.\nNew RNA that is synthesized during labeling. These will slowly accumulate to steady-state levels as they are both synthesized and degraded.\n\nAssuming that the synthesis and degradation rate constants are constant throughout the label time, the dynamics of old and new RNA looks like:\n\n\n\n\n\n\n\n\nBecause the rate constants are unchanging, so is the total amount of RNA at any given time. Thus, the amount of Old RNA + New RNA = steady-state RNA level. Because of this, there is a simple relationship between the turnover kinetics of an RNA and the fraction new (abbreviated fn) for that RNA:\n\\[\n\\begin{aligned}\n\\text{fn} &= \\frac{[\\text{New RNA}]}{[\\text{RNA}]} \\\\\n\\text{fn} &= \\frac{[\\text{RNA}]_{\\text{ss}}\\ast(1 - e^{-\\text{kdeg}\\ast \\text{tl}})}{[\\text{RNA}]_{\\text{ss}}} \\\\\n\\text{fn} &= 1 - e^{-\\text{kdeg}\\ast \\text{tl}}\n\\end{aligned}\n\\] where tl is the amount of time for which the cells were labeled. This simple relationship reveals some of the power of NR-seq. NR-seq provides information about RNA kinetics inaccessible to analyses of standard RNA-seq data (i.e., read counts). We can also use this relationship to modify our simulation and set the more biologically interpretable degradation rate constant rather than the fraction new:\n\n\n\n\n\n\n\n\n\n\nSimulating multiple transcripts\nSo far, we have focused on simulating data for a single transcript. In actuality, NR-seq is a high throughput method that provides information about all of the appreciably expressed transcripts in the cells from which you extracted RNA. Thus, we can set the number of transcripts we would like to simulate, and then draw kinetic parameters from a chosen distribution.\nOne thing we need to consider though is how the rate constants give rise to expected read counts for each transcript. We have discussed how the steady-state levels of a given RNA are a function of its synthesis and degradation rate constants, but is an RNA’s abundance related to its RNA-seq coverage?\nThe answer comes from realizing that RNA-seq is a measure of relative RNA abundance, not absolute abundance. That is, the number of reads you get from an RNA is a function of how abundant that RNA is, relative to all other RNAs in the sequenced pool. In addition, abundance in this setting is a function not only of the molecular abundance of an RNA (i.e., the number of molecules of that RNA present in the average cell), but also of the length of the RNA. This is because we are sequencing short fragments of an RNA, and thus the probability that we sequence a fragment from a given RNA depends on how many fragments in our pool come from that RNA. This is roughly \\([\\text{RNA}]_{\\text{ss}} \\ast \\text{Length}\\). In this simulation, we will make the simplifying assumption that all transcripts are the same length, and will thus only need to consider the relative steady-state abundances of each RNA. Thus, the parameters that we need to set are:\n\nThose determining the rate constant distributions from which we sample transcript-specific ksyn’s and kdeg’s.\nThe total number of reads in our library. These will be randomly divided among the various simulated transcripts.\nThose determining the U-content distribution from which we sapmle transcript-specific U-content’s.\n\nDeciding 1) and 3) is often best done through comparing candidate distributions to real data, and choosing parameters that make the simulated data look as close to the real data as possible. The result of this may look like:"
  },
  {
    "objectID": "posts/post-with-code/index.html#summary",
    "href": "posts/post-with-code/index.html#summary",
    "title": "Introducing NR-seq by simulating NR-seq",
    "section": "Summary",
    "text": "Summary\nIn this post, we introduced NR-seq, and built a simulation of NR-seq data to explore several aspects of NR-seq data. In the next post, we will discuss various strategies for analyzing this data, using the simulation we built here to test our strategies."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isaac Vock",
    "section": "",
    "text": "I am a PhD student at Yale University in the lab of Matthew Simon. I have developed a number of bioinformatic tools for processing and analyzing nucleotide recoding RNA-seq (NR-seq) experiments. NR-seq is a class of methods for quantifying the kinetics of RNA synthesis and degradation, and includes methods like TimeLapse-seq (developed in the Simon lab), SLAM-seq, TUC-seq, etc. These are powerful tools that are allowing us to study the dynamic lives of RNA at unprecedented resolution, and I have had a lot of fun dreaming up ways in which we can extract exciting biological insights from NR-seq data.\nI am also a statistician, and have developed a number of novel methods for analyzing NR-seq data. One of my passions is helping biologists better understand the statistical methods powering their favorite bioinformatic tools. This led me to develop a course at Yale called “Statistical Intuition for Modern (RNA) Biochemists”. It’s inspired by Susan Holmes and Wolfgang Huber’s book of a similar title and Richard McCelreath’s cultural phenomenon “Statistical Rethinking”. It aims to be an accessible introduction to statistics for biochemistry majors. We will be covering the basic machinery of statistical modeling and its use in popular methods like linear modeling and clustering. Rather than relying on mathematical formalism to convey these concepts though, we will be making use of simulations and interactive exercises to help students develop an intuition for key concepts. Finally, we will start the course by introducing student’s to RNA-seq, so as to have a common data language that we can connect all conecpts back to. Eventually, all of the course material will be hosted at this repo."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Isaac Vock",
    "section": "Education",
    "text": "Education\n\n\nYale University | New Haven, CT | August 2019 - Present\nPhD in Molecular Biophysics and Biochemistry\n\n\nCentre College | Danville, KY | August 2015 - May 2019\nB.S. in Physics, minor in Math"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Isaac Vock",
    "section": "Experience",
    "text": "Experience\n\n\nProgramming in R (since 2019)\nScripting (example repo)\nR package development (most notably, EZbakR)\nShiny app development (e.g., interactive TimeLapse-seq simulator)\n\n\nSnakemake pipeline development (since 2021)\nfastq2EZbakR: Flexible processing of NR-seq data.\nNRsim: Simulating NR-seq data to test analysis strategies and pipelines.\nAnnotationCleaner: Assembling annotations using StringTie and some custom scripts.\nTHE_Aligner: Aligning almost any kind of RNA-seq data.\nPROseq_etal: PRO-seq/ChIP-seq/ATAC-seq pipeline.\n\n\nOther experience\nPython (since 2021; used throughout Snakemake pipelines listed above)\nC (since 2023; example repo)\nPytorch (since 2023; example repo)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "My main projects",
    "section": "",
    "text": "Introduced Snakemake pipeline (fastq2EZbakR) and R package (EZbakR)\nGeneralizes feature assignment: the set of annotated genomic features that you can perform analyses on.\nGeneralizes mixture modeling: how the fraction of reads from labeled RNA are estimated.\nGeneralizes dynamical systems modeling: how kinetic parameters are estimated from NR-seq data.\nGeneralizes comparative analyses: how kinetic parameters are compared across biological conditions.\n\nLink to preprint"
  },
  {
    "objectID": "about.html#the-ezbakr-suite",
    "href": "about.html#the-ezbakr-suite",
    "title": "My main projects",
    "section": "",
    "text": "Introduced Snakemake pipeline (fastq2EZbakR) and R package (EZbakR)\nGeneralizes feature assignment: the set of annotated genomic features that you can perform analyses on.\nGeneralizes mixture modeling: how the fraction of reads from labeled RNA are estimated.\nGeneralizes dynamical systems modeling: how kinetic parameters are estimated from NR-seq data.\nGeneralizes comparative analyses: how kinetic parameters are compared across biological conditions.\n\nLink to preprint"
  },
  {
    "objectID": "about.html#bakr",
    "href": "about.html#bakr",
    "title": "My main projects",
    "section": "bakR",
    "text": "bakR\n\nSummary\n\n\n\nIntroduced an R package (bakR) designed to compare RNA synthesis and degradation rate constants across biological conditions.\nAlso introduced a hierarchical modeling strategy to increase statistical power of these comparisons.\n\nLink to paper"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Isaac’s blog",
    "section": "",
    "text": "Intro to R: Day 1\n\n\n\n\n\n\nstats\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing NR-seq data: the basics\n\n\n\n\n\n\nnrseq\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing NR-seq by simulating NR-seq\n\n\n\n\n\n\nnrseq\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\nIsaac Vock\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnrseq\n\n\nstats\n\n\n\n\n\n\n\n\n\nDec 13, 2024\n\n\nIsaac Vock\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nrseq_analysis/index.html",
    "href": "posts/nrseq_analysis/index.html",
    "title": "Analyzing NR-seq data: the basics",
    "section": "",
    "text": "In my last post, I introduced NR-seq by walking through the development of an NR-seq simulator. That post implicitly introduced some of the complexities of interpreting NR-seq data. In this post, we will tackle these challenges head-on and build up a rigorous strategy by which to analyze NR-seq data. We will do this in a piece-meal fashion, first developing a simple but flawed strategy, until eventually working up to mixture modeling (the current gold-standard for NR-seq analyses). No statistical model is perfect though, so we will finish with a discussion and exploration of the limitations of this gold-standard."
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#nr-seq-a-reminder",
    "href": "posts/nrseq_analysis/index.html#nr-seq-a-reminder",
    "title": "Analyzing NR-seq data: the basics",
    "section": "NR-seq: a reminder",
    "text": "NR-seq: a reminder\nIn an NR-seq experiment, there are two populations of RNA: those synthesized in the presence of label (a.k.a. labeled, or new, RNA) and those which were synthesized prior to metabolic labeling (a.k.a unlabeled, or old, RNA). The first task of any NR-seq analysis is for a given species of RNA (e.g., RNA transcribed from a particular gene), quantify the relative amounts of these two populations. This is referred to as that species’ “fraction new” or “new-to-total ratio (NTR)”. Downstream analyses are then aimed at interpreting these fraction news/NTRs. This post will only concern itself with fraction new estimation. I will use the term “fraction new” for the remainder of this post.\nTo estimate the fraction new, we rely on the mutational content of mapped sequencing reads. NR-seq involves chemically recoding metabolic label (e.g., s4U) so that reverse transcriptase reads it as a different nucleotide (e.g., a cytosine). Thus, reads from new RNA will have, on average, more mutations than reads from old RNA. This observation is the key to analyzing NR-seq data.\nTo test the strategies discussed, we will use simulated data. This allows us to know the ground truth and explore the robustness of any approach. Here is the function that we will use to simulate data, as well as some helper functions we can use to assess analysis strategies:"
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#a-simple-approach-mutational-cutoffs",
    "href": "posts/nrseq_analysis/index.html#a-simple-approach-mutational-cutoffs",
    "title": "Analyzing NR-seq data: the basics",
    "section": "A simple approach: mutational cutoffs",
    "text": "A simple approach: mutational cutoffs\nIf reads from new RNA have more mutations on average than those from old RNA, maybe we can just use a simple mutational cutoff to classify individual reads as from old or new RNA. The fraction of reads that come from the latter is then our estimate for the fraction new. This approach has been popular since the advent of NR-seq, and is implemented in popular bioinformatic tools for analyzing NR-seq data like SLAMDUNK. Let’s simulate some data and test out this approach\n\n\n\n\n\n\n\n\nIf you run this code with the default simulation parameters, you’ll see that the estimates are decent. The 1+ mutation cutoff for newness looks better than the 2+ cutoff, with the former yielding estimates that consistently correlate pretty well with the simulated ground truth.\nSo that’s all it takes to analyze NR-seq data? Not so fast. In our simulation, there is a default metabolic label incorporation + conversion rate of 5%. While this is a standard “good” incorporation rate, if you analyze as many NR-seq datasets as I have you will quickly notice that there is a lot of dataset-to-dataset variation in the incorporation rate. For example, there is a tremendous amount of cell line-to-cell line variation in the readiness of s4U incorporation, with some cell lines (e.g., HEK293 and HeLa) uptaking s4U with great tenacity and others (e.g., neuronal cell lines) having typically much lower s4U incorporation rates. In addition, incorporation rates also can correlate with biological condition. For example, knocking out key factors in RNA metabolism (e.g., degradation factors) can significantly impact incorporation rates. In general, incorporation rates seem to correlate strongly with general metabolic rates, and anything that perturbs these rates will likely affect incorporation rates.\nThis lattermost observation is particularly dangerous when it comes to applying the simple mutation content cutoff analysis strategy. Often, we don’t just care about what an RNA’s dynamics look like in one biological condition, but rather how it differs between two more different conditions (e.g., WT vs. KO of your favorite gene, untreated vs. drug treated, etc.). If an analysis method is not robust to variation in incorporation rates, it risks making technical variability look like biological signal.\nThus, what happens if we simulate a different incorporation rate? If you tweak the simulation above (set phigh in simulate_nrseq() to a different value than its default of 0.05 and rerun code):\n\n\n\nAccuracy of cutoff approach for range of phighs\n\n\nThe key takeaway from this investigation is that the accuracy of the cutoff-based approach is heavily reliant on the incorporation rate. Since incorporation rate often correlates with biology, this represents a dangerous confounder for mutation cutoff analyses. We need a more robust analysis strategy."
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#a-better-idea-statistical-modeling",
    "href": "posts/nrseq_analysis/index.html#a-better-idea-statistical-modeling",
    "title": "Analyzing NR-seq data: the basics",
    "section": "A better idea: statistical modeling",
    "text": "A better idea: statistical modeling\nThe problem with the cutoff based approach is two-fold:\n\nIt’s possible for reads from labeled RNA to have no mutations. This is because the metabolic label has to compete with the regular nucleotide for incorporation, which is what keeps incorporation rates relatively low in most NR-seq experiments.\nIt’s possible for reads from unlabeled RNA to have mutations. This can be due to RT errors, sequencing errors, alignment errors, unmasked SNPs, etc.\n\nThus, a mutation in a read does not make it definitively from new RNA, and a lack of mutations does not make it definitively from old RNA. How can we navigate this inherent uncertainty? This is exactly what statistical modeling was built for.\nStatistical modeling first means coming up with a model that specifies how likely every possible data point is. If you tell me the number of mutable nucleotides, the number of mutations in a read, whether it came from old or new RNA, and whatever can be specified about the process by which mutations arise in reads, I should be able to use this model to calculate a likelihood for that piece of data.\n\n\n\n\n\n\nWhat is a data point’s “likelihood”?\n\n\n\n\n\nThe likelihood of a data point is the probability of seeing that data, given all of the information you provided, often written as P(data | parameters). In this case, we are dealing with discrete data (integer mutation counts), meaning that this likelihood can also be interpreted as the probability of getting that data point given all of the specified parameters. In a continuous setting, interpreting this is a bit more complicated, as the probability of any specific continuous outcome is 0.\n\n\n\nIn practice, this often involves specifying a convenient to work with probability distribution that describes the variability in your data. To do this, you need to make some assumptions about your data. For NR-seq data, it is common to assume:\n\nFor reads from new RNA, there is a set probability (call it phigh) that a given mutable nucleotide (e.g., uridines in an s4U labeling NR-seq experiment) is mutated. This phigh is the same for all such reads, regardless of the RNA species of origin.\nFor reads from old RNA, there is also a set probability of mutation (call it pold) for all such reads.\nAll nucleotides are independent. Whether or not a given nucleotide is mutated has no impact on the probability that other nucleotides in that read are also mutated (given the status of the read as coming from old or new RNA).\n\nThese are actually the exact assumptions that we used to simulate data above and in the introduction to NR-seq blog. These assumptions lend themselves to a particular model: a two-component binomial mixture model.\n\nTwo-component binomial mixture model\n“Two-component binomial mixture model” is a mouthful, so let’s break it down.\n“Two-component” = the model supposes that there are two populations in your data. In our case, this is reads from old RNA and reads from new RNA.\n“binomial” = data from each of the populations is modeled as following a binomial distribution. We’ve seen this distribution in the intro to NR-seq post. It describes a situation where you have a certain number of independent “trials” (e.g., mutable nucleotides), with a set probability of “success” (e.g., mutation of the nucleotide) for each trial.\n“mixture model” = you don’t know which population any given data point comes from. This is known as a “latent-variable model”, which can pose some computational challenges when trying to estimate the parameters of such a model. These challenges will turn out to be fairly easy to navigate in this setting, but will limit our efforts to extend and improve this model in future sections.\nTo summarize, we are assuming that each sequencing read comes from one of two populations: old RNA or new RNA. The mutational content of both types of reads is well modeled as following a binomial distribution. The parameters of these binomial distributions are the number of mutable nucleotides and the probability that each of these nucleotides gets mutated. We don’t need to estimate the number of mutable nucleotides (this is just more data), but we do not know a priori the two mutation rates. Thus, we need to estimate these two parameters, as well as the quantity of primary interest: the fraction new. We can schematize this model as such:\n\n\n\nTwo-component binomial mixture model"
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#fitting-a-two-component-binomial-mixture-model",
    "href": "posts/nrseq_analysis/index.html#fitting-a-two-component-binomial-mixture-model",
    "title": "Analyzing NR-seq data: the basics",
    "section": "Fitting a two-component binomial mixture model",
    "text": "Fitting a two-component binomial mixture model\n\nThe basics\nA TCBMM has three parameters that need to be estimated for each RNA feature:\n\nThe fraction new\nThe probability of a mutation in reads from new RNA (\\(p_{\\text{high,TC}}\\) in the TCBMM figure above)\nThe probability of a mutation in reads from old RNA (\\(p_{\\text{low,TC}}\\) in the TCBMM figure above)\n\n\n\n\n\n\n\nWhat is an “RNA feature”?\n\n\n\n\n\nOur goal is to estimate the fraction of RNA molecules that are new/labeled for a given species of RNA. Our definition of “species” is technically flexible, and is what I refer to as an “RNA feature”. The most common choice for a feature is a gene. That is, we estimate the fraction of RNA molecules produced from a given gene that are new. In practice though, there are a lot more features we may be interested in analyzing. See the EZbakR preprint for a description of some other options.\n\n\n\nIn this post, we will estimate these via the method of maximum likelihood. That means we will find parameter estimates that maximum the likelihood of our data. In theory, this is simple: just write a function to calculate the likelihood for any combination of parameter values and data, and use the optimization algorithm of your choice. Here’s what that might look like for the TCBMM:\n\n\n\n\n\n\n\n\nAlready, this quickly whipped up strategy is working pretty well. For one, we have largely solved the problem of phigh/plow dependence on estimate accuracy. Play around with different values of plow/phigh in the simulation and prove this for yourself, but that is the main advantage of the TCBMM approach. You can also see that the more data we have, the better our estimates get (on average). This is a nice trend, and means that paying for more sequencing depth can have a significant positive impact on the quality of our estimates. Technically, a similar trend holds for the mutation content cutoff strategy, but because we can’t be sure if our estimates are biased or not, more reads could just yield higher confidence wrong estimates.\n\n\nComplete pooling to improve estimate stability\nOne thing you should note though is that there are a handful of highly inaccurate estimates. These are almost always low coverage features, but can we do better? I will argue yes, thanks to a two-step fitting approach that is implemented in tools like bakR/EZbakR and GRAND-SLAM.\nHaving to estimate both the labeled/unlabeled read mutation rates as well as the fraction of reads from each population is fundamentally challenging. While it is technically identifiable (ignoring label flipping, which is easy to deal with in this setting), low coverage features typically have too little information to accurately estimate all of these parameters. Does a feature have only a few high mutation content reads because the fraction new is low, or because the mutation rate in reads from new RNA (phigh) low? Tough to distinguish these two if you only have 10s of reads. The common solution to this problem is “complete pooling of the mutation rate estimates”.\nComplete pooling refers to a spectrum of model archetypes possible in multi-group analyses (e.g., estimating the fraction new for multiple different RNAs, like RNAs produced from different genes). Instead of estimating a separate phigh and plow for each feature, how about we use all of the data to calculate a single phigh and plow for all features? Doing this means assuming that there is very little feature-to-feature phigh or plow variation, but there is decent evidence that this assumption often holds, especially if analyzing data with standard label times (e.g., multi-hour label times in human cell lines).\nThe modified strategy might look like:\n\n\n\n\n\n\n\n\nThis should look notably better. For example, here is what I get from a 200 feature simulation for a range of phighs:\n\n\n\nAccuracy of cutoff approach for range of phighs\n\n\nOf course, it will always be more difficult to estimate the fraction new for a low coverage feature vs. a high coverage one. Despite this, complete pooling of the mutation rate estimates has significantly stabilized low coverage estimates, making them far more accurate than in the no pooling case."
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#going-beyond-tcbmm",
    "href": "posts/nrseq_analysis/index.html#going-beyond-tcbmm",
    "title": "Analyzing NR-seq data: the basics",
    "section": "Going beyond TCBMM",
    "text": "Going beyond TCBMM\nThe power of mixture modeling lies both in its robustness as well as its extensibility. TCBMM makes several assumptions about the mutational content of NR-seq reads. Namely that:\n\nEvery uridine in a labeled RNA was equally likely to get replaced with s4U\nEvery uridine captured by a sequencing read was equally likely to give rise to a non-s4U-related mutation due to sequencing errors, alignment errors, etc.\n\nIn future posts, I will discuss a number of ideas for how to extend and improve NR-seq TCBMM’s. These include:\n\nThree-component mixture modeling, where a second population of reads from unlabeled RNA with a higher mutation rate (presumably due to heightened alignment errors) is modeled.\nOverdisperse mixture modeling where either an overdisperse binomial distribution (e.g., a beta-binomial) replaces one or both of the binomial distribution components, or where a different incorporation rate parameter is estimated for fast and slow turnover RNA.\nHierarchical mixture modeling where a sample-wide average incorporation rate is inferred and used as a strongly regularizing prior to estimate feature-specific incorporation rates.\nModeling the transcription process, which at short label times leads to an expected position-dependency in the incorporation rate (more 5’ nucleotides will be on average less well labeled than more 3’ nucleotides).\n\nWhile all of these are theoretically promising, the challenge of fitting more complex models is two-fold:\n\nTheir increased flexibility comes with an increased risk of overfitting. In this setting, this leads to estimate instability, where a better model fit yields extreme conclusions about RNA dynamics (i.e., unusually high fraction new and thus unrealistically rapid turnover kinetics).\nWhile an alternative model may capture one aspect of the true data generating process unaccounted for by TCBMM, it may amplify biases that arise from not accounting for some other aspect of the data generating process.\n\nMore on this in later posts!"
  },
  {
    "objectID": "posts/nrseq_analysis/index.html#summary",
    "href": "posts/nrseq_analysis/index.html#summary",
    "title": "Analyzing NR-seq data: the basics",
    "section": "Summary",
    "text": "Summary\nIn this post, we introduced two common NR-seq analysis strategies: mutational content cutoffs and two-component binomial mixture models (TCBMM’s). We saw how while the former is easy to implement, efficient, and intuitive, it risks providing biased estimates. In addition, the magnitude of these biases is a function of technical details that can vary between biological conditoins. This is why TCBMM’s are typically superior for estimating the fraction of reads that come from labeled RNA in an NR-seq experiment. We also explored how complete pooling of phigh and plow estimates can improve the accuracy of fraction new estimates, especially for low coverage features."
  },
  {
    "objectID": "posts/Rintro_day1/index.html",
    "href": "posts/Rintro_day1/index.html",
    "title": "Intro to R: Day 1",
    "section": "",
    "text": "This worksheet will walk you through some basic concepts in R. I would suggest copying code shown here into an R script and running it yourself so that you can play around with the presented examples."
  },
  {
    "objectID": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-simple_calc",
    "href": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-simple_calc",
    "title": "Intro to R: Day 1",
    "section": "Pre-requisite knowledge for simple_calc()",
    "text": "Pre-requisite knowledge for simple_calc()\n\nMath in R\nThe simplest use case of R is using it to do math:\n\n1 + 1 # Addition\n\n[1] 2\n\n10 - 1.5 # Subtraction\n\n[1] 8.5\n\n2 * 3 # Multiplication\n\n[1] 6\n\n10.12 / 17.99 # Division\n\n[1] 0.5625347\n\n5 ^ 2 # Exponentiation\n\n[1] 25\n\n\n\n\nNumeric variables in R\nYou can store numbers in “variables”. This is like a special box in your computer’s memory labeled with a name (like my_number). When you put a number into this box (for example, 10), we say you have assigned the value 10 to the variable my_number.\nIn R, you’d do this by writing:\n\nmy_number &lt;- 10\nmy_number\n\n[1] 10\n\n\n\n\n\n\n\n\nSeeing the value of variables\n\n\n\n\n\nTyping and executing print(my_number) or just my_number will print out the value of the variable to your console.\n\n\n\nHere is what’s happening in this code:\n\nmy_number is the label on the box in memory.\n&lt;- is like an arrow pointing from the value to the box, meaning “put this value in that box”.\n`10 is the actual number you are storing.\n\nYou can then do math like this just like with regular numbers:\n\nmy_number * 2\n\n[1] 20\n\nmy_number ^ 3\n\n[1] 1000\n\nmy_number - 4\n\n[1] 6\n\n\nmy_number does not change value in any of the above lines. To change the value of my_number, you would have to assign it the new value with &lt;- again:\n\nmy_number # my_number is 10\n\n[1] 10\n\nmy_number &lt;- 1001 # my_number is now 1001\nmy_number # Confirm new value of my_number\n\n[1] 1001\n\n\n\n\nStrings in R\nYou can store more than numbers in variables. For example, you can store text, which is referred to as a “string”:\n\nmy_string &lt;- \"Hello\"\nmy_string2 &lt;- 'Bye'\n\nmy_string\n\n[1] \"Hello\"\n\nmy_string2\n\n[1] \"Bye\"\n\n\nYou tell R that you are storing text by wrapping that text in \"\" or ''.\n\n\n\n\n\n\nUseful string tools (aka functions)\n\n\n\n\n\nBelow are some useful tools that R provide you to work with strings. These are called functions, a concept discussed later.\n\npaste(..., sep = \" \"): paste() allows you to stitch together multiple strings, with a chosen separator text between strings (sep argument). Having no separator (sep = \"\") is identical to using a different function paste0():\n\n\nstring1 &lt;- \"Hello\"\nstring2 &lt;- \"friend.\"\nstring3 &lt;- \"It's been too long\"\n\npaste(string1, string2)\n\n[1] \"Hello friend.\"\n\npaste(string1, string2, sep = \"\")\n\n[1] \"Hellofriend.\"\n\npaste0(string1, string2)\n\n[1] \"Hellofriend.\"\n\npaste(string1, string2, string3)\n\n[1] \"Hello friend. It's been too long\"\n\npaste(string1, string2, collapse = \"_\")\n\n[1] \"Hello friend.\"\n\n\n\nnchar(): This will give you the number of individual characters in your text string:\n\n\nstring1 &lt;- \"Hello\"\nnchar(string1)\n\n[1] 5\n\n\n\ngsub(pattern, replacement, x): This allows you to look for the string pattern in the query string x, and replace it with the string replacement:\n\n\ntext &lt;- \"Hello, Hello, Hello!\"\ngsub(\"Hello\", \"Hi\", text)\n\n[1] \"Hi, Hi, Hi!\"\n\n\n\ngrepl(pattern, x): This is similar to gsub() but just searches for string pattern in string x and spits out TRUE if it finds it\n\n\ntext &lt;- \"Hello, Hello, Hello!\"\ngrepl(\"Hello\", text)\n\n[1] TRUE\n\n\nThere is a whole R package called stringr devoted to making working with strings in R easier and more intuitive, so you might want to look into that as well!\n\n\n\n\n\nBooleans in R\nAnother thing that is commonly stored in variables is logical values (TRUE or FALSE), otherwise known as “booleans”:\n\nmy_bool &lt;- TRUE\nmy_bool2 &lt;- FALSE\n\nmy_bool\n\n[1] TRUE\n\nmy_bool2\n\n[1] FALSE\n\n\nYou can do a sort of math with booleans, referred to as “boolean logic”. This takes as input two (in the case of AND and OR) or one (in the case of NOT) boolean variables and outputs a new boolean. The most common examples are:\nAND (&)\n\nBoth of the booleans must be TRUE for the output to be TRUE:\n\n\nTRUE & TRUE # This is TRUE\n\n[1] TRUE\n\nTRUE & FALSE # This is FALSE\n\n[1] FALSE\n\nFALSE & TRUE # This is FALSE\n\n[1] FALSE\n\nFALSE & FALSE # This is FALSE\n\n[1] FALSE\n\n\nOR (|)\n\nAt least one of the booleans must be TRUE for the output of this to be TRUE\n\n\nTRUE | TRUE # This is TRUE\n\n[1] TRUE\n\nTRUE | FALSE # This is TRUE\n\n[1] TRUE\n\nFALSE | TRUE # This is TRUE\n\n[1] TRUE\n\nFALSE | FALSE # This is FALSE\n\n[1] FALSE\n\n\nNOT (!)\n\nUnlike AND and OR, this takes a single boolean value as input\nThis reverses the value of the boolean:\n\n\n!TRUE # This is FALSE\n\n[1] FALSE\n\n!FALSE # This is TRUE\n\n[1] TRUE\n\n\nFinally, you can compare the value of two variables to see if they are the same. If the are variable_1 == variable_2 will return TRUE, otherwise it will return FALSE:\n\n\"Hello\" == \"Hello\" # TRUE\n\n[1] TRUE\n\n\"Hi\" == \"Bye\" # FALSE\n\n[1] FALSE\n\n1 == 1 # TRUE\n\n[1] TRUE\n\nmy_number &lt;- 1\nmy_number2 &lt;- 2\nmy_number == my_number2\n\n[1] FALSE\n\n\n\n\nFunctions in R\nA function in R is like a “recipe” for a mini “machine” that does one specific job. You give it some inputs (called arguments), it follows the steps you’ve defined, and then it gives you a result.\nFunctions help you organize your code so you can reuse it instead of writing the same steps again and again. Here is a simple example:\n\n# Function name: my_function\n# Arguments: x and y\n# Output: x + y\nmy_function &lt;- function(x, y){\n  \n  # 1. Inside the curly braces, write the steps of what you will do with x and y\n  \n  # We will add x and y\n  result &lt;- x + y\n  \n  # 2. Tell the function what to output (i.e., its \"return value\")\n  return(result)\n  \n}\n\n\nmy_function is the name of the function (like a label on the mini machine).\nfunction(x,y) { ... } says “I am creating a function that expects two inputs, called x and y.\nInside the { ... }, you can write as much code as you want; this is the instructions for what you want the function to do with the inputs\nreturn(result) sends the output of the function back to you.\n\nAfter creating my_function, you can call it (computer science lingo meaning “use the function”) by typing:\n\nmy_function(3,5)\n\n[1] 8\n\nmy_new_number &lt;- my_function(2, 2)\nmy_new_number\n\n[1] 4\n\n\nSometimes, you want one (or more) of your function’s inputs to have a “fallback” value if the user doesn’t supply one. That’s where default arguments come in. For example:\n\nmy_new_function &lt;- function(x, y = 10){\n\n  result &lt;- x + y\n  \n  return(result)\n  \n}\n\nmy_new_function now only needs you to supply x. You can supply x and y, but if you don’t supply y, it will give y a default value of 10 by default:\n\nmy_new_function(x = 1)\n\n[1] 11\n\nmy_new_function(x = 2, y = 20)\n\n[1] 22\n\nmy_new_function(2, 20) # Will fill arguments in order, so x = 2 and y = 20 here\n\n[1] 22\n\n\n\n\n\n\n\n\nAdvanced function advice\n\n\n\n\n\n\nTip 1: argument with small set of possible values\nSometimes, one of the arguments of your function may have a set number of possible values that you intend for a user to input. You can specify this as such:\n\nmy_options &lt;- function(a, b, greeting = c(\"Hi\", \"Bye\", \"Huh?\")){\n  \n  # Check to make sure the user supplied one of the valid options\n  greeting &lt;- match.arg(greeting)\n  \n  print(greeting)\n  \n  result &lt;- a + b\n  \n  return(result)\n  \n}\n\n# Uses first option by default\nmy_options(2, 2)\n\n[1] \"Hi\"\n\n\n[1] 4\n\nmy_options(2, 2, \"Huh?\")\n\n[1] \"Huh?\"\n\n\n[1] 4\n\n\n\n\nTip 2: Catching errors\nIn all of our examples so far, we have assumed that the user has supplied a particular kind of data for each argument. Mostly, we have assumed that numbers are being passed to many of our example functions, numbers that we can add. What if they messed up though and passed a string, for example? We can catch this and throw an error message:\n\nmy_valuecheck &lt;- function(a, b){\n  \n  # check if a is a number\n  stopifnot(is.numeric(a))\n  \n  # check if b is a number, but with a slightly different strategy\n  # if-statements are discussed more later.\n  if(!is.numeric(b)){\n    \n    stop(\"b isn't a number\")\n    \n  }\n  \n  result &lt;- a + b\n  \n  return(result)\n}\n\nThis function will work as normal if a and b are numbers, but will throw informative error messages if not. You will also get an error in the old version of this function that didn’t have the stopifnot() lines, but this error might be far more cryptic and hard to understand. You will also get a different error depending on what is wrong about a and/or b, further confusing you or other users of this function.\n\n\n\n\n\n\nControl flow (if-else statements)\nAn if-else statement is one of the most common ways to control the flow of a program. It lets your code make decisions based on whether a condition is TRUE or FALSE.\n\nif checks if something is TRUE\nelse covers what happens if it is not TRUE\nYou can add extra steps in between using else if to handle different possible conditions\n\nThe basic structure looks like:\n\nif (condition1){\n  # This code runs if 'condition1' is TRUE\n}else if(condition2){\n  # This code runs if 'condition2' is TRUE\n}else{\n  # This code runs if both 'condition1' and 'condition2' are FALSE\n}\n\nThink of this code as asking a set of questions:\n\nIf condition1 is TRUE, do something.\nElse if condition2 is TRUE, do something else\nElse, if neither condition1 nor condition2 are TRUE, do a default thing.\n\nA real example might look like:\n\nx &lt;- 5\n\nif(x &gt; 3){\n  print(\"x is greater than 3\")\n}else if(x &lt; 5){\n  print(\"x is between 3 and 5\")\n}else{\n  print(\"x is greater than or equal to 5\")\n}\n\n[1] \"x is greater than 3\"\n\n\nConditions in R must evaluate to a single TRUE or FALSE. Common ways to form conditions are comparison operators:\n\n==: Check if two things are equal (e.g., a == b). a and b can be numbers, strings, booleans, etc.\n!=: Check if two things are not equal 1 &lt;, &gt;, &lt;=, &gt;=: Less than, greater than, less than or equal to, or greater than or equal to, respectively.\n\nHere is an example of how you might use control flow in a function:\n\ngreetUser &lt;- function(user_input){\n  \n  # Check if user_input equals \"Hello\"\n  if (user_input == \"Hello\"){\n    return(\"Hi there! Nice to meet you.\")\n  }else if(user_input == \"Goodbye\"){\n    return(\"See you later! Take care.\")\n  }else{\n    return(\"I'm not sure how to respond to that...\")\n  }\n  \n}\n\ngreetUser(\"Hello\")\n\n[1] \"Hi there! Nice to meet you.\"\n\ngreetUser(\"Comment allez-vous?\")\n\n[1] \"I'm not sure how to respond to that...\""
  },
  {
    "objectID": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-vector_calc",
    "href": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-vector_calc",
    "title": "Intro to R: Day 1",
    "section": "Pre-requisite knowledge for vector_calc()",
    "text": "Pre-requisite knowledge for vector_calc()\n\nVectors\nIn R, a vector is a container that holds multiple values of the same data type (such as numbers, strings, or booleans). You can think of it like a row of boxes, each containing a value of the same kind.\nYou can create a vector with the c() function (short for “combine” or “concatenate”). Here are a few example:\n\n# A numeric vector\nnumbers &lt;- c(10, 20, 30, 40)\n\n# A character (string) vector\nwords &lt;- c(\"cat\", \"dog\", \"bird\")\n\n# A boolean vector\nbools &lt;- c(TRUE, FALSE, TRUE)\n\nOften, you will want to access specific elements or sets of elements of a vector. To do this, you can use square brackets [ ]:\n\n# Get the first element of 'numbers'\nnumbers[1] # 10\n\n[1] 10\n\n# Get the second element of 'words'\nwords[2]\n\n[1] \"dog\"\n\n# Get multiple elements at once\nnumbers[c(1, 3)] # This gives the 1st and 3rd elements: c(10, 30)\n\n[1] 10 30\n\n# Exclude specific elements\nbools[-1] # This gives everything but the 1st element\n\n[1] FALSE  TRUE\n\n\nYou can also change values of specific elements:\n\n# See what 'numbers' is now\nnumbers\n\n[1] 10 20 30 40\n\n# Change a value\nnumbers[2] &lt;- 99\n\n# Check 'numbers' now\nnumbers\n\n[1] 10 99 30 40\n\n\nSometimes, it will be useful to check what kind of data is in a vector. This can be done with the class() function:\n\nclass(numbers) # \"numeric\"\n\n[1] \"numeric\"\n\nclass(words) # \"character\"\n\n[1] \"character\"\n\nclass(bools) # \"logical\" (another word for boolean)\n\n[1] \"logical\"\n\n\nYou can also check the value with functions like is.numeric(). is.character(), or is.logical():\n\nis.numeric(numbers) # TRUE\n\n[1] TRUE\n\nis.numeric(words) # FALSE\n\n[1] FALSE\n\nis.character(numbers) # FALSE\n\n[1] FALSE\n\nis.logical(numbers) # FALSE\n\n[1] FALSE\n\nis.character(words) # TRUE\n\n[1] TRUE\n\nis.logical(bools) # TRUE\n\n[1] TRUE\n\n\n\n\n\n\n\n\nUseful vector functions\n\n\n\n\n\nBelow are some useful functions that allow you to create vectors or lookup some information about a vector:\n\nlength(v): returns the number of elements in the vector v:\n\n\nlength(c(1, 2, 3))\n\n[1] 3\n\n\n\nseq(from, to, length.out) or seq(from, to, by): Creates a vector starting from the number from (default value of 1), to the number to (default value of 1). If you set length.out, then you will get a vector of length.out elements. If you set by, then you specify the distance between adjacent elements:\n\n\nseq(from = 1, to = 5, length.out = 5)\n\n[1] 1 2 3 4 5\n\nseq(from = 1, to = 5, by = 1)\n\n[1] 1 2 3 4 5\n\n\n\nrep(x, times): Creates a vector containing the value x repeated times times:\n\n\nrep(x = 1, times = 10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\n\nstart:end: Same as seq(from = start, to = end, by = 1):\n\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n0.5:2.5\n\n[1] 0.5 1.5 2.5\n\n\n\n\n\n\n\nLoops\nA loop is a way to tell R to “do something multiple times”. This unlocks one of the powerful aspects of computers: their ability to do multiple things quickly.\nThere are two commonly used types of loops: for loops and while loops.\nA for loop in R iterates (or “loops”) over each element of a vector and does something with it. For example, if we want to print every element of a numeric vector:\n\nnumbers &lt;- c(10, 20, 30, 40)\n\n# Loop over the values\nfor(value in numbers){\n  print(value)\n}\n\n[1] 10\n[1] 20\n[1] 30\n[1] 40\n\n# Loop over the vector 1 to the length of the vector\nfor(i in 1:length(numbers)){\n  \n  print(numbers[i])\n \n}\n\n[1] 10\n[1] 20\n[1] 30\n[1] 40\n\n# Fancier alternative to the above code\nfor(i in seq_along(numbers)){\n  \n  print(numbers[i])\n  \n}\n\n[1] 10\n[1] 20\n[1] 30\n[1] 40\n\n\nWhat’s happening here?\n\nfor (value in numbers) means “go through each element of numbers and temporarily call that element value. for(i in 1:length(numbers) creates a vector (1:length(numbers)) which is a vector of whole numbers from 1 to the length of the vector numbers. Each of these whole numbers is then temporarily called i. seq_along(numbers) does pretty much the same things as 1:length(numbers).\nprint(value) means we display the current value on the screen.\nR will do this until it has gone through all elements in numbers.\n\nA while loop keeps going as long as some condition is TRUE. Suppose we want to keep adding numbers from a vector until the total sum exceeds 50:\n\nnumbers &lt;- c(10, 20, 30, 40, 50)\ntotal &lt;- 0 # Start total at 0\ni &lt;- 1 # Start index at 1\n\nwhile(i &lt;= length(numbers) & total &lt;= 50){\n  \n  # Add to total\n  total &lt;- total + numbers[i]\n  \n  # Track which element we are on\n  i &lt;- i + 1\n  \n}\n\nprint(total)\n\n[1] 60\n\nprint(i)\n\n[1] 4\n\n\nWhat’s going on here?\n\nwhile(i &lt;= length(numbers) && total &lt;= 50) - The loop will continue running while two conditions are both TRUE:\n\n\nWe haven’t reached the end of the vector (i &lt;= length(numbers)) and\nThe total hasn’t exceeded 50 (total &lt;= 50).\n\n\nInside the loop, we add the i-th element of numbers to total.\nWe then move i to the next element by adding 1.\nAs soon as one of the conditions in 1. become FALSE, the loop stops."
  },
  {
    "objectID": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-calc_df_stats",
    "href": "posts/Rintro_day1/index.html#pre-requisite-knowledge-for-calc_df_stats",
    "title": "Intro to R: Day 1",
    "section": "Pre-requisite knowledge for calc_df_stats()",
    "text": "Pre-requisite knowledge for calc_df_stats()\n\nNavigating directories and file paths\nWhen you work in R, you’ll often deal with files (like CSV files) that sit in folders (directories) on your computer. To load these files into R so that you can work with and analyze them, you need to tell R where they are. In addition, it is important to know where you are while working in R.\nWhen I say “know where you are”, I am referring to your “working directory”. When you open up Rstudio, there is some folder on your computer that R will call your “working directory”. You can see what this folder is at any time with getwd():\n\n# You get to see what my working directory path is\ngetwd()\n\n[1] \"C:/Users/isaac/Documents/Simon_Lab/isaacvock.github.io/posts/Rintro_day1\"\n\n\nIf you want to change this directory, you can switch to a new directory with setwd(\"/path/to/new/working/directory\").\nYou can specify a file path to setwd() in one of two ways:\n\nAs an absolute path. This means specifying exactly where a folder is on your computer (like “C:/Users/YourName/Documents/” on Windows or “/Users/YourName/Documents/ on Macs).\nAs a relative path. This means telling R how to get from the current working directory to the folder. For example, if you are in the directory “Documents” and want to move to a folder called “data” inside of “Documents, you could run setwd(\"data\"), assuming”Documents” is your current working directory.\n\n\n\nReading a file with readr\nThe readr package (part of the tidyverse collection of packages), provides user-friendly functions for reading in data. For example, you can read a csv file like so:\n\nmy_data &lt;- read_csv(\"path/to/mydata.csv\")\n\n\nread_csv(\"path/to/mydata.csv\") reads the CSV file located at the specified path (either a relative or absolute path) and creates a data frame (more on those soon).\nWe’re storing that data frame in a variable called my_data.\n\n\n\nData Frames\nA data frame is a table-like structure with rows and columns, commonly used for storign datasets in R. Each column is usually a vector of a particular type (numeric, character, boolean, etc.), and all columns have the same length.\nTo create a data frame you can run code like this:\n\nages &lt;- c(30, 25, 35)\n\n# You can either specify the vector directly\n# or provide the name of a vector you previously created\npeople_df &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age = ages,\n  Score = c(100, 95, 90)\n)\n\npeople_df\n\n     Name Age Score\n1   Alice  30   100\n2     Bob  25    95\n3 Charlie  35    90\n\n\nHere are some ways you can interact with the data inside of a data frame:\n\nYou can grab an entire column with $ or [[&lt;col name as string&gt;]]:\n\n\n# This will give you a vector\npeople_df$Name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n# This will be the same vector\npeople_df[[\"Name\"]]\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n\n\nYou can grab an entire column with [ , &lt;column number&gt;]:\n\n\n# This will give you a data frame with one column\npeople_df[, 1]\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\n# This will give you a data frame with multiple columns\npeople_df[,c(1, 2)]\n\n     Name Age\n1   Alice  30\n2     Bob  25\n3 Charlie  35\n\n\n\nYou can get all columns in a specific row with [&lt;row number&gt;, ]:\n\n\n# This will give you a data frame with one row\npeople_df[1,]\n\n   Name Age Score\n1 Alice  30   100\n\n# This will give you a data frame with multiple rows\npeople_df[c(1, 3), ]\n\n     Name Age Score\n1   Alice  30   100\n3 Charlie  35    90\n\n\n\n\nLists\nA list is like a container in R that can hold a mix of different types of items, like a data frame. Lists are more flexible though, and can hold things of different sizes. A list can hold:\n\nA numeric vector\nA string vector\nA single number\nAn entire data frame\nAnother list\n\nAll at once!\nHere is how you can create a list:\n\nmy_list &lt;- list(\n  name = \"Alice\",\n  age = 30,\n  scores = c(100, 95, 90),\n  is_student = FALSE,\n  df = data.frame(a = c(1, 2, 3), b = c(\"a\", \"b\", \"c\"))\n)\n\nTo access elements of a list, you can:\n\nUse the $ operator (if the elements have names):\n\n\nmy_list$name\n\n[1] \"Alice\"\n\nmy_list$scores\n\n[1] 100  95  90\n\n\n\nThe [[ ]] operator with the element’s name (if it has one), or its position:\n\n\nmy_list[[\"name\"]]\n\n[1] \"Alice\"\n\nmy_list[[1]]\n\n[1] \"Alice\"\n\n\n\n\n\n\n\n\nFancy looping over lists (and data frames)\n\n\n\n\n\nOften, you will want to go element by element of a list and do something with each element. In addition, data frame columns are equivalent to elements of a list (actually, under the hood, a data frame is just a list that forces the list elements to be the same size). You could write a for loop, but there are popular alternatives that can make your code cleaner and easier to read. R has a version of these, but the R package purrr has improved versions of these that I prefer.\n\nmap(): takes a single list as input\n\n\nlibrary(purrr)\n\nnumbers &lt;- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(10, 20, 30, 40, 50)\n)\n\n# Outputs a list, one element for original element of the list\nmap(numbers, function(x) sum(x))\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 150\n\n# Outputs a vector numbers, one element per original list element\n# Also using an alternative notation\nmap_dbl(numbers, ~ sum(.x))\n\n[1]   6  15 150\n\n\n\nmap2(): takes two lists as input\n\n\nnumbers &lt;- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(10, 20, 30, 40, 50)\n)\n\nnumbers2 &lt;- list(\n  c(-1, -2, -3),\n  c(12, 13),\n  c(2, 4, 6, 8, 10, 12)\n)\n\n# Outputs a list, one element for original element of the list\nmap2(numbers, numbers2, ~ sum(.x) + sum(.y))\n\n[[1]]\n[1] 0\n\n[[2]]\n[1] 40\n\n[[3]]\n[1] 192\n\n# Outputs a vector numbers, one element per original list element\nmap2_dbl(numbers, numbers2, function(x, y) sum(x) + sum(y))\n\n[1]   0  40 192\n\n\n\npmap() allows you to provide a named list of inputs:\n\n\n# A list of vectors\nlists_of_inputs &lt;- list(\n  a = c(1, 3, 5),\n  b = c(2, 4, 6),\n  c = c(10, 20, 30)\n)\n\npmap(lists_of_inputs, function(a, b, c) a + b + c)\n\n[[1]]\n[1] 13\n\n[[2]]\n[1] 27\n\n[[3]]\n[1] 41\n\npmap_dbl(lists_of_inputs, function(a, b, c) a + b + c)\n\n[1] 13 27 41"
  }
]