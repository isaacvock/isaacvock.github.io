---
title: "Analyzing NR-seq data: the basics"
author: "Isaac Vock"
date: "2024-12-22"
categories: [nrseq]
format: 
  live-html:
    toc: true
webr:
  packages:
    - dplyr
    - ggplot2
    - gridExtra
    - purrr
engine: knitr
editor: 
  markdown: 
    wrap: 72
image: "tcbmm.png"
---

{{< include ../../_extensions/r-wasm/live/_knitr.qmd >}}

In my last post, I introduced NR-seq by walking through the development of an NR-seq simulator. That post implicitly introduced some of the complexities of interpreting NR-seq data. In this post, we will tackle these challenges head-on and build up a rigorous strategy by which to analyze NR-seq data. We will do this in a piece-meal fashion, first developing a simple but flawed strategy, until eventually working up to mixture modeling (the current gold-standard for NR-seq analyses). No statistical model is perfect though, so we will finish with a discussion and exploration of the limitations of this gold-standard.

## NR-seq: a reminder

In an NR-seq experiment, there are two populations of RNA: those synthesized in the presence of label (a.k.a. labeled, or new, RNA) and those which were synthesized prior to metabolic labeling (a.k.a unlabeled, or old, RNA). The first task of any NR-seq analysis is for a given species of RNA (e.g., RNA transcribed from a particular gene), quantify the relative amounts of these two populations. This is referred to as that species' "fraction new" or "new-to-total ratio (NTR)". Downstream analyses are then aimed at interpreting these fraction news/NTRs. This post will only concern itself with fraction new estimation. I will use the term "fraction new" for the remainder of this post.

To estimate the fraction new, we rely on the mutational content of mapped sequencing reads. NR-seq involves chemically recoding metabolic label (e.g., s4U) so that reverse transcriptase reads it as a different nucleotide (e.g., a cytosine). Thus, reads from new RNA will have, on average, more mutations than reads from old RNA. This observation is the key to analyzing NR-seq data.

To test the strategies discussed, we will use simulated data. This allows us to know the ground truth and explore the robustness of any approach. Here is the function that we will use to simulate data, as well as some helper functions we can use to assess analysis strategies:

```{webr}
#| edit: false

#' Simulate NR-seq data
#' 
#'@param nt Number of transcripts.
#'@param seqdepth Total number of reads.
#'@param readlen Read length.
#'@param tl Length of labeling.
#'@param pnew Metabolic label incorporation and conversion rate.
#'@param pold Background mutation rate.
#'@param kdeg_logmean Average transcript log(kdeg).
#'@param kdeg_logsd Standard deviation of transcript log(kdeg)'s.
#'@param ksyn_logmean Average transcript log(ksyn).
#'@param ksyn_logsd Standard deviation of transcript log(kdeg)'s.
#'@param Ucont_alpha Beta distribution parameter for transcript U-contents.
#'@param Ucont_beta Other beta distribution parameter for transcript U-contents.
simulate_nrseq <- function(nt = 200, 
                           seqdepth = 1000*nt,
                           readlen = 150,
                           tl = 4,
                           pnew = 0.05,
                           pold = 0.002,
                           kdeg_logmean = -1.9,
                           kdeg_logsd = 0.7,
                           ksyn_logmean = 2.3,
                           ksyn_logsd = 0.7,
                           Ucont_alpha = 25,
                           Ucont_beta = 75){
  
  ### Simulate transcript parameters
  kdegs <- rlnorm(nt, kdeg_logmean, kdeg_logsd)
  ksyns <- rlnorm(nt, ksyn_logmean, ksyn_logsd)
  Rss <- ksyns / kdegs
  rel_abundance <- Rss / sum(Rss)
  fns <- 1 - exp(-kdegs*tl)
  
  Uconts <- rbeta(nt, Ucont_alpha, Ucont_beta)
  
  ### Simulate read counts for each transcript
  reads_per_t <- rmultinom(1, 
                           size = seqdepth,
                           prob = rel_abundance)[,1]
  
  ### Simulate read-specific data
  
  newness <- rbinom(seqdepth,
                    size = 1,
                    prob = rep(fns, times = reads_per_t))
  
  nT <- rbinom(seqdepth,
               size = readlen,
               prob = rep(Uconts, times = reads_per_t))
  
  TC <- rbinom(seqdepth,
               size = nT,
               prob = pnew * newness + pold)
  
  ### Compile data
  
  sim_df <- tibble(
    transcript = paste0("transcript", rep(1:nt, times = reads_per_t)),
    TC = TC,
    nT = nT,
    newness = newness
  )
  
  par_df <- tibble(
    transcript = paste0("transcript", 1:nt),
    kdeg = kdegs,
    ksyn = ksyns,
    fn = fns,
    reads = reads_per_t
  )
  
  return(
    list(
      cB = sim_df,
      truth = par_df
    )
  )
  
}


### Calculate logit and inverse logit
logit <- function(x) log(x / (1-x))
inv_logit <- function(x) exp(x) / (1 + exp(x))

```

## A simple approach: mutational cutoffs

If reads from new RNA have more mutations on average than those from old RNA, maybe we can just use a simple mutational cutoff to classify individual reads as from old or new RNA. The fraction of reads that come from the latter is then our estimate for the fraction new. This approach has been popular since the advent of NR-seq, and is implemented in popular bioinformatic tools for analyzing NR-seq data like SLAMDUNK. Let's simulate some data and test out this approach


```{webr}
### Simulate data
simdata <- simulate_nrseq(nt = 1000)


### Analyze data
estimates <- simdata$cB %>%
  dplyr::count(transcript, TC, nT) %>%
  dplyr::group_by(transcript) %>%
  dplyr::summarise(
    new_1plus = sum(n[TC > 0]),
    new_2plus = sum(n[TC > 1]),
    reads = sum(n)
  ) %>%
  dplyr::mutate(
    fraction_new_1plus = new_1plus / reads,
    fraction_new_2plus = new_2plus / reads
  )


### Assess analysis accuracy
p1 <- estimates %>%
  dplyr::inner_join(simdata$truth,
                    by = "transcript") %>%
  ggplot(aes(x = fn,
             y = fraction_new_1plus)) +
  geom_point(alpha = 0.5) + 
  theme_classic() + 
  geom_abline(slope = 1,
              intercept = 0,
              color = 'darkred',
              linewidth = 1,
              linetype = 'dotted') + 
  xlab("True fn") + 
  ylab("1+ mutation fn est.")

p2 <- estimates %>%
  dplyr::inner_join(simdata$truth,
                    by = "transcript") %>%
  ggplot(aes(x = fn,
             y = fraction_new_2plus)) +
  geom_point(alpha = 0.5) + 
  theme_classic() + 
  geom_abline(slope = 1,
              intercept = 0,
              color = 'darkred',
              linewidth = 1,
              linetype = 'dotted') + 
  xlab("True fn") + 
  ylab("2+ mutation fn est.")

grid.arrange(p1, p2,
             nrow = 1,
             ncol = 2)
```

If you run this code with the default simulation parameters, you'll see that the estimates are decent. The 1+ mutation cutoff for newness looks better than the 2+ cutoff, with the former yielding estimates that consistently correlate pretty well with the simulated ground truth. 

So that's all it takes to analyze NR-seq data? Not so fast. In our simulation, there is a default metabolic label incorporation + conversion rate of 5%. While this is a standard "good" incorporation rate, if you analyze as many NR-seq datasets as I have you will quickly notice that there is a lot of dataset-to-dataset variation in the incorporation rate. For example, there is a tremendous amount of cell line-to-cell line variation in the readiness of s4U incorporation, with some cell lines (e.g., HEK293 and HeLa) uptaking s4U with great tenacity and others (e.g., neuronal cell lines) having typically much lower s4U incorporation rates. In addition, incorporation rates also can correlate with biological condition. For example, knocking out key factors in RNA metabolism (e.g., degradation factors) can significantly impact incorporation rates. In general, incorporation rates seem to correlate strongly with general metabolic rates, and anything that perturbs these rates will likely affect incorporation rates.

This lattermost observation is particularly dangerous when it comes to applying the simple mutation content cutoff analysis strategy. Often, we don't just care about what an RNA's dynamics look like in one biological condition, but rather how it differs between two more different conditions (e.g., WT vs. KO of your favorite gene, untreated vs. drug treated, etc.). If an analysis method is not robust to variation in incorporation rates, it risks making technical variability look like biological signal.

Thus, what happens if we simulate a different incorporation rate? If you tweak the simulation above (set `pnew` in `simulate_nrseq()` to a different value than its default of 0.05 and rerun code):

![Accuracy of cutoff approach for range of pnews](Cutoff_vs_pnew.png)

The key takeaway from this investigation is that the accuracy of the cutoff-based approach is heavily reliant on the incorporation rate. Since incorporation rate often correlates with biology, this represents a dangerous confounder for mutation cutoff analyses. **We need a more robust analysis strategy.**

## A better idea: statistical modeling

The problem with the cutoff based approach is two-fold:

1. It's possible for reads from labeled RNA to have no mutations. This is because the metabolic label has to compete with the regular nucleotide for incorporation, which is what keeps incorporation rates relatively low in most NR-seq experiments.
1. It's possible for reads from unlabeled RNA to have mutations. This can be due to RT errors, sequencing errors, alignment errors, unmasked SNPs, etc.

Thus, a mutation in a read does not make it definitively from new RNA, and a lack of mutations does not make it definitively from old RNA. How can we navigate this inherent uncertainty? This is exactly what statistical modeling was built for.

Statistical modeling first means coming up with a model that specifies how likely every possible data point is. If you tell me the number of mutable nucleotides, the number of mutations in a read, whether it came from old or new RNA, and whatever can be specified about the process by which mutations arise in reads, I should be able to use this model to calculate a likelihood for that piece of data. 

::: {.callout-tip collapse="true"}
## What is a data point's "likelihood"?

The likelihood of a data point is the probability of seeing that data, given all of the information you provided, often written as P(data | parameters). In this case, we are dealing with discrete data (integer mutation counts), meaning that this likelihood can also be interpreted as the probability of getting that data point given all of the specified parameters. In a continuous setting, interpreting this is a bit more complicated, as the [probability of any specific continuous](https://www.youtube.com/watch?v=ZA4JkHKZM50) outcome is 0.

:::

In practice, this often involves specifying a convenient to work with probability distribution that describes the variability in your data. To do this, you need to make some assumptions about your data. For NR-seq data, it is common to assume:

1. For reads from new RNA, there is a set probability (call it pnew) that a given mutable nucleotide (e.g., uridines in an s4U labeling NR-seq experiment) is mutated. This pnew is the same for all such reads, regardless of the RNA species of origin.
1. For reads from old RNA, there is also a set probability of mutation (call it pold) for all such reads.
1. All nucleotides are independent. Whether or not a given nucleotide is mutated has no impact on the probability that other nucleotides in that read are also mutated (given the status of the read as coming from old or new RNA).

These are actually the exact assumptions that we used to simulate data above and in the introduction to NR-seq blog. These assumptions lend themselves to a particular model: a two-component binomial mixture model.

### Two-component binomial mixture model

"Two-component binomial mixture model" is a mouthful, so let's break it down.

"Two-component" = the model supposes that there are two populations in your data. In our case, this is reads from old RNA and reads from new RNA.

"binomial" = data from each of the populations is modeled as following a binomial distribution. We've seen this distribution in the intro to NR-seq post. It describes a situation where you have a certain number of independent "trials" (e.g., mutable nucleotides), with a set probability of "success" (e.g., mutation of the nucleotide) for each trial.

"mixture model" = you don't know which population any given data point comes from. This is known as a "latent-variable model", which can pose some computational challenges when trying to estimate the parameters of such a model. These challenges will turn out to be fairly easy to navigate in this setting, but will limit our efforts to extend and improve this model in future sections.

To summarize, we are assuming that each sequencing read comes from one of two populations: old RNA or new RNA. The mutational content of both types of reads is well modeled as following a binomial distribution. The parameters of these binomial distributions are the number of mutable nucleotides and the probability that each of these nucleotides gets mutated. We don't need to estimate the number of mutable nucleotides (this is just more data), but we do not know a priori the two mutation rates. Thus, we need to estimate these two parameters, as well as the quantity of primary interest: the fraction new. We can schematize this model as such:

![Two-component binomial mixture model](tcbmm.png){width=50%}


## Fitting a two-component binomial mixture model

A TCBMM has three parameters that need to be estimated for each RNA feature: 

1. The fraction new
1. The probability of a mutation in reads from new RNA ($p_{\text{high,TC}}$ in the TCBMM figure above)
1. The probability of a mutation in reads from old RNA ($p_{\text{low,TC}}$ in the TCBMM figure above)


::: {.callout-tip collapse="true"}
## What is an "RNA feature"?

Our goal is to estimate the fraction of RNA molecules that are new/labeled for a given species of RNA. Our definition of "species" is technically flexible, and is what I refer to as an "RNA feature". The most common choice for a feature is a gene. That is, we estimate the fraction of RNA molecules produced from a given gene that are new. In practice though, there are a lot more features we may be interested in analyzing. See the [EZbakR preprint](https://www.biorxiv.org/content/10.1101/2024.10.14.617411v1) for a description of some other options.

:::

In this post, we will estimate these via the method of maximum likelihood. That means we will find parameter estimates that maximum the likelihood of our data. In theory, this is simple: just write a function to calculate the likelihood for any combination of parameter values and data, and use the optimization algorithm of your choice. Here's what that might look like for the TCBMM:


```{webr}

### Define the data likelihood
tcbmm_L <- function(params, muts, nucs, n){
  
  ### Convert to natural scale
  phigh <- inv_logit(params[1])
  plow <- inv_logit(params[2])
  fxn_new <- inv_logit(params[3])
  
  ### Calculate likelihood
  L <- n*log(
    fxn_new * dbinom(muts, nucs, prob = phigh) + 
      (1 - fxn_new) * dbinom(muts, nucs, prob = plow)
  )
  
  ### optim() minimizes by default
  ### minimizing negative likelihood = maximizing likelihood
  return(-sum(L))
  
}

### Simulate data
simdata <- simulate_nrseq(nt = 1000, pnew = 0.02)


### Analyze data
estimates <- simdata$cB %>%
  dplyr::count(transcript, TC, nT) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(transcript) %>%
  
  ## Fit model
  dplyr::summarise(
    fit = list(I(optim(
      c(logit(0.05),
        logit(0.002),
        logit(0.5)),
      fn = tcbmm_L,
      muts = TC,
      nucs = nT,
      n = n,
      method = "L-BFGS-B",
      lower = c(-9, -9, -9),
      upper = c(0, 0, 9)
    )))
  ) %>%
  
  ## Extract parameter estimates
  dplyr::mutate(
    p1 = inv_logit(purrr::map_dbl(fit, ~ .x$par[1])),
    p2 = inv_logit(purrr::map_dbl(fit, ~ .x$par[2])),
    fxn = inv_logit(purrr::map_dbl(fit, ~ .x$par[3]))
  ) %>%
  dplyr::ungroup() %>%
  dplyr::rowwise() %>%
  
  ## Deal with potential label flipping
  dplyr::mutate(
    phigh = pmax(p1, p2),
    plow = pmin(p1, p2),
    fxn_new = ifelse(p1 > p2, fxn, 1 - fxn)
  )

### Assess accuracy
ptcbmm_rcol <- estimates %>%
  dplyr::inner_join(simdata$truth,
                    by = "transcript") %>%
  ggplot(aes(x = fn,
             y = fxn_new,
             color = log10(reads))) +
  geom_point(alpha = 0.75) + 
  scale_color_viridis_c() + 
  theme_classic() + 
  geom_abline(slope = 1,
              intercept = 0,
              color = 'darkred',
              linewidth = 1,
              linetype = 'dotted') + 
  xlab("True fn") + 
  ylab("TCBMM fn est.")

ptcbmm_rcol

```

Already, this quickly whipped up strategy is working pretty well. For one, we have largely solved the problem of phigh/plow dependence on estimate accuracy. Play around with different values of plow/phigh in the simulation and prove this for yourself, but that is the main advantage of the TCBMM approach. You can also see that the more data we have, the better our estimates get (on average). This is a nice trend, and means that paying for more sequencing depth can have a significant positive impact on the quality of our estimates. Technically, a similar trend holds for the mutation content cutoff strategy, but because we can't be sure if our estimates are biased or not, more reads could just yield higher confidence wrong estimates.

One thing you should note though is that there are some pretty bad outliers.