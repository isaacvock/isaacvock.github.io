{
  "hash": "b2f05ca94120aa18ad6a92c905d2a785",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Analyzing NR-seq data: the basics\"\nauthor: \"Isaac Vock\"\ndate: \"2024-12-22\"\ncategories: [nrseq]\nformat: \n  live-html:\n    toc: true\nwebr:\n  packages:\n    - dplyr\n    - ggplot2\n    - gridExtra\nengine: knitr\neditor: \n  markdown: \n    wrap: 72\nimage: \"tcbmm.png\"\n---\n::: {.cell}\n\n:::\n\n\n\nIn my last post, I introduced NR-seq by walking through the development of an NR-seq simulator. That post implicitly introduced some of the complexities of interpreting NR-seq data. In this post, we will tackle these challenges head-on and build up a rigorous strategy by which to analyze NR-seq data. We will do this in a piece-meal fashion, first developing a simple but flawed strategy, until eventually working up to mixture modeling (the current gold-standard for NR-seq analyses). No statistical model is perfect though, so we will finish with a discussion and exploration of the limitations of this gold-standard.\n\n## NR-seq: a reminder\n\nIn an NR-seq experiment, there are two populations of RNA: those synthesized in the presence of label (a.k.a. labeled, or new, RNA) and those which were synthesized prior to metabolic labeling (a.k.a unlabeled, or old, RNA). The first task of any NR-seq analysis is for a given species of RNA (e.g., RNA transcribed from a particular gene), quantify the relative amounts of these two populations. This is referred to as that species' \"fraction new\" or \"new-to-total ratio (NTR)\". Downstream analyses are then aimed at interpreting these fraction news/NTRs. This post will only concern itself with fraction new estimation. I will use the term \"fraction new\" for the remainder of this post.\n\nTo estimate the fraction new, we rely on the mutational content of mapped sequencing reads. NR-seq involves chemically recoding metabolic label (e.g., s4U) so that reverse transcriptase reads it as a different nucleotide (e.g., a cytosine). Thus, reads from new RNA will have, on average, more mutations than reads from old RNA. This observation is the key to analyzing NR-seq data.\n\nTo test the strategies discussed, we will use simulated data. This allows us to know the ground truth and explore the robustness of any approach. Here is the function that we will use to simulate data, as well as some helper functions we can use to assess analysis strategies:\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n\n#' Simulate NR-seq data\n#' \n#'@param nt Number of transcripts.\n#'@param seqdepth Total number of reads.\n#'@param readlen Read length.\n#'@param tl Length of labeling.\n#'@param pnew Metabolic label incorporation and conversion rate.\n#'@param pold Background mutation rate.\n#'@param kdeg_logmean Average transcript log(kdeg).\n#'@param kdeg_logsd Standard deviation of transcript log(kdeg)'s.\n#'@param ksyn_logmean Average transcript log(ksyn).\n#'@param ksyn_logsd Standard deviation of transcript log(kdeg)'s.\n#'@param Ucont_alpha Beta distribution parameter for transcript U-contents.\n#'@param Ucont_beta Other beta distribution parameter for transcript U-contents.\nsimulate_nrseq <- function(nt = 200, \n                           seqdepth = 50000,\n                           readlen = 150,\n                           tl = 4,\n                           pnew = 0.05,\n                           pold = 0.002,\n                           kdeg_logmean = -1.9,\n                           kdeg_logsd = 0.7,\n                           ksyn_logmean = 2.3,\n                           ksyn_logsd = 0.7,\n                           Ucont_alpha = 25,\n                           Ucont_beta = 75){\n  \n  ### Simulate transcript parameters\n  kdegs <- rlnorm(nt, kdeg_logmean, kdeg_logsd)\n  ksyns <- rlnorm(nt, ksyn_logmean, ksyn_logsd)\n  Rss <- ksyns / kdegs\n  rel_abundance <- Rss / sum(Rss)\n  fns <- 1 - exp(-kdegs*tl)\n  \n  Uconts <- rbeta(nt, Ucont_alpha, Ucont_beta)\n  \n  ### Simulate read counts for each transcript\n  reads_per_t <- rmultinom(1, \n                           size = seqdepth,\n                           prob = rel_abundance)[,1]\n  \n  ### Simulate read-specific data\n  \n  newness <- rbinom(seqdepth,\n                    size = 1,\n                    prob = rep(fns, times = reads_per_t))\n  \n  nT <- rbinom(seqdepth,\n               size = readlen,\n               prob = rep(Uconts, times = reads_per_t))\n  \n  TC <- rbinom(seqdepth,\n               size = nT,\n               prob = pnew * newness + pold)\n  \n  ### Compile data\n  \n  sim_df <- tibble(\n    transcript = paste0(\"transcript\", rep(1:nt, times = reads_per_t)),\n    TC = TC,\n    nT = nT,\n    newness = newness\n  )\n  \n  par_df <- tibble(\n    transcript = paste0(\"transcript\", 1:nt),\n    kdeg = kdegs,\n    ksyn = ksyns,\n    fn = fns,\n    reads = reads_per_t\n  )\n  \n  return(\n    list(\n      cB = sim_df,\n      truth = par_df\n    )\n  )\n  \n}\n\n\n#' \n```\n:::\n\n\n## A simple approach: mutational cutoffs\n\nIf reads from new RNA have more mutations on average than those from old RNA, maybe we can just use a simple mutational cutoff to classify individual reads as from old or new RNA. The fraction of reads that come from the latter is then our estimate for the fraction new. This approach has been popular since the advent of NR-seq, and is implemented in popular bioinformatic tools for analyzing NR-seq data like SLAMDUNK. Let's simulate some data and test out this approach\n\n\n\n::: {.cell}\n```{webr}\n### Simulate data\nsimdata <- simulate_nrseq()\n\n\n### Analyze data\nestimates <- simdata$cB %>%\n  dplyr::count(transcript, TC, nT) %>%\n  dplyr::group_by(transcript) %>%\n  dplyr::summarise(\n    new_1plus = sum(n[TC > 0]),\n    new_2plus = sum(n[TC > 1]),\n    reads = sum(n)\n  ) %>%\n  dplyr::mutate(\n    fraction_new_1plus = new_1plus / reads,\n    fraction_new_2plus = new_2plus / reads\n  )\n\n\n### Assess analysis accuracy\np1 <- estimates %>%\n  dplyr::inner_join(simdata$truth,\n                    by = \"transcript\") %>%\n  ggplot(aes(x = fn,\n             y = fraction_new_1plus)) +\n  geom_point(alpha = 0.5) + \n  theme_classic() + \n  geom_abline(slope = 1,\n              intercept = 0,\n              color = 'darkred',\n              linewidth = 1,\n              linetype = 'dotted') + \n  xlab(\"True fn\") + \n  ylab(\"1+ mutation fn est.\")\n\np2 <- estimates %>%\n  dplyr::inner_join(simdata$truth,\n                    by = \"transcript\") %>%\n  ggplot(aes(x = fn,\n             y = fraction_new_2plus)) +\n  geom_point(alpha = 0.5) + \n  theme_classic() + \n  geom_abline(slope = 1,\n              intercept = 0,\n              color = 'darkred',\n              linewidth = 1,\n              linetype = 'dotted') + \n  xlab(\"True fn\") + \n  ylab(\"2+ mutation fn est.\")\n\ngrid.arrange(p1, p2,\n             nrow = 1,\n             ncol = 2)\n```\n:::\n\n\nIf you run this code with the default simulation parameters, you'll see that the estimates are decent. The 1+ mutation cutoff for newness looks better than the 2+ cutoff, with the former yielding estimates that consistently correlate pretty well with the simulated ground truth. \n\nSo that's all it takes to analyze NR-seq data? Not so fast. In our simulation, there is a default metabolic label incorporation + conversion rate of 5%. While this is a standard \"good\" incorporation rate, if you analyze as many NR-seq datasets as I have you will quickly notice that there is a lot of dataset-to-dataset variation in the incorporation rate. For example, there is a tremendous amount of cell line-to-cell line variation in the readiness of s4U incorporation, with some cell lines (e.g., HEK293 and HeLa) uptaking s4U with great tenacity and others (e.g., neuronal cell lines) having typically much lower s4U incorporation rates. In addition, incorporation rates also can correlate with biological condition. For example, knocking out key factors in RNA metabolism (e.g., degradation factors) can significantly impact incorporation rates. In general, incorporation rates seem to correlate strongly with general metabolic rates, and anything that perturbs these rates will likely affect incorporation rates.\n\nThis lattermost observation is particularly dangerous when it comes to applying the simple mutation content cutoff analysis strategy. Often, we don't just care about what an RNA's dynamics look like in one biological condition, but rather how it differs between two more different conditions (e.g., WT vs. KO of your favorite gene, untreated vs. drug treated, etc.). If an analysis method is not robust to variation in incorporation rates, it risks making technical variability look like biological signal.\n\nThus, what happens if we simulate a different incorporation rate? If you tweak the simulation above (set `pnew` in `simulate_nrseq()` to a different value than its default of 0.05 and rerun code):\n\n![Accuracy of cutoff approach for range of pnews](Cutoff_vs_pnew.png)\n\nThe key takeaway from this investigation is that the accuracy of the cutoff-based approach is heavily reliant on the incorporation rate. Since incorporation rate often correlates with biology, this represents a dangerous confounder for mutation cutoff analyses. **We need a more robust analysis strategy.**\n\n## A better idea: statistical modeling\n\nThe problem with the cutoff based approach is two-fold:\n\n1. It's possible for reads from labeled RNA to have no mutations. This is because the metabolic label has to compete with the regular nucleotide for incorporation, which is what keeps incorporation rates relatively low in most NR-seq experiments.\n1. It's possible for reads from unlabeled RNA to have mutations. This can be due to RT errors, sequencing errors, alignment errors, unmasked SNPs, etc.\n\nThus, a mutation in a read does not make it definitively from new RNA, and a lack of mutations does not make it definitively from old RNA. How can we navigate this inherent uncertainty? This is exactly what statistical modeling was built for.\n\nStatistical modeling first means coming up with a model that specifies how likely every possible data point is. If you tell me the number of mutable nucleotides, the number of mutations in a read, whether it came from old or new RNA, and whatever can be specified about the process by which mutations arise in reads, I should be able to use this model to calculate a likelihood for that piece of data. \n\n::: {.callout-tip collapse=\"true\"}\n## What is a data point's \"likelihood\"?\n\nThe likelihood of a data point is the probability of seeing that data, given all of the information you provided, often written as P(data | parameters). In this case, we are dealing with discrete data (integer mutation counts), meaning that this likelihood can also be interpreted as the probability of getting that data point given all of the specified parameters. In a continuous setting, interpreting this is a bit more complicated, as the [probability of any specific continuous](https://www.youtube.com/watch?v=ZA4JkHKZM50) outcome is 0.\n\n:::\n\nIn practice, this often involves specifying a convenient to work with probability distribution that describes the variability in your data. To do this, you need to make some assumptions about your data. For NR-seq data, it is common to assume:\n\n1. For reads from new RNA, there is a set probability (call it pnew) that a given mutable nucleotide (e.g., uridines in an s4U labeling NR-seq experiment) is mutated. This pnew is the same for all such reads, regardless of the RNA species of origin.\n1. For reads from old RNA, there is also a set probability of mutation (call it pold) for all such reads.\n1. All nucleotides are independent. Whether or not a given nucleotide is mutated has no impact on the probability that other nucleotides in that read are also mutated (given the status of the read as coming from old or new RNA).\n\nThese are actually the exact assumptions that we used to simulate data above and in the introduction to NR-seq blog. These assumptions lend themselves to a particular model: a two-component binomial mixture model.\n\n### Two-component binomial mixture model\n\n\"Two-component binomial mixture model\" is a mouthful, so let's break it down.\n\n\"Two-component\" = the model supposes that there are two populations in your data. In our case, this is reads from old RNA and reads from new RNA.\n\n\"binomial\" = data from each of the populations is modeled as following a binomial distribution. We've seen this distribution in the intro to NR-seq post. It describes a situation where you have a certain number of independent \"trials\" (e.g., mutable nucleotides), with a set probability of \"success\" (e.g., mutation of the nucleotide) for each trial.\n\n\"mixture model\" = you don't know which population any given data point comes from. This is known as a \"latent-variable model\", which can pose some computational challenges when trying to estimate the parameters of such a model. These challenges will turn out to be fairly easy to navigate in this setting, but will limit our efforts to extend and improve this model in future sections.\n\nTo summarize, we are assuming that each sequencing read comes from one of two populations: old RNA or new RNA. The mutational content of both types of reads is well modeled as following a binomial distribution. The parameters of these binomial distributions are the number of mutable nucleotides and the probability that each of these nucleotides gets mutated. We don't need to estimate the number of mutable nucleotides (this is just more data), but we do not know a priori the two mutation rates. Thus, we need to estimate these two parameters, as well as the quantity of primary interest: the fraction new. We can schematize this model as such:\n\n![Two-component binomial mixture model](tcbmm.png)\n\n\n## Fitting a two-component binomial mixture model\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}