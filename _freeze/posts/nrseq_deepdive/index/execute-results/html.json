{
  "hash": "3a45e71261bfe0c3765cfc5bd83a06d1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"NR-seq: an in-depth review\"\nauthor: \"Isaac Vock\"\ndate: \"2025-11-18\"\ncategories: [nrseq]\nformat: \n  html:\n    toc: true\nengine: knitr\neditor: \n  markdown: \n    wrap: 72\nimage: \"NRseq.png\"\nbibliography: references.bib\n---\n\n## NR-seq: a review\n\nThis is a reproduction of Chapter 1 of [my\nthesis](https://github.com/isaacvock/Thesis), an in-depth technical\nreview of NR-seq data. It is intended to be a living document that\nevolves as the field evolves, and will thus be occasionally updated\n(edit time scale: \\~ monthly).\n\n## Abstract\n\nThe kinetics of gene expression are largely invisible to methods that\nonly probe steady-state RNA abundances. Nucleotide recoding RNA-seq\nmethods (TimeLapse-seq, SLAM-seq, TUC-seq, etc.) were developed to\novercome this limitation. These methods combine metabolic labeling with\nunique chemistries to track the dynamics of labeled and unlabeled RNA\nand resolve the kinetic ambiguities of vanilla RNA-seq. Despite their\npromise, analyzing NR-seq data presents several unique bioinformatic\nchallenges. While software packages exist that implement gold-standard\nanalysis strategies, misconceptions about how to properly analyze and\ninterpret NR-seq data persist. In some cases, this has led to the\nwidespread adoption of potentially flawed analysis paradigms. To address\nthis, I present a detailed overview of NR-seq analyses. I cover best\npractices, current software implementations, and optimal experimental\ndesign. I also discuss the landscape of NR-seq extensions, as these\nrepresent exciting areas with unique bioinformatic challenges. I hope\nthat this will be a useful resource to the community of NR-seq users and\ndevelopers.\n\n## Introduction\n\nFrom birth to death, an RNA's life cycle is tightly regulated. A key\naspect of this regulation is fine-tuning of the rate at which each stage\nprogresses, from transcription initiation to RNA degradation. Developing\na mechanistic understanding of gene expression regulation requires\nmethods to probe the kinetics of the RNA life cycle (transcription,\nprocessing, export, degradation, etc.) [@RN96].\n\nWhile standard RNA-seq begins to solve this problem, it provides limited\ninformation about the kinetics of the processes which determine an RNA's\nabundance. Nucleotide recoding RNA-seq (NR-seq; TimeLapse-seq [@RN88],\nSLAM-seq [@RN89], TUC-seq [@RN192], etc.) overcomes these limitations.\nNR-seq relies on metabolic labeling, which involves feeding cells a\nnucleotide analog that gets incorporated into RNA synthesized after the\nstart of labeling. The rate at which old, unlabeled RNA degrades and\nnew, labeled RNA accumulates provides information about the kinetics of\nRNA metabolism (@fig-1). To determine how much of a given population of\nRNA is labeled, NR-seq relies on novel chemistries that recode the\nhydrogen bond pattern of the metabolic label to facilitate detection of\nlabeled RNA via chemically induced mutations in sequencing reads\n(@fig-1). NR-seq is thus a powerful method for resolving the kinetic\nambiguities of standard RNA-seq.\n\n![**Overview of a standard NR-seq analysis**, adapted from (Vock and\nSimon 2023). NR-seq typically involves labeling cells with s^4^U and\nchemically recoding s^4^U to a cytosine. The data provided by this\nmethod is counts of sequencing reads containing a certain number of\nmutations and a certain number of mutable nucleotides. This data can be\nused to infer the abundance of labeled and unlabeled RNA, which can be\nmodeled as a function of kinetic parameters (e.g., synthesis and\ndegradation rate constants). An important quantity in this task is the\nfraction of reads from new RNA, or new-to-total ratio (NTR; denoted θ\nthroughout this chapter) whose estimation is a key part of any NR-seq\nanalysis.](NRseq.png){#fig-1}\n\nUnlocking the full potential of NR-seq data requires rigorous and\nwell-founded analysis strategies. While such strategies have been put\nforth and implemented in a number of bioinformatic tools [@RN155;\n@RN158; @RN180; @RN181], misunderstanding regarding how to best\ninterpret and analyze NR-seq data are common. Therefore, it is crucial\nto create gold-standard analysis guidelines for users and developers of\nNR-seq methods.\n\nTowards that end, I present a comprehensive overview of the analysis of\nNR-seq data. I will provide a combination of accessible big-picture\nsummaries of my main points, as well as rigorous mathematical formalism\nto back up key assertions. I also will conclude with a brief overview of\nexisting extensions of the original NR-seq methodology, their\napplications, and unique aspects of their analyses. Throughout, I will\npoint readers to bioinformatic tools implementing gold-standard analysis\nstrategies while also highlighting fundamental challenges posed by\nNR-seq analyses. I hope that this serves as a useful resource for the\nlarger community of NR-seq users and promotes best analysis practices in\nthis exciting and growing field.\n\n## Summary\n\nBelow I will briefly summarize the main takeaways that are expanded upon\nthroughout the remaining of this post. There are four unique challenges\nwhen analyzing NR-seq data: 1) estimating how many reads come from\nlabeled RNA; 2) inferring kinetic parameters from 1); 3) aligning and\nprocessing NR-seq reads; 4) optimizing the experimental design for\nachieving 1) and 2). I thus divide my advice into that pertaining to\neach of these points:\n\n**Inferring the number of reads that come from labeled RNA:**\n\n-   Mixture modeling is the most robust and accurate strategy by which\n    to assess the fraction of reads from a given mutational population.\n    This strategy is implemented in tools such as\n    [GRAND-SLAM](https://github.com/erhard-lab/gedi),\n    [bakR](https://github.com/simonlabcode/bakR),\n    [Halfpipe](https://github.com/IMSBCompBio/Halfpipe), and\n    [EZbakR](https://github.com/isaacvock/EZbakR).\n-   Mutation content cutoffs (e.g., classifying reads as \"labeled\" if\n    they have a certain number of T-to-C mutations) can yield highly\n    biased estimates of labeled RNA abundance.\n-   Mixture models necessarily make assumptions about the distribution\n    of mutations in reads from new and old RNA. It is important to\n    assess these assumptions when interpreting mixture model fits.\n-   Extending the simplest two-component mixture model is promising but\n    potentially fraught. Carefully validate improvements in model fit\n    and stability of parameter estimates. Consider using regularization\n    strategies to avoid unrealistic parameter estimates.\n\n**Inferring kinetic parameters:**\n\n-   Unless using a label time significantly shorter than the average\n    half-life of RNAs of interest (for example, this is around 4 hours\n    for mRNAs in mice and humans), the number of reads from labeled RNA\n    is reflective of both transcription and turnover kinetics, not just\n    the former.\n-   The relative abundances of reads from labeled and unlabeled RNA is\n    best modeled as a function of both transcription and degradation\n    rate constants. The functional form of this relationship depends on\n    your model for the dynamics of the RNA species you are probing.\n-   Assuming that the probed RNA populations are at steady-state during\n    labeling simplifies the task of kinetic parameter estimation. This\n    assumption can break down when labeling is done during or following\n    a perturbation. Analysis strategies exist which relax this\n    assumption, but they rely on having measurements of RNA abundances\n    at the start of labeling (i.e., no-label RNA-seq data from that time\n    point or a labeled NR-seq data in which RNA was extracted at that\n    point). grandR and EZbakR implement both of these analysis\n    strategies.\n-   Combining NR-seq with subcellular fractionation is a powerful way by\n    which to explore the kinetics of processes invisible to whole-cell\n    NR-seq. Analyzing this data requires a strategy to normalize read\n    counts and integrate information across all compartments. EZbakR\n    implements such strategies.\n\n**Processing NR-seq data:**\n\n-   Aligning NR-seq reads is difficult due to the chemically induced\n    T-to-C mismatches. While 3-base genome alignment strategies, popular\n    in analyses of bisulfite sequencing data, are a potential solution,\n    they often provide only minimal advantages over standard 4-base\n    alignment approaches while also suffering from their own unique\n    biases and limitations. This is in large part due to the fact that\n    in NR-seq reads, most Ts are not converted to Cs. Thus, the\n    downsides of aligning to a lower complexity genome may often nullify\n    the benefits of not penalizing T-to-C mismatches.\n-   Specialized alignment approaches may provide an improvement over\n    either a standard 3- or 4-base genome alignment approach.\n    grandRescue is a recently developed approach that implements 4-base\n    alignment followed by 3-base alignment of reads that fail to align\n    via the 4-base strategy. This approach can help recover high\n    mutation content sequencing reads.\n-   When processing NR-seq data, you have a choice to make about what\n    genomic features you want to assign reads to and perform analyses\n    on. Most tools support performing analyses at the gene- (or 3'-UTR\n    if using 3'-end sequencing) level, but fastq2EZbakR significantly\n    expands the set of features you can analyze to include transcript\n    isoforms, exon-exon junctions, etc.\n\n**Experimental design:**\n\n-   Label-free control samples (a.k.a. no-label controls) are crucial to\n    detect potential biases introduced by labeling (e.g., dropout of\n    labeled RNA).\n-   No-label controls can be used to both assess and correct for these\n    biases in some cases. grandR and EZbakR provide strategies for this\n    task.\n-   Bias correction strategies make assumptions like any statistical\n    method, and these assumptions should be assessed when interpreting\n    the output of bias correction.\n-   While unique analysis strategies have been proposed that may be\n    strictly compatible with pulse-chases, pulse-chases suffer from the\n    following serious shortcomings:\n    -   Prolonged exposure to metabolic label, which can lead to adverse\n        effects\n    -   Increased variance in kinetic parameter estimation due to having\n        to compare the estimated fraction of reads that are labeled at\n        the end of the pulse to that at the end of the chase. Compare\n        this with a steady-state pulse-label analysis where the only\n        source of variance is that of the fraction labeled estimate for\n        the pulse.\n    -   The analysis is complicated by the potential for incomplete\n        competition of metabolic label with the chase nucleotide and\n        recycling of metabolic label from degraded RNA.\n    -   Higher cost due to the necessity of more samples (set of pulses\n        and set of chases).\n-   The best label time for standard NR-seq kinetic parameter inference\n    is around the median half-life of the RNAs you wish to probe the\n    kinetics of. It's better to undershoot than overshoot this target\n    though to avoid adverse effects of prolonged metabolic labeling.\n-   Accurate kinetic parameter estimates are obtainable with standard\n    RNA-seq sequencing depth. More depth can significantly improve these\n    analyses though. Sequencing depth is particularly important if\n    wanting to perform analyses on the sub-gene (e.g., exon-exon\n    junction) level.\n\nAdditionally, an exciting advance in the NR-seq field is the number of\nunique extensions that have been developed to apply nucleotide recoding\nto the study of several aspects of RNA dynamics. This review concludes\nwith a summary of currently published NR-seq extensions.\n\n## A brief history of NR-seq\n\nOne of the first strategies to assess the kinetics of RNA synthesis and\ndegradation combined global transcriptional inhibition with RNA-seq\n[@RN108; @RN106; @RN109]. RNA levels following a time course of\ninhibition could be fit to exponential decay curves to assess turnover\nkinetics in high-throughput. Combining this with pre-inhibition\nabundance information allows one to assess synthesis kinetics. While\nthis approach continues to be widely used, it suffers from a number of\ndrawbacks. For one, rigorous normalization is needed to track the\ndecreasing absolute levels of RNA. As RNA-seq only provides a relative\nmeasure of RNA abundance, this typically requires exogenous spike-ins to\naccount for the global differences in RNA abundance between inhibition\ntimepoints [@RN113; @RN114]. This introduces additional experimental\ncomplexity that requires optimization. In addition, global transcription\ninhibition causes the cells to launch myriad stress responses, many of\nwhich affect transcript stability [@RN110; @RN112]. This represents a\nconfounder that complicates the interpretation of transcriptional\ninhibition data. Thus, a strategy to probe the kinetics of RNA without\nsignificant perturbation of the system was needed.\n\nMetabolic labeling with nucleotide analogs offers one such strategy\n[@RN116; @RN119; @RN115; @RN120]. Cells will incorporate these labels\ninto nascent RNA, leading to the existence of two distinct populations\nof RNA: unlabeled, old RNA that existed at the start of labeling and new\nRNA that had the potential to incorporate metabolic label. Tracking the\ndynamics of these two populations yields the information necessary to\ndissect transcription and degradation kinetics of each RNA in a\npopulation of cells. Originally, doing so required biochemically\nseparating the two populations and sequencing each (or more specifically\nenriching for one and sequencing the enriched and input samples). This\neither relied on chemistries that conjugated biotin to incorporated\nlabels so that labeled RNA could be separated from unlabeled via a\nbiotin-streptavidin pull down [@RN117; @RN119], or immunoprecipitation\nvia antibodies that specifically recognize the metabolic label [@RN120].\nWhile powerful, these enrichment-based techniques require substantial\namounts of starting RNA, introduce biochemical biases during enrichment,\nand cannot distinguish the desired enriched RNAs from nontrivial levels\nof contamination [@RN121; @RN460]. Labeled and unlabeled spike-ins have\nbeen proposed to quantify and account for some of these challenges, but\nthese introduce their own experimental challenges and are unable to\naccount for length biases in the enrichment [@RN196]. Further innovation\nwas thus required to improve the robustness of metabolic labeling\nstrategies.\n\nSeveral labs (including the Simon lab, where I did my PhD) addressed\nthese shortcomings by developing nucleotide recoding RNA-seq methods\n(NR-seq). These techniques combine s^4^U metabolic labeling and\nnucleotide recoding chemistry (TimeLapse [@RN88], SLAM [@RN89], TUC\n[@RN192], etc.) to either convert or disrupt the hydrogen bonding\npattern of incorporated s^4^U. This yields apparent T-to-C mutations in\nthe RNA-seq data that indicate sites of s^4^U incorporation and can be\nused to estimate the fraction of extracted RNA that was synthesized\nafter the introduction of metabolic label. This adds kinetic information\nto the snapshot provided by RNA-seq while eliminating the need for\nenrichment of labeled RNA. A simplified schematic of NR-seq data and its\nanalysis is presented in @fig-1. T-to-C mutations in sequencing reads\ncan be used to bioinformatically quantify the levels of labeled and\nunlabeled RNA, with the Simon lab originally introducing the now\ngold-standard mixture modeling approach for this task [@RN88]. Simple\nkinetic models of the trajectories of these two species relate the\nkinetic parameters of interest to the data obtained. This is how NR-seq\ncan quantify the kinetics of gene expression.\n\n## Analyzing NR-seq data\n\nHere I discuss how to analyze processed NR-seq data before discussing\nthe details of NR-seq data processing. This will allow me to better\nmotivate the importance of optimally processed NR-seq data. This means I\nwill assume that you have information about how many instances of a\ngiven read-vs-reference mismatch type (e.g., T-to-C mismatches) were in\neach read, and how many mutable nucleotides were in the reference\nsequence to which this read aligned (e.g., number of reference T's).\nThroughout these and later sections, my discussion will assume a\npulse-label (vs. a pule-chase) design was utilized, as the optimality of\nthis approach will be discussed later.\n\n### Modeling the mutational content of sequencing reads\n\nIn NR-seq data, two populations of sequencing reads exist: those\noriginating from RNA synthesized prior to metabolic labeling (the\nunlabeled RNA; old RNA in a pulse-label design), and those originating\nfrom RNA synthesized during metabolic labeling (the labeled RNA; new RNA\nin a pulse-label design). On average, the mutational content of the\nlatter will be higher than that of the former. Analyzing NR-seq data\nrequires making use of this fact to infer the fraction of reads coming\nfrom each of these two populations for a given read.\n\nOriginally, two ideas were proposed. The simplest was to use a mutation\ncontent cutoff to classify reads as coming from labeled or unlabeled RNA\n[@RN156]. In other words, all reads with N or more (N usually being 1 or\n2) mutations were classified as \"labeled\", and all other reads were\nclassified as \"unlabeled\". This strategy is intuitive and\ncomputationally efficient. Despite this, it suffers from some serious\nshortcomings. For one, the mutational content of reads from labeled RNA\nis largely a function of three factors: 1) the U-content of the region\nof the RNA from which the read is derived, 2) the metabolic label\nincorporation and chemical recoding efficiencies, and 3) the background\nmutation rate. While the background mutation rate is often fairly\nconstant across samples and RNAs, the other two factors are subject to\nlarge amounts of read-to-read and sample-to-sample variability. For\nexample, different RNAs can have very different average U-contents. In\naddition, perturbing cellular metabolism often decreases label\nincorporation rates [@RN159; @RN94]. This latter point is an especially\nconcerning batch effect, as it can cause the mutation content of reads\nfrom a given feature to vary not because the amount of labeled RNA\ndiffers between two conditions, but because the incorporation rate is\nlower in one condition versus the other. The result is that mutation\ncontent cutoffs often provide a simple but biased estimate for the\namount of labeled RNA from a given feature [@RN180].\n\nA more robust analysis strategy is mixture modeling [@RN88; @RN155]. In\nthis strategy, assumptions are made about the distributions that best\ndescribe the expected mutational content from labeled and unlabeled RNA.\nFor example, the number of mutations in reads from these two populations\ncould be modeled as following a Poisson distribution with some mean, a\nmean which is necessarily higher in the labeled RNA reads than the\nunlabeled RNA reads. Due to the high amounts of read-to-read variance in\nU-content though, modeling the mutational content as a binomial\ndistribution that takes into account both the incorporation/recoding\nrate as well as the read's U-content is optimal [@RN88; @RN155]. This\nstrategy (or slight variants of it), known as two-component binomial\nmixture modeling (TCBMM), was thus implemented in analysis software such\nas GRAND-SLAM, bakR, Halfpipe, and EZbakR. Mixture modeling has been\nshown to provide unbiased estimates of labeled RNA abundance, even in\nthe face of relatively low incorporation/recoding rates (@fig-2).\n\n![**Simple mutation cutoff analysis strategies are less accurate than\nmixture modeling.** From [@RN180]. Comparison of accuracy of 3 analysis\nstrategies on 3 simulated datasets. Each column represents analysis for\na particular dataset. Datasets differ in their simulated plabeled (pnew\nin formalism presented here; 1%, 2.5%, and 5% mutation rates). Each row\nrepresents a particular analysis strategy. Top row: labeled reads are\ndefined as those with at least 1 (\\> 0) mutation. This is the strategy\nimplemented in SLAMDUNK by default. Middle row: labeled reads are\ndefined as those with at least 2 (\\> 1) mutations. Represents another\ncommonly used NR-seq analysis cutoff. Bottom row: EZbakR analysis with\ntwo-component mixture modeling. Points are colored by density in all\nplots. $\\theta$ = fraction of reads from labeled RNA. Red dotted lines\nrepresent perfect estimation.](Figure_S1.png){#fig-2}\n\nThe likelihood function for TCBMM (@fig-3) can be generalized as such:\n\n$$\n\\begin{gather}\nL(\\theta, p_{\\text{new}}, p_{\\text{old}}) = \\theta \\cdot \\text{Binomial}(\\text{nM}, \\text{nN}, p_{\\text{new}}) + (1 - \\theta) \\cdot \\text{Binomial}(\\text{nM}, \\text{nN}, p_{\\text{old}})  \\\\\n\\text{Binomial}(\\text{nM}, \\text{nN}, p_{\\text{old}}) =  \\binom{\\text{nN}}{\\text{nM}}p^{\\text{nM}}(1-p)^{\\text{nN} - \\text{nM}} \\\\ \n\\theta = \\text{fraction of reads from labeled RNA (aka the new-to-total ratio, or NTR)} \\\\\n\\text{nM} = \\text{number of mutations (e.g., T-to-C mutations in a standard }\\text{s}^{4}\\text{U} \\text{ NR-seq analysis)} \\\\\n\\text{nN} = \\text{number of mutable nucleotides (e.g., Ts in a standard }\\text{s}^{4}\\text{U} \\text{ NR-seq analysis)} \\\\\np_{\\text{new}}, p_{\\text{old}} = \\text{mutation probability in reads from new (labeled) and old (unlabeled) RNA}\n\\end{gather}\n$$\n\n![**Two-component binomial mixture model.** Adapted from [@RN159]. The\ncounts of T-to-C mutations, given a number of mutable nucleotides, can\nbe modeled as coming from a mixture of two binomial distribution. One of\nthese (that describing the mutation content of reads from new RNA), has\na higher probability of a “success” (mutation) than the\nother.](TCBMM.png){#fig-3}\n\n### Modifying TCBMM\n\nThe power of mixture modeling lies in both its robustness as well as its\nextensibility. TCBMM makes several assumptions about the mutational\ncontent of NR-seq reads; namely that:\n\n-   Every uridine in an RNA synthesized in the presence of label was\n    equally likely to get replaced with s^4^U. This is formalized above\n    by there being only one $p_{\\text{new}}$.\n-   Every sequenced uridine in an unlabeled RNA was equally likely to\n    give rise to a non-s^4^U-related mutation due to sequencing errors,\n    alignment errors, etc. This is formalized above by there being only\n    one $p_{\\text{old}}$.\n-   By default, all existing tools (GRAND-SLAM, bakR, Halfpipe, and\n    EZbakR) assume that the mutation rate in reads from labeled and\n    unlabeled RNA are sample-wide global parameters. That is, all RNAs\n    transcribed from all genes are assumed to have the same rate of\n    s^4^U incorporation, and reads from these RNAs are subject to the\n    same background mutation rate. Thus$p_{\\text{new}}$ and\n    $p_{\\text{old}}$ are assumed to be the same for all genes in the\n    above formalism.\n\nIf users find one or more of these assumptions to be violated, they can\nattempt to modify and extend this model. Towards that end, several\nmodification of standard TCBMM have been proposed. These include:\n\n-   Three-component mixture modeling, where a second population of reads\n    from unlabeled RNA with a higher mutation rate (presumably due to\n    heightened alignment errors) is modeled.\n-   Overdisperse mixture modeling where an overdisperse binomial\n    distribution (e.g., a beta-binomial) replaces one or both of the\n    binomial distribution components, or where a different incorporation\n    rate parameter is estimated for fast and slow turnover RNA.\n-   Hierarchical mixture modeling where a sample-wide average\n    incorporation rate is inferred and used as a strongly regularizing\n    prior to estimate feature-specific incorporation rates.\n-   Modeling the transcription process, which at short label times leads\n    to an expected position-dependency in the incorporation rate.\n\nWhile all of these are theoretically promising, the challenge of fitting\nmore complex models is two-fold. 1) Their increased flexibility comes\nwith an increased risk of overfitting. This can lead to estimate\ninstability, where a better model fit yields extreme conclusions about\nRNA dynamics (e.g., unusually high fraction new and thus unrealistically\nrapid turnover kinetics). 2) While an alternative model may capture one\naspect of the true data generating process unaccounted for by TCBMM, it\nmay amplify biases that arise from not accounting for some other aspect\nof the data generating process.\n\nTo illustrate point 1, consider the task of fitting a TCBMM with\nfeature-specific mutation rates. While in theory, it is straightforward\nto obtain maximum likelihood estimates for the parameters of such a\nmodel, model flexibility can make interpretation of maximum likelihood\nparameters fraught. Intuitively, this is because changing different\nparameters can have similar expected impacts on your data. A higher\nfraction new will yield more reads with high mutational content, but so\nwill a low fraction new combined with a higher background mutation rate.\nWhile with enough reads these two situations can be accurately\ndeconvolved, this analysis is highly uncertain for low coverage\nfeatures.\n\nTo illustrate point 2, consider the idea of three-component mixture\nmodeling. While this can capture certain types of overdispersion in\nmutation rates from old RNA reads, it can amplify biases from not\nmodeling overdispersion in mutation rates from new RNA reads. A\nthree-component mixture model will classify many moderate mutation rate\nreads as \"old\", when in fact they may represent a preponderance of low\nmutation rate new reads. This kind of overdispersion is made even more\nlikely by the fact that metabolic label availability will likely ramp\nup and down over time [@RN193]. Thus, reads from RNA synthesized at\ndifferent time points may have different true mutation rates.\n\nHow can one navigate building more complex models while avoiding some of\nthese problems? Point 1 can be addressed through regularization. From a\nBayesian perspective, this means using one's domain expertise or trends\nin these high-throughput datasets to craft informative priors that\nconstrain the parameter search space [@RN203]. For example, to fit a\nhierarchical mixture model in EZbakR, where each feature is allowed to\nhave its own new read mutation rate ($p_{\\text{new}}$), I crafted a\nstrategy to infer strongly regularizing priors from sample-wide trends\n[@RN180]. These priors were designed to be very conservative to limit\nestimate variance.\n\nPoint 2 represents the fundamental challenge of statistical modeling:\ncrafting a model of the data generating process that faithfully captures\nmost of the relevant sources of variance in one's data. This is\ndifficult, but several strategies exist to navigate this complexity.\nInformation criteria are a popular metric by which to compare fits of a\nmore complex model to that of a simpler model. These criteria are\ndesigned to penalize model complexity to avoid rewarding overfit models\nwith better metrics [@RN208; @RN207]. While simple criteria like the\nAkaike information criteria (AIC) are popular due to their\nimplementation ease, more robust metrics have been developed since the\nadvent of AIC [@RN197; @RN200; @RN198]. For example, in the context of\nmixture modeling, the widely applicable information criteria (WAIC;\na.k.a. the Watanabe-Akaike Information Criteria) may provide a number of\nadvantages over AIC [@RN199; @RN200].\n\nInformation criteria are not panacea though. Information criteria are\nrigorous ways to assess if added model complexity is capture a\nsignificant amount of variance in your data that a simple model fails to\naccount for. Even if a more complex model is succeeding by this metric,\nit could still be providing biased estimates [@RN208; @RN207]. Thus,\nwhen designing new NR-seq models, it is advisable to use a multi-pronged\napproach that weighs several metrics when deciding if a more complex\nmodel is worth using. If adopting a Bayesian approach, information\ncriteria can be complemented with posterior predictive checks, where\ndata is simulated from the fit model and compared to the analyzed data\n[@RN202]. Serious discrepancies between simulated and real data can\nreveal model mis-specifications and guide model improvement. We also\nsuggest comparing results given by standard TCBMM with those provided by\na more complex model. Extreme sample-wide discrepancies between the two\nmay signify that the more complex model is overfitting or providing\nunstable estimates. Discrepancies should thus be thoroughly explored and\nexplained. Finally, one should assess model robustness through\nsimulations from a data generating process more complicated than that\nused for model fitting [@RN159]. If the bias introduced by these true\nvs. assumed data generating process discrepancies is amplified by use of\na more complex model relative to TCBMM, we urge caution in adopting the\nmore complex model. The simplicity and robustness of TCBMM makes it an\neffective baseline with which to compare alternative models.\n\n### Transcript isoform analyses of short-read data\n\nTranscript isoforms are the RNA species whose synthesis and degradation\nkinetics are of biological significance. Despite this, quantifying the\nNTRs of individual transcript isoforms in a short-read NR-seq experiment\nis challenging. This is because most short reads cannot be unambiguously\nassigned to a specific transcript isoform. Strategies have been\ndeveloped to overcome these challenges in the context of quantifying the\nabundances of isoforms [@RN209; @RN212; @RN213; @RN211; @RN210]. I thus\ndeveloped a similar approach to estimate isoform-specific NTRs in short\nread NR-seq data [@RN94].\n\nThe approach, implemented in the EZbakR suite, combines standard NR-seq\nTCBMM with transcript isoform quantification. This approach estimates\nNTRs for each observed transcript equivalence class (TEC; i.e., the set\nof isoforms with which a read is compatible [@RN214]) and integrates\nthis with estimates of transcript isoform abundances from standard tools\nfor this task. EZbakR is able to estimate isoform-specific NTRs by\ndeconvolving TEC NTRs using a novel beta mixing model. I used this\napproach while in the Simon lab, and in collaboration with Bobby Hogg's\nlab at the NIH, to study the synthesis and degradation kinetics of\nindividual transcript isoforms, and to identify NMD sensitive isoforms\n[@RN94].\n\nAccurate transcript isoform analyses require annotations of expressed\nisoforms [@RN215; @RN139]. In our work presenting the isoform-NTR\nestimation strategy, we noted that standard off-the-shelf references did\nnot faithfully reflect our particular cell line's transcriptome. We thus\nexplored strategies using StringTie2 and custom filtering to build more\naccurate, bespoke annotations [@RN134]. We showed that this approach\nsignificantly improved the accuracy of transcript-isoform level\nanalyses. While a powerful approach, the need to build custom\nannotations adds to the complexity of the workflow. In addition, tools\nfor building such annotations are not without their limitations, as ab\ninitio assembly is a fundamentally difficult task. Having matched,\nunlabeled, long read data can improve assembly, but presents its own\nchallenges and shortcomings [@RN134]. Thus, while isoform-level analyses\nrepresent a powerful new paradigm in analyses of NR-seq data, I urge\nusers to carefully assess the potential of annotation-related biases in\ntheir analyses.\n\n### Modeling and correcting for dropout in NR-seq data\n\nLike all other RNA-seq based methods, NR-seq data can be plagued by\nvarious biases. The most prominent example of this is dropout, a\nphenomenon observed across many distinct datasets [@RN185; @RN176].\nDropout is the underrepresentation of reads from labeled RNA. While its\norigins are not fully understood, it has been proposed to be caused by a\ncombination of disproportionate loss of labeled RNA during RNA\nextraction and library preparation, loss of high mutation content reads\nduring alignment, and toxicity due to s^4^U labeling.\n\nDropout in NR-seq data can be detected and quantified with the help of\nno-label controls. A no-label control refers to data from samples not\nfed with a metabolic label. These are important controls that should be\nincluded in all NR-seq datasets. A simple model of dropout is that there\nexists a global rate at which label-containing RNA is lost relative to\nunlabeled RNA, referred to as the dropout rate [@RN176]. Thus, rapidly\nturned over RNA that are more highly labeled will be disproportionately\naffected compared to more stable, relatively unlabeled RNA. Comparing\nthe estimated turnover rate in the labeled samples to the no-label vs.\nlabeled read counts can thus reveal dropout. More specifically, dropout\nlooks like a strong correlation between these quantities. Plots like\nthose shown in @fig-4 can be easily made with both grandR and EZbakR.\n\n![**Visualizing dropout.** A simple model of dropout was simulated using\nEZbakR’s EZSimulate() function, where labeled RNA is lost relative to\nunlabeled RNA at a rate denoted above each pair of plots (pdo). grandR\nimplements a visualization strategy that correlates the NTR rank (1 =\nsmallest) vs. the log-difference in +label and -label read coverage\n(dropout). The strength of correlation between these is related to the\nrate of dropout. bakR implements a visualization strategy that allows\nfor direct assessment of the fit of a simple dropout model on the data.\nThe quantities plotted are similar to those in grandR, just on different\nmodel-relevant scales. EZbakR implements both of these visualization\nstrategies.](Dropout_plots.png){#fig-4}\n\nIf your NR-seq data suffers from non-trivial amounts of dropout, you can\nemploy strategies to correct for dropout. grandR was the first tool to\nimplement such a strategy. It follows from a model alluded to above and\nassumes that there is a sample-wide rate (call it\n$\\text{p}_{\\text{do}}$) at which labeled RNA is disproportionately lost\nrelative to unlabeled RNA. If this is the case, then then true fraction\nof reads from labeled RNA ($\\theta_{\\text{true}}$) is related to the\ndropout-biased estimate ($\\theta_{\\text{do}}$) like so:\n\n$$\n\\begin{gather}\n\\theta_{\\text{true}} = \\frac{\\theta_{\\text{do}}\\cdot\\text{p}_{\\text{do}}}{\\theta_{\\text{do}}\\cdot\\text{p}_{\\text{do}} + (1-\\theta_{\\text{do}})}\n\\end{gather}\n$$\n\nSimilarly, a relationship exists between the true expected read counts\nfrom a given feature and the observed, labeled sample read count:\n\n$$\n\\begin{gather}\n\\text{R}_{\\text{true}} = \\text{R}_{\\text{do}} \\cdot \\frac{\\theta_{\\text{G}}\\cdot(1-\\text{p}_{\\text{do}}) + (1-\\theta_{\\text{G}})}{\\theta_{\\text{true}}\\cdot(1-\\text{p}_{\\text{do}}) + (1-\\theta_{\\text{G}})} \\\\\n\\theta_{\\text{G}} = \\frac{\\theta_{\\text{G,do}}}{(1-\\text{p}_{\\text{do}}) + \\theta_{\\text{G,do}} \\cdot \\text{p}_{\\text{do}}} \\\\\n\\theta_{\\text{G,do}} = \\frac{\\sum_{\\text{j=1}}^{\\text{NF}} \\theta_{\\text{do,j}} \\cdot \\text{R}_{\\text{j}}}{\\sum_{\\text{j=1}}^{\\text{NF}} \\text{R}_{\\text{j}}}\n\\end{gather}\n$$\n\nUsing these theoretical relationships, a dropout rate can be estimated\nthat, after correcting read counts, yields no correlation between the\nlabeled vs. no-label read count ratio and the turnover kinetics of an\nRNA. This strategy is implemented in grandR. bakR implements a similar\nstrategy by which the relationship between dropout and the NTR is\nmodeled and fit with the method of maximum likelihood. EZbakR implements\nboth of these strategies.\n\nDropout correction is a powerful addition to the NR-seq analysis\ntoolkit. Despite this, it is not without its limitations. The\nrequirement for matched no-label samples in all conditions tested adds\nto the experimental burden. In addition, even when no-label data is\ncollected, resource constraints often lead researchers to only collect a\nsingle replicate of this data, as it is not itself a useful NR-seq\nsample. This can make the dropout metric, the ratio of labeled:no-label\nread counts, noisy.\n\nTo address these limitations, EZbakR implements a strategy I refer to as\ndropout normalization. Dropout normalization does not require any\nno-label data and involves comparing internally normalized NTRs rather\nthan read counts across samples. The strategy starts by identifying the\nlowest dropout sample (e.g., that which provides the lowest median\nuncorrected half-life estimate) and estimating dropout in other samples\nrelative to this sample. This estimated relative dropout rate is then\nused to correct NTRs and read counts in all samples. This strategy has\nproven particularly useful as dropout rates often correlate with\nbiological conditions, which risks confounding comparative analyses if\nnot properly accounted for. The downside of dropout normalization is\nthat it tends to normalize out global differences in half-life\nestimates, even if these are biologically real and not solely the result\nof dropout. Dropout is thus similar in spirit to RNA-seq read count\nnormalization methods such as the median-of-ratios or TMM, as well as a\nsimpler median-kdeg normalization strategy implemented in grandR, which\nall effectively assume that there are no real global differences in RNA\nlevels/turnover kinetics across samples. Dropout normalization\nalternatively assumes that any global changes in RNA turnover kinetics\nare dropout driven. Users should thus be aware of this assumption when\nusing dropout normalization.\n\n## Kinetic parameter estimation with NR-seq data\n\nOne of the original motivations for developing NR-seq was to robustly\nestimate the kinetics of RNA synthesis and degradation. In this section,\nI discuss the modeling and assumptions necessary to make this possible.\n\n### Standard kinetic analyses\n\nVanilla, bulk NR-seq is a powerful method by which to quantify the\nkinetics of RNA synthesis and degradation. A typical kinetic analysis\nmeans modeling the NTR of reads from a feature (e.g., the union of exons\nat a gene) as a function of the RNA's synthesis and degradation rate\nconstants. The simplest identifiable model of this sort assumes that\nmature mRNA is synthesized at a rate $\\text{k}_{\\text{syn}}$ and\ndegraded with a rate constant $\\text{k}_{\\text{deg}}$. This model can be\nformalized via the following analytically tractable differential\nequation:\n\n$$\n\\begin{gather}\n\\frac{\\text{dR}}{\\text{dt}} = \\text{k}_{\\text{syn}} - \\text{k}_{\\text{deg}} \\cdot \\text{R} \\\\\n\\text{Solution (with R(0) = 0)}: \\text{R(t)} = \\frac{\\text{k}_{\\text{syn}}}{\\text{k}_{\\text{deg}}} \\cdot (1 - \\text{e}^{\\text{k}_{\\text{deg}} \\cdot \\text{t}}) \\\\\n\\end{gather}\n$$\n\nAt steady-state, the following relationships hold:\n\n$$\n\\begin{gather}\n\\frac{\\text{dR}}{\\text{dt}} = 0;\\text{ }\\text{R}_{\\text{ss}} = \\frac{\\text{k}_{\\text{syn}}}{\\text{k}_{\\text{deg}}} \\\\\n\\text{NTR} = \\frac{\\text{R(tl)}}{\\text{R}_{\\text{ss}}} = 1 - \\text{e}^{\\text{k}_{\\text{deg}} \\cdot \\text{t}} \\\\\n\\text{k}_{\\text{deg}} = \\frac{-\\text{ln}(1-\\text{NTR})}{\\text{t}_{\\text{label}}}\n\\end{gather}\n$$ This model makes several explicit assumptions:\n\n-   Steady-state: this means that during the labeling, RNA levels are\n    not changing. While individual cells may rarely ever be at\n    steady-state, e.g., as transcript levels are regulated throughout\n    the cell cycle, in bulk NR-seq this means assuming that the average\n    RNA levels across all cells assayed are constant.\n-   Zeroth-order synthesis kinetics (i.e., transcription is a Poisson\n    process): this means that there exists a single, constant rate of\n    transcription during the labeling. While once again often violated\n    in single cells due to phenomena such as transcriptional bursting\n    [@RN217], it is likely a decent model of bulk transcriptional\n    behavior [@RN122].\n-   First-order degradation (i.e., exponential decay): this means that\n    RNAs have a characteristic half-life that does not change over\n    either the course of the labeling or their lifetime. This means\n    assuming that RNAs are \"ageless\" or that the probability an RNA\n    degrades in the next instance is independent of how long it has been\n    around. This assumption breaks down if multiple rate-limiting steps\n    separate an RNA's birth from its death (e.g., if RNA export from the\n    nucleus is on a similar time-scale of cytoplasmic degradation), or\n    if there exist multiple sub-populations of RNA with different decay\n    kinetics (e.g., if nuclear mRNA is degraded at a different rate than\n    cytoplasmic mRNA) [@RN216; @RN218; @RN91]. While this assumption is\n    inevitably violated to some extent due to the complexity of the RNA\n    life cycle, it has consistently proven to be a reasonable\n    approximation in a variety of settings, while still providing a\n    useful (if somewhat biased) picture of general turnover kinetics\n    when it is violated.\n\n### Non-steady-state modeling\n\nThe steady-state assumption makes analyzing and interpreting NR-seq data\neasy. Despite this, it is often violated in contexts where you are\napplying a perturbation to cells shortly before or during labeling.\nNarain et al. proposed a strategy to obtain unbiased estimates even in\nthis setting [@RN146]. The intuition behind how this approach works is\nthat no matter what, the dynamics of old (unlabeled) RNA is entirely a\nfunction of turnover kinetics. Thus, if you know the levels of an RNA at\nthe start of labeling, then that combined with its level at the end of\nlabeling tells you how much RNA decayed in that time frame. Formally:\n\n$$\n\\begin{gather}\n\\text{R}_{\\text{old}}(\\text{t}) = \\text{R}_{\\text{init}} \\cdot e^{-\\text{k}_{\\text{deg}} \\cdot \\text{t}} \\\\\n\\text{R}_{\\text{new}}(\\text{t}) = \\frac{\\text{k}_{\\text{syn}}}{\\text{k}_{\\text{deg}}} \\cdot (1 - e^{\\text{k}_{\\text{deg}} \\cdot \\text{t}}) \\\\\n-\\frac{\\text{ln}(\\frac{\\text{R}_{\\text{old}}(\\text{t}_{\\text{label}})}{\\text{R}_{\\text{init}}})}{\\text{t}_{\\text{label}}} = \\text{k}_{\\text{deg}} \\\\\n\\frac{\\text{R}_{\\text{new}}(\\text{t}_{\\text{label}})}{1-e^{\\text{k}_{\\text{deg}} \\cdot \\text{t}}}\\cdot \\text{k}_{\\text{deg}} = \\text{k}_{\\text{syn}}\n\\end{gather}\n$$\n\nAn estimate for $\\text{R}_{\\text{init}}$ typically comes from matched\nRNA-seq data collected at a time point equivalent to that at which\nlabeling was started for a given labeled sample. Note, the stability or\nsynthesis rate of an RNA could be changing during the labeling. The\n$\\text{k}_{\\text{deg}}$ and $\\text{k}_{\\text{syn}}$ estimates from this\nstrategy should thus be thought of as the time-averaged value of a\npotentially time-varying $\\text{k}_{\\text{deg}}(\\text{t})$ and\n$\\text{k}_{\\text{syn}}(\\text{t})$. Both grandR and EZbakR implement this\nanalysis strategy.\n\nWhile having an approach that yields theoretically unbiased estimates of\nturnover and synthesis kinetics even in the face of non-steady-state\ndynamics is powerful, it is not without its limitations. For one, a\nunique experimental design is required. If you are missing RNA-seq data\nfrom the start of any labeling period, this strategy cannot be applied.\nIn addition, even with the proper experimental design, this analysis\napproach has some weaknesses. Its statistical properties are suboptimal\nrelative to the steady-state analysis. In particular, the variance of\nthe $\\text{k}_{\\text{deg}}$ estimation is typically higher than that of\nthe steady-state estimate, especially at low NTRs (@fig-5). Intuitively,\nthis is because the steady-state estimate is only a function of a single\nsample's NTR, while the non-steady-state estimate requires comparing an\nNTR estimate combined with a normalized read count in one sample to a\nnormalized read count in another sample. Thus, while the estimate is\ntechnically unbiased, its mean-squared error is not necessarily lower\nthan that of the biased steady-state analysis. This problem can\npartially be addressed through greater sequencing depth, label time\noptimization, and more replicates, but it is a challenge that users\nshould be aware of.\n\n![**Comparison of steady-state and non-steady-state (NSS) analysis\nstrategies.** Steady-state data was simulated using EZSimulate() at 3\ndifferent label times (2, 4, and 6 hours). Both of these analysis\nstrategies are appropriate in this setting; this comparison is meant to\nshow differences in the best-case variance properties of these two\napproaches. 2 replicates of labeled and 2 replicates of no-label data\nwere simulated. The simulated (true) and estimated degradation rate\nconstants are compared in all plots. Points are colored by their local\ndensity. Red dotted line represents perfect\nestimation.](NSS.png){#fig-5}\n\n### Subcellular NR-seq\n\nSeveral studies have combined NR-seq with subcellular fractionation to\ninvestigate the kinetics of subcellular RNA trafficking. To date, such\nefforts have included sequencing chromatin-associated RNA [@RN91;\n@RN220], nuclear RNA [@RN91; @RN220; @RN151; @RN162; @RN177; @RN149],\ncytoplasmic RNA [@RN91; @RN220; @RN151; @RN162; @RN177; @RN149],\nribosome-associated RNA [@RN91], and membrane-associated RNA [@RN177].\nThese data necessitate fitting more complex kinetic models to\nmulti-sample datasets. While Halfpipe was the first tool to support\nanalyses of one particular fractionation scheme, EZbakR is the first\ntool designed to support fitting any such model.\n\nEZbakR makes use of the fact that the most common identifiable models of\nRNA dynamics that subcellular NR-seq has been used to fit can be\nformalized as a linear system of ordinary differential equations (ODEs)\nof the form:\n\n$$\n\\begin{gather}\n\\textbf{R} = \\text{M} \\cdot \\dot{\\textbf{R}} + \\textbf{b} \\\\\n\\textbf{R} = \\text{vector of RNA species abundances of length N} \\\\\n\\text{M} = \\text{NxN matrix of first-order kinetic parameters} \\\\\n\\dot{\\textbf{R}} = \\text{time derivative of } \\textbf{R} \\\\\n\\textbf{b} = \\text{vector of zeroth-order rate constants (e.g., synthesis rate)}\n\\end{gather}\n$$\n\nAny such system of ODEs has an analytic solution inferrable from the\nmatrix M and forcing vector $\\textbf{b}$. EZbakR makes use of this fact\nto efficiently fit these models without needing to rely on numerical\nintegration. It also provides means for uncertainty quantification that\nis capable of flagging instances of practical unidentifiability, i.e.,\nwhen specific parameters of a model are outside of the dynamic range of\nwhat can be reasonably estimated given experimental details and noise.\n\nEZbakR also implements a novel strategy by which to normalize certain\nclasses of subcellular NR-seq. This strategy relies on having data from\nindividual fractions, as well as a sufficient set of combinations of\nfractions (e.g., nuclear, cytoplasmic, and whole cell). It uses the fact\nthat the total fraction of reads in each of the sub-compartments that\nare new (when adjusted for average length differences in the RNA\npopulations) is related to the same quantity in the combination\ncompartment [@RN177; @RN180]. The scale factor that relates these\nquantities is precisely the ratio of absolute abundances between the\nsub-compartments. For the whole-cell, nuclear, and cytoplasmic example,\nthis relationship looks like:\n\n$$\n\\begin{gather}\n\\theta_{\\text{WC}} = \\frac{[\\text{C}]}{[\\text{C}] + [\\text{N}]}\\theta_{\\text{C}} + \\frac{[\\text{N}]}{[\\text{C}] + [\\text{N}]}\\theta_{\\text{N}} = s\\cdot\\theta_{\\text{C}} + (1-s)\\cdot\\theta_{\\text{N}} \\\\\n\\theta = \\text{total fraction of reads from new RNA in a given compartment} \\\\\n\\text{WC = whole-cell; C = cytoplasm; N = nucleus} \\\\\n[\\text{C}] = \\text{absolute levels of cytoplasmic RNA} \\\\\n[\\text{N}] = \\text{absolute levels of nuclear RNA}\n\\end{gather}\n$$\n\nAn appropriate RNA-seq read count normalization scale factor can be\ninferred from the value of $s$. While this strategy requires having\nNR-seq in all the necessary sub- and combination-compartments, recent\nwork suggests that it may be possible to relax this requirement and\ninstead only require at least standard RNA-seq data for all such\ncompartments [@RN467].\n\n## Processing NR-seq data\n\nThe previous sections assumed that we had optimally processed NR-seq\ndata. In this section, I discuss how to obtain such data and the\nchallenges involved.\n\n### Aligning NR-seq data\n\nThe first data processing step with NR-seq-specific challenges is\naligning reads to the genome. The mutations introduced by recoding\nchemistries make this more challenging than in standard RNA-seq. These\nmismatches risk yielding inaccurate alignments or causing some reads to\nbe unmappable.\n\nSLAMDUNK, one of the first bioinformatic pipelines developed for\nprocessing NR-seq data, utilized the NextGenMap aligner to overcome this\nchallenge [@RN184; @RN156]. NextGenMap is a non-splice aware aligner\nthat provides an alternative mismatch scoring function designed for\nNR-seq data. This scoring function penalizes all but T-to-C mismatches,\nameliorating the negative impact of these mismatches on alignment\naccuracy. In addition, SLAMDUNK is optimized for 3'-end sequencing data.\nBecause of this, it is able to reduce the sequence search space by\naligning to the set of annotated 3'-UTRs rather than the entire genome.\nSLAMDUNK also implements a unique multi-mapping read assignment strategy\nthat preferentially assigns ambiguous reads to the more likely 3'-UTR.\nDespite being designed for 3'-end sequencing, some have used SLAMDUNK to\nprocess full-length NR-seq data [@RN244]. While NextGenMap's lack of\nsplice awareness makes it impossible to accurately align reads from\nthese libraries to a genome, one can instead opt for alignment to a\ntranscriptome. This has a number of downsides (e.g., improper alignment\nof intron mapping reads) but can still provide reasonable results\n[@RN245].\n\nFor full-length NR-seq data, standard splice-aware aligners like STAR\nand HISAT2 were originally preferred [@RN160; @RN221]. Using these tools\ntypically requires relaxing the default mismatch penalization. Unlike in\nNextGenMap, no such aligner allows for this to be done in a mismatch\ntype-specific manner (technically BASAL [@RN466], a recently developed\naligner, can also do this, but it cannot provide the unique MD BAM file\ntag that is required for all existing NR-seq pipelines). This leads to a\nnecessary reduction in alignment accuracy to retain high mutation\ncontent reads. To address this challenge, HISAT-3N was developed, which\naligns reads to a 3-base genome, i.e. a genome where all Ts are\nconverted to Cs [@RN179]. By similarly converting Ts in all reads to Cs,\nreads could be aligned without penalizing T-to-C mismatches. While this\nhas been useful in recovering high mutation content reads, it is not\nwithout its own unique downsides. The lower complexity genome increases\nthe likelihood that a read does not uniquely map to a given locus, and\ncan more generally reduce the accuracy of read alignment. Recently, a\nrigorous simulation benchmark was performed to compare HISAT-3N and STAR\nalignment of NR-seq data [@RN222]. It concluded that neither is strictly\nsuperior to the other, with each having unique strengths and weaknesses.\nIt remains to be seen if the same holds true for more recent 3N-aligners\nlike rmapalign3N ([Muller et al.\n2025](https://pmc.ncbi.nlm.nih.gov/articles/PMC12509878/)). As STAR is\nmore widely used and actively maintained aligner than HISAT-3N, we and\nothers typically opt for STAR when aligning full-length RNA NR-seq data.\n\nIdeally, a strategy would exist to get the best of both 4- and 3-base\nalignment. Towards that end, grandRescue was developed [@RN176].\ngrandRescue first aligns reads with STAR to a 4-base genome. Then, reads\nthat failed to align are aligned with STAR to a 3-base genome (while\nHISAT-3N makes 3-base genome alignment convenient, it is technically\npossible with any aligner. grandRescue provides the necessary helper\nfunctions to make this user friendly). This allows grandRescue to\noutperform both STAR and HISAT-3N on the task of accurately aligning\nhigh mutation content reads from full-length NR-seq data. It also showed\nimproved alignment accuracy over NextGenMap transcriptome alignment.\nWhile grandRescue has not yet reached STAR's level of bioinformatic\nmaturity, it introduced a powerful paradigm for the accurate alignment\nof NR-seq reads.\n\n### Counting mutations\n\nOnce NR-seq reads have been aligned to a genome, many of the additional\nprocessing steps are shared with standard RNA-seq analyses. Despite\nthis, a unique aspect of NR-seq data processing post-alignment is\nmutation counting.\n\nNR-seq reads from labeled RNA will *on average* contain more mutations\nof a particular type than those from unlabeled reads. Thus, quantifying\nthe number of mutations of the relevant type (e.g., T-to-C mutations in\nstandard s^4^U NR-seq experiments) is a key component of any NR-seq\npipeline. Unfortunately, there are no flexible, general-purpose\nbioinformatic tools specifically for this purpose. Instead, every NR-seq\npipeline typically reimplements a solution to this problem from scratch.\nSLAMDUNK is the closest exception, as it includes a mutation counting\nmodule that in theory could be slotted into other NR-seq pipelines.\nDespite this, SLAMDUNK's mutation counting module makes some rigid\nassumptions (e.g., single-end, forward stranded libraries) that make it\ndifficult to use in many situations. In addition, SLAMDUNK does not\ncurrently provide read-specific mutation counts and instead provides\nfeature-wide summarization of this information. This makes it\nincompatible with optimal downstream analyses (namely mixture modeling;\nsee the earlier section on \"Modeling the mutation content of sequencing\nreads\"). Finally, SLAMDUNK can only provide information about T-to-C\nmutational content, making it incompatible with s^6^G NR-seq datasets\n[@RN36; @RN92].\n\nfastq2EZbakR and GRAND-SLAM lack the modularity of SLAMDUNK, but are\ncompatible with a wider array of NR-seq library types. Both can quantify\nT-to-C and G-to-A mutations. fastq2EZbakR is additionally able to count\nany combination of all mutation types. This allows these tools to\nsupport s^6^G-based NR-seq assays, and in the case of fastq2EZbakR, even\npotentially support the wider array of mutation-based chemical probing\nexperiments.\n\nA general feature of most NR-seq processing tools is the ability to mask\nsingle nucleotide polymorphisms (SNPs) when counting mutations. Despite\nits ubiquity, the relative efficacy of different mutational artifact\ncalling strategies in NR-seq data is understudied. Several approaches\nhave been suggested, including a simple cutoff of the fraction of times\na given nucleotide is identified as having a mismatch with the\nreference, or the use of established variant calling software like\nVarScan [@RN461] or BCFtools ([Danecek et al.\n2021](https://academic.oup.com/gigascience/article/10/2/giab008/6137722)).\nAlso, while SNP calling has been shown to significantly impact analyses\nrelying on simple mutation-content cutoffs for quantifying labeled read\nabundances, mixture modeling may be more robust to high background\nmutation rates and uncalled SNPs. Despite this, systematic, non-random\nmutations may violate the assumptions of independence underlying popular\nmixture modeling strategies, especially in lower complexity libraries\nlike those from 3'-end sequencing, highlighting the potential importance\nof SNP-making and its continued optimization.\n\n### Feature assignment\n\nNR-seq reads often must be assigned to the annotated features (e.g.,\ngenes) to which they belong. Read-by-read feature assignment can then be\ncombined with mismatch calls to estimate the feature's NTR and metabolic\nkinetic parameters. While most NR-seq pipelines provide a limited set of\nfeatures to which this assignment can be done, fastq2EZbakR recently\nsignificantly expanded the flexibility of NR-seq feature assignment.\nfastq2EZbakR can assign reads to genes (exons and introns), exclusively\nexonic regions of genes, exonic bins, transcript equivalence classes,\nand exon-exon junctions. While some analyses of NR-seq only require\nstandard gene-level quantification, fastq2EZbakR's expanded set of\nfeature assignment strategies allows for a higher resolution dissection\nof RNA dynamics.\n\n### Processed data format\n\nWhat is the best way to store and format processed NR-seq data? For most\nanalyses of vanilla RNA-seq data, the answer to this question is a count\nmatrix [@RN224]. For each feature analyzed and each sample collected, a\ncount matrix notes the number of times a read came from that feature in\nthat sample or tracks some more sophisticated measure of that feature's\nabundance. Count matrices are the ideal processed RNA-seq data format\nfor several reasons. Their size largely scales with the number of\nsamples, not the depth of sequencing. This is because the number of\nannotated features in a given organism (i.e., the number of rows of the\ncount matrix) is mostly fixed, and no matter how many reads you have in\na given sample, each feature will end up with a single number for each\nsample. This format also lends itself to highly efficient linear algebra\noperations and is widely used in a number of bioinformatic libraries\n[@RN463; @RN462; @RN465; @RN464]. Thus, one could argue that a similar\nprocessed data format should be used for NR-seq.\n\nTowards that end, several popular NR-seq tools provide processed data to\nusers in a form analogous to a count matrix. Both SLAMDUNK and\nGRAND-SLAM provide a table with a fixed set of data points for each\ngenes. SLAMDUNK provides users with information about the number of\nreads with conversions from a given feature, and GRAND-SLAM provides an\nNTR estimate for each feature from mixture modeling. The former is\nsuboptimal for reasons discussed in the section on \"Analyzing NR-seq\ndata\" but the latter represents the gold standard in NR-seq analysis\noutput. Thus, NR-seq tools should at least be able to provide NTR\nestimates and read counts for each feature in each sample collected.\n\nDespite this, I argue that there are a number of important limitations\nto providing users with this form of processed data. In my experience,\nwhen things go wrong with an NR-seq analysis, the problems start at the\nmixture modeling step. Thus, by only providing users mixture model\noutput, and thus limited strategies by which to interrogate model fit\nand identify potential model biases, this form of processed data risks\nobscuring issues in an NR-seq dataset. In addition, only providing NTR\nestimates limits innovation in NR-seq analyses. If a user wants to\ndevelop their own NTR estimation strategy, they must also develop their\nown pipeline to generate data in a form compatible with their modeling\nstrategy.\n\nFor this reason, fastq2EZbakR (and bam2bakR before it) provides user\nwith the processed data necessary to fit state-of-the-art models of\nNR-seq data. For each feature and sample, fastq2EZbakR reports the\nnumber of reads with a given number of mismatches of the relevant type\n(e.g., T-to-C in a single label, s^4^U-fed, NR-seq experiment) and a\ngiven number of mutable nucleotides (e.g., T's in the reference). The\nSimon lab termed this file format a \"counts binomial\", or cB, file. The\nrecently developed Halfpipe provides similar data though in a different\nformat. With this information, users can either make use of mixture\nmodel fitting strategies implemented in tools like EZbakR or develop\ntheir own unique analysis strategy.\n\nProcessed NR-seq data formats like cB files significantly democratizes\nthe analysis of NR-seq data. Despite this, it is not without its\nlimitations. Unlike with GRAND-SLAM's output, the size of a cB file\nscales with the sequencing depth. While the scaling is sub-linear, as\ndata can be compressed by tracking the number of reads with identical\nmutation and mutable nucleotide content, the output is still often far\nmore unwieldy than GRAND-SLAM's simple table. To address this\nlimitation, fastq2EZbakR implements an alternative output type that\ninvolves creating separate files for separate samples. While the size of\nthese individual files will still scale with the sequencing depth,\nsplitting the files in this manner supports analysis strategies that\neffectively load only a single sample into RAM at a time. EZbakR\nimplements such an analysis strategy, optimized with the help of the\n[Apache Arrow project](https://arrow.apache.org/) and its R frontend,\nwhich has significantly increased the scalability of its analyses\n[@RN94].\n\nUnique decisions have to be made regarding how to format and distribute\nNR-seq data. While count matrix-like formats are familiar, scalable, and\nintuitive, they may significantly limit the types of analyses users can\nperform. Alternative data formats can address these issues. While more\ncompressed than read-level formats (e.g., BAM files), these alternatives\nstill have to sacrifice RAM scalability. We suggest that all NR-seq\npipelines should produce output of this latter type, while optionally\nproviding simpler, count matrix-like output for those looking to avoid\nthe extra hassles.\n\n## Optimal NR-seq experimental design\n\nNo matter how optimal your data processing and analysis pipeline is, bad\ndata can fundamentally limit obtainable insights. In this section, I\ndiscuss how to best design an NR-seq experiment to avoid these problems.\n\n### Choosing the population of RNA to sequence\n\nIn theory, NR-seq can be applied to a wide array of library preparation\nstrategies to probe the dynamics of your choice of RNA. SLAM-seq\noriginally made use of 3'-end sequencing, and a commercially available\nkit for this library preparation strategy was developed shortly after\nits initial publication. Combining NR-seq with 3'-end sequencing has\nseveral advantages. For one, it limits the amount of pre-mRNA in your\nlibrary. Pre-mRNA are typically rapidly turned over and can thus lead to\nslight overestimation in turnover kinetics if not accounted for in\ndownstream analyses. In addition, 3'-end sequencing allows for some\nlevel of isoform deconvolution, allowing analyses to distinguish the\nkinetics of alternative 3'-UTR isoforms \\[RN240\\]. This is useful as\n3'-UTRs are well established hubs of post-transcriptional regulation\n[@RN243; @RN242; @RN241].\n\nAlternatively, NR-seq can be combined with any standard full-length\nRNA-seq prep. TimeLapse-seq originally made use of this strategy, and it\nhas been used with other recoding chemistries as well [@RN88; @RN151;\n@RN177]. While lacking some of the unique advantages of 3'-end NR-seq,\nfull-length NR-seq provides more information. For one, the dynamics of\npre-mRNA and mature mRNA can be assessed in the same sample with\nfull-length RNA data [@RN180]. In addition, I developed strategies using\nfull-length RNA NR-seq data to infer the turnover and synthesis kinetics\nof individual transcript isoforms [@RN94]. I worked with Bobby Hogg's\nlab at the NIH to apply this approach to the study of NMD-sensitive\nisoforms, many of which are not the result of alternative 3'-UTR usage\nand thus invisible to 3'-end NR-seq [@RN246].\n\nDespite the common use cases, the power of NR-seq lies in its ability to\nbe combined with myriad library preparation strategies. For example,\nNR-seq has been combined with small RNA sequencing to probe the dynamics\nof miRNA. It has also been combined with a number of unique library\npreparation strategies to assess the dynamics of many different\nbiological processes (see section on extensions at the end) [@RN170].\nThus, users have copious flexibility when deciding what RNA they want to\nsequence in an NR-seq experiment.\n\n### Labeling and sequencing depth\n\nWhen designing and NR-seq experiment, there are 3 major\nconsiderations: 1) how deeply to sequence, 2) what concentration of\ns^4^U to use, and 3) how long to label with s^4^U.\n\nThe answer to the first is somewhat trivial: as deep as you can\ncomfortably afford. Sequencing coverage has a significant impact on the\naccuracy of NR-seq analyses, and thus maximizing coverage is beneficial\n(@fig-6). That being said, high quality, reproducible estimates of RNA\nhalf-lives have been obtained from a wide range of sequencing depths. If\nyou are looking to analyze specific feature classes though, like splice\njunctions, exonic bins, or transcript isoforms, depth can be an even\nmore important consideration [@RN94]. On the other hand, if performing\n3'-end NR-seq, equally confident estimates can be obtained with less\ndepth due to the lower complexity of these libraries.\n\nOptimization of s^4^U labeling conditions is a more subtle challenge.\nFirst, it is important to assess the extent to which s^4^U is getting\nincorporated into the system that you are studying. This can be done\nthrough low-throughput assays such as TAMRA dot blots, or small scale\nsequencing experiments [@RN178]. We have found there to be a significant\namount of system-to-system variability in s^4^U incorporation rates. A\nstandard s^4^U concentration of 100 uM has proven effective in a wide\narray of settings. Despite this, some cell liens may take up s^4^U far\nmore readily, to the point that it is best to significantly decrease\ns^4^U concentrations to avoid cytotoxic effects [@RN127]. Others may\nrequire much higher concentrations to get appreciable incorporation\nrates. It is thus important to assess the s^4^U incorporation tendencies\nin any new system in which you have not previously performed NR-seq.\n\nThe length of time to label with s^4^U is the final major consideration\nfor an NR-seq experiment. Statistical arguments have been made in favor\nof a label time around the median half-life of the RNAs you are\ninterested in studying [@RN157]. For human mRNA, this is around 4 hours\n[@RN120; @RN460]. This argument reflects the fact that the label time\nestablishes the dynamic range of your half-life estimation. RNAs with\nhalf-lives much longer than your label time will have nearly\nundetectable levels of labeling, and those with half-lives much shorter\nthan your label time will be almost completely labeled. A naive estimate\nof the dynamic range can be made by noting that in the most extreme\ncase, you can claim that you have a single labeled or unlabeled read,\nmaking the range of meaningful half-life estimates:\n\n$$\n\\begin{gather}\n\\text{Minimum} \\text{ t}_{1/2} \\text{ estimate} = \\frac{\\text{ln}(2)\\cdot \\text{tl}}{-\\text{ln}(1 - \\frac{\\text{N} - 1}{\\text{N}})} \\\\\n\\text{Maximum} \\text{ t}_{1/2} \\text{ estimate} = \\frac{\\text{ln}(2)\\cdot \\text{tl}}{-\\text{ln}(1 - \\frac{\\text{1} }{\\text{N}})} \\\\\n\\text{N} = \\text{number of reads}\n\\end{gather} \n$$\n\nIn practice, uncertainties in the half-life estimate get larger the\nfurther the true half-life is from the label time (@fig-6).\n\n![**Estimate accuracy as a function of NTR.** EZbakR’s SimulateOneRep()\nfunction was used to simulate data for 10,000 features, all of which\nwere given 100, 1000, or 10000 reads. Features had a range of NTR\nvalues, evenly spaced from 0.01 to 0.99. NTRs were estimated with\nEZbakR. Top: comparison of EZbakR NTR estimates to the simulated truth\n(both on a logit-scale). Bottom: The logit(NTR) uncertainty as a\nfunction of NTR. The uncertainty is computed from the Hessian and can\nthus be thought of as how steeply the likelihood falls off around the\nmaximum likelihood value. Larger values represent slow decreases in\nlikelihood and high uncertainty (lots of logit(NTR) estimates explain\nthe data reasonably well).](Uncertainties.png){#fig-6}\n\nDespite these theoretical arguments, shorter label times are usually\npreferable to longer ones. Long-term exposure to s^4^U can have\ncytotoxic effects, so minimizing this risk is advisable [@RN127;\n@RN128]. In addition, dropout may be more likely the higher proportion\nof your RNA that is labeled. Because of this, we suggest a slight\ndeviation from theoretical statistical optimality and propose that an\noptimal label time close to, but shorter than, the median half-life of\nthe RNA that you are interested in probing.\n\nFinally, the best-case scenario is to have multiple different label\ntimes (e.g., 1 hour, 2 hour, 4 hours) in each biological condition.\nHaving multiple label times expands the dynamic range of your half-life\nestimation [@RN157; @RN193]. In addition, grandR implements a strategy\nthat uses multiple label times to correct for biases that can arise due\nto gradual ramp up in s^4^U availability over time [@RN193]. It assumes\nthat the longest label time is closest to be the ground-truth\n\"effective\" label time (this is because the s^4^U ramp-up time is the\nsmallest percentage of that label time), and identifies adjusted label\ntimes for the other time points that yield, on average, estimates in\nagreement with the longest label time. More generally, having multiple\nlabel times is useful for assessing the extent to which models assumed in\nyour NR-seq analysis accurately describe the dynamics of RNAs assayed.\n\n### Pulse-label vs. pulse-chase\n\nThere are two ways to perform the labeling in an NR-seq experiment: a\npulse-label and a pulse-chase (@fig-7). Pulse-labeling refers to\ntreating cells with metabolic label for a certain period of time, after\nwhich RNA is extracted, treated with nucleotide recoding chemistry, and\nsequenced. In a pulse-chase, labeling is typically done for a much\nlonger period of time, followed by a chase with the unmodified\nnucleotide (e.g., uridine). RNA is extracted after the chase.\n\nWhile technically equivalent in terms of the information provided, a\npulse-label design provides a number of advantages over a pulse-chase\ndesign. Pulse-chases necessitate extended exposure of cells to s^4^U.\nThis can have serious cytotoxic effects that risk biasing your analyses.\nIn addition, the metabolic label can be recycled from degraded RNA\nduring the chase and returned to the NTP pool, complicating analysis of\nthis data (this compounds the orthogonal problem of imperfect washout of\nmetabolic label during the chase) [@RN141; @RN143]. Finally, estimating\nkinetic parameters in a pulse-chase requires estimating the NTR after\nthe pulse and the chase, and comparing these NTR values. For stable RNA,\nthis can yield ambiguous estimates when the estimated NTR after the\nchase is lower than that after the pulse (e.g., due to experiment noise,\nestimation uncertainty, or experimental biases). In contrast,\npulse-label designs can limit a cell's exposure to s^4^U, aren't\nconfounded by label recycling, and can provide estimates of RNA turnover\nkinetics from NTR estimates in a single sample. Because of this, we\nstrongly suggest preferring a pulse-label to a pulse-chase design. This\nsuggestion is further backed up by a recent meta-analysis I performed of\npublished NR-seq datasets, where I found that estimates from pulse-label\nNR-seq experiments were far more consistent across cell lines, labs,\nmethod variations, etc. than those from pulse-chases [(Vock et al.\n2025)](https://www.biorxiv.org/content/10.1101/2025.08.19.671151v1.full).\n\n![**Two NR-seq labeling strategies.** Pulse-labeling refers to labeling\ncells for a certain amount of time, after which RNA is extracted. A\npulse-chase refers to labeling cells for (typically) a much longer\nperiod of time, chasing with the regulate nucleotide (e.g., uridine),\nand extracting RNA after the chase. These two designs are symmetric (the\nunlabeled RNA in one behaves like the labeled RNA in another). Despite\nthis, pulse-labels provide a number of technical advantages to a\npulse-chase, and thus should be the default NR-seq labeling strategy\n(see text for discussion).](Labeling.png){#fig-7}\n\n## NR-seq extensions\n\nNucleotide recoding has not only been used to augment bulk RNA-seq. A\nvariety of other sequencing methods have been combined with metabolic\nlabeling and nucleotide recoding. These approaches are opening up\nexciting avenues for studying various aspects of RNA biology. In this\nsection, I briefly review the universe of current NR-seq extensions.\n\n### TT-NR-seq\n\nOriginally, s^4^U metabolic labeling was combined with biochemical\nenrichment of labeled RNA [@RN117; @RN116; @RN468]. While the\nlimitations of this approach inspired the development of nucleotide\nrecoding approaches, nucleotide recoding also provides a unique route by\nwhich to improve these methods. This is exciting as enrichment-based\nmethods are still useful for studying rapid processes, such as\nco-transcriptional splicing and cleavage and polyadenylation [@RN196;\n@RN248]. A major challenge in analyzing these enrichment-based metabolic\nlabeling data though is the inevitable presence of unlabeled RNA\ncontamination in the enriched RNA population. The magnitude of biases\nintroduced by this contamination is greater in settings where very short\nlabel times are required, precisely the settings in which\nenrichment-based strategies are still needed. Applying nucleotide\nrecoding to this data has the potential to allow users to\nbioinformatically filter out unlabeled RNA contamination [@RN88;\n@RN170].\n\nIn theory, standard mixture modeling can be applied to TT-NR-seq data so\nas to estimate the amount of reads from a given feature that come from\nunlabeled RNA contamination. In practice, the extent to which these\nmodels are fully appropriate in this setting is underexplored. Concerns\nabout overdispersion of mutation rate distributions are particularly\nvalid in settings with very short label times, as s^4^U concentrations\nare likely actively increasing during the labeling. Thus, regions of an\nRNA produced early in the labeling are likely exposed to lower\nconcentrations than those produced later in the labeling. This means\nthat instead of there being a single new read mutation rate, there is a\npotentially broad distribution of mutation rates. Strategies to model\nthis distribution could both address this problem and provide\ninteresting orthogonal information about the timing and kinetics of\nvarious processes.\n\n### STL-seq\n\nIn metazoans, transcription initiation at protein coding genes is often\nfollowed by promoter-proximal pausing, where RNA Pol II halts\ntranscription 20-60 nts downstream of the TSS and awaits further signal\nto continue into productive elongation or prematurely terminate [@RN226;\n@RN227; @RN225]. To probe the steady-state levels of paused polymerase,\nStart-seq was developed [@RN228]. This approach involves sequencing\nshort, capped RNAs housed within the paused polymerase. To assess the\nkinetics of pause departure and initiation, the Simon lab combined this\napproach with nucleotide recoding, in a method termed\nStart-TimeLapse-seq, or STL-seq [@RN169].\n\nSTL-seq has provided unique insights regarding the kinetics of\npromoter-proximal pausing. There are number of analysis challenges and\nlimitations to consider when analyzing STL-seq data though. For one,\nunlike vanilla NR-seq's deconvolution of synthesis and degradation\nkinetics, STL-seq does not completely solve the problem of kinetic\nambiguities plaguing steady-state analyses of pausing. The paused\npolymerase has two distinct potential fates: release into productive\nelongation and premature termination (Kamieniarz-Gdula and Proudfoot\n2019). Thus, the pause site departure kinetics estimated by STL-seq\nrepresent a combination of the kinetics of these two processes. Fully\nresolving the kinetics of pausing requires combining STL-seq with\ninhibition of promoter-proximal pause release via drugs like\nflavopiridol. This has the downside of being a harsh perturbation that\ncan affect the kinetics of the processes being studied, the very same\ndownside that partially inspired the development of metabolic labeling\nmethods in the first place.\n\nSTL-seq also presents some unique analysis challenges. For one, STL-seq\nreads are fairly short (\\< 80 nucleotides, as short as 20). Aligning\nthese short reads is made more challenging by the chemically induced\nT-to-C mismatches. 3-base alignment was originally shown to\nsignificantly improve the recovery of high mutation content reads\n[@RN169]. Despite this, rigorous benchmarks of different alignment\nstrategies for STL-seq data have not yet been developed. Drawing\ninspiration from SLAMDUNK's 3'-UTR alignment strategy, we suspect that a\nstrategy using no-label data to infer TSS sequences and aligning s^4^U\nlabeled data to this constrained sequence space may provide some\nadvantages. In addition, using NextGenMap and its custom T-to-C mismatch\npenalization function may provide further advantages.\n\nIn addition, mixture modeling of STL-seq data may suffer from the same\nchallenges faced by mixture modeling of TT-NR-seq data, due to the\nnecessarily short label times (\\~5 minutes). The discussion in that\nsection about the potential of modeling the distribution of new read\nmutation rates hold here as well. In summary, further innovation of the\nprocessing and analysis of STL-seq data may provide significant\ndividends.\n\n### Dual-labeling and TILAC\n\nWhile NR-seq experiments typically make use of s^4^U, s^6^G has also\nshown to be compatible with these methods [@RN36; @RN92]. This has\nopened the door for dual-label designs. While EZbakR and GRAND-SLAM\nimplement strategies to fit mixture models to this general class of\nmethods, the potential use cases of dual-labeling are currently\nunderexplored.\n\nTo my knowledge, the only existing specialized dual-labeling method\ncurrently published is TILAC [@RN167]. TILAC draws inspiration from the\nproteomics methods SILAC [@RN231] to make use of dual-labeling for\nrigorous normalization of complex sequencing experiments. TILAC involves\nmixing s^4^U labeled cells with s^6^G labeled cells. The idea is that\nthese two populations represent distinct biological conditions that you\nare comparing gene expression levels in. The relative abundances of\nreads from s^4^U and s^6^G labeled RNA from a given gene in this\nexperiment provides information about the relative expression of that\ngene in the two conditions. This allows for rigorous normalization\nwithout the need for spike-ins or statistical assumptions.\n\nTILAC is an exciting paradigm for the normalization of complex\nsequencing experiments. Despite this, it and other potential dual\nlabeling approaches are not without their unique challenges. s6G is more\ncytotoxic than s^4^U, raising the concern of adverse effects due to\nlabeling [@RN232; @RN233]. In addition, s^6^G may not be readily\nincorporated into the NTP pool to the same extent that s^4^U is.\nFinally, for true dual labeling approaches (i.e., labeling the same\npopulation of RNA with both s^4^U and s^6^G), we have found that\nincorporation of these two labels may interfere with one another. This\ncombined with the unique bioinformatic challenge of aligning reads with\nhigh levels of both T-to-C and G-to-A conversions makes generating high\nquality, dual labeled NR-seq data challenging. Despite this, we suspect\nthat strategies to address these challenges could open novel and\nexciting routes for finer-grained dissection of RNA kinetics (e.g.,\ninvestigation of RNA aging dynamics).\n\n### NascentRibo-seq\n\nApproaches like Ribo-seq are commonly used to infer the efficiency of\ntranslation initiation [@RN230]. Despite this, these analyses are\nfundamentally plagued by kinetic ambiguities that require strong\nassumptions to interpret in this way. Thus, a method termed Nascent\nRibo-seq was developed, which combines nucleotide recoding with Ribo-seq\n[@RN166]. This approach allowed the authors to assess the kinetics of\npolysome assembly. This approach proved particularly useful in settings\nwhere cells are actively responding to a perturbation, as the active\nregulation of mRNA levels would bias standard Ribo-seq analyses of\nribosome loading efficiency.\n\n### scNR-seq\n\nOne of the most active areas of NR-seq extensions is its application to\nsingle-cell RNA-seq. A number of distinct approaches have been developed\nthat combine scRNA-seq approaches with s^4^U metabolic labeling and\nnucleotide recoding [@RN161; @RN164; @RN165; @RN163; @RN174; @RN171].\nThis includes work to further optimize the scalability,\nease-of-implementation, and conversion efficiency of these protocols\n[@RN187].\n\nIn addition, scNR-seq has seen a number of exciting bioinformatic\ndevelopments. Work from Jonathan Weissman's showed that metabolic\nlabeling scRNA-seq approaches have the potential to significantly\nimprove analyses of RNA velocity [@RN172]. This paper also introduced a\npipeline for processing scNR-seq data, dynast (GRAND-SLAM is the only\nother existing tool that provides support for single-cell specific\nprocessing tasks). This has spurred several groups to improve upon their\ninitial metabolic labeling-based velocity analyses [@RN171; @RN188].\nThis route is particularly promising, as it theoretically eliminates the\nneed to rely on low and biased coverage for intronic regions of genes in\nmost scRNA-seq libraries.\n\nscNR-seq is also opening up brand new classes of analyses. Recently,\nFlorian Erhard’s group developed a combined experimental and\nbioinformatic approach making use of scNR-seq to infer cell\ntype-specific responses to perturbations [@RN189]. The idea is to feed\ncells the metabolic label at the same time that they are treated with a\ndrug or perturbed in some other way. By inferring the degradation\nkinetics of RNA in a given cell (or group of similar cells) after the\nperturbation, the starting levels of each RNA (i.e., before application\nof the perturbation) can be inferred. This allowed them to identify how\neach cell type present in the starting population uniquely responded to\nthe treatment. Using a non-linear causal inference strategy, they were\nalso able to identify gene’s whose regulation were driving the larger\nchanges seen [@RN234; @RN189].\n\nThis approach, implemented in the new R package HetSeq, showcases how\ninnovative experimental design and data analysis can make use of\nnucleotide recoding to unlock new avenues for biological discovery.\nDespite the theoretical promise of this approach, inferring the starting\nlevels of RNA in these experiments is incredibly challenging. The\nsteady-state assumption is almost inevitably violated. While we have\ndiscussed approaches to achieve unbiased parameter estimation in\nnon-steady-state contexts, HetSeq requires an experimental design\nfundamentally incompatible these strategies (HetSeq estimates the\nquantity that needs to be measured for non-steady-state analyses, the\ninitial RNA levels) [@RN146]. In addition, single cell data is notably\nsparse, making accurate estimation of cell type specific NTRs (and thus\ndegradation rate constants) difficult. While GRAND-SLAM and HetSeq\nimplement a number of strategies to mitigate these challenges, further\ndenoising of the analysis may be achievable through collection of\nmatched, unperturbed scRNA-seq data. This data could be used to generate\nan atlas of initial cell states that the noisy inferred cell states are\nmapped onto ([Lotfollahi et al.\n2022](https://www.nature.com/articles/s41587-021-01001-7); [Lotfollahi\net al. 2024](https://pubmed.ncbi.nlm.nih.gov/38729109/)).\n\n### PerturbSci-Kinetics\n\nMany biologists using NR-seq methods are not necessarily interested in\nhow the kinetics of RNA synthesis and degradation change when cells are\nperturbed. Approaches like Perturb-seq have provided an unprecedented,\nhigh-throughput look at the impact of genetic perturbations on gene\nexpression [@RN237]. To dissect how regulation of transcription and RNA\ndecay underlies these changes, Perturb-seq was recently combined with\nnucleotide recoding, in an approach termed PerturbSci-Kinetics [@RN186].\nWhile the method and its bioinformatic toolkit is in its nascence, it\nrepresents a paradigm shift in the scale at which we can investigate the\nmechanisms of gene expression regulation.\n\n### Tether-seq\n\nThe Simon lab recently developed an approach that combines a\nTT-NR-seq-style experiment with single-nucleotide bioinformatic analyses\nto identify sites of small molecule binding across the transcriptome\n[@RN178]. This approach, termed Tether-seq, relies on treating cells\nwith small molecule drugs containing a disulfide moiety, and using this\nhandle to covalently bind the small molecule to RNAs with which it\nassociates. Nucleotide recoding is then used to identify the exact sites\nto which the small molecule tethered. This approach facilitates the\nidentification of weak binders, while also establishing an exciting\nparadigm for single-nucleotide NR-seq analyses.\n\n### Long read NR-seq\n\nCombining nucleotide recoding with long read sequencing technologies\npresents a promising avenue by which to dissect the isoform-specific\nkinetics of gene expression. While it is possible to glean insights\nabout the synthesis and degradation kinetics of individual transcript\nisoforms from short read NR-seq [@RN94], long read technologies can open\nup investigation of isoform-specific dynamics largely invisible to short\nread data (e.g., coupling of TSS and TES usage) [@RN238]. There is\ncurrently only one described attempt at long read NR-seq, in which TUC\nchemistry has been combined with PacBio long read sequencing [@RN154].\n\nDespite this, some long read sequencing technologies provide avenues by\nwhich to perform enrichment-free metabolic labeling analyses without\nnucleotide recoding chemistry. In particular, nanopore direct RNA\nsequencing has been used by several groups to directly distinguish\nlabeled from unlabeled RNA [@RN191; @RN190; @RN249]. These approaches\nhave utilized a range of machine learning architectures, often relying\non convolutional neural networks, to achieve this end. While\ndistinguishing labels like s^4^U and 5-EU in nanopore current signal is\nchallenging, these approaches are promising avenues towards high\nfidelity dissection of isoform-specific kinetics. This approach was even\nrecently combined with subcellular fractionation to probe the kinetics\nof more than just bulk RNA synthesis and degradation [@RN182]. Thus,\nlong read NR-seq and direct RNA sequencing of metabolically labeled\nsamples will likely be a powerful tool in dissecting the dynamics of\ntranscript isoforms.\n\n## Conclusions\n\nNR-seq represents a powerful paradigm shift in the study of RNA\ndynamics. Making full effective use of these methods requires rigorous\nanalysis strategies robust to technical and biological variance as well\nas biases that can plague these experiments. While a number of tools\nhave been developed to implement these strategies, new NR-seq extensions\noften demand unique solutions to these problems. Thus, it is important\nthat users and developers of NR-seq technologies are aware of the dos\nand don’ts of NR-seq data processing and analysis.\n\nIn this review, I have laid out the tenets of a solid NR-seq analysis\nfoundation. This includes rigorous modeling of the mutational data in\nNR-seq reads, awareness regarding biases such as dropout that can plague\nthese data, accurate kinetic modeling of the labeled and unlabeled RNA\nabundances, proper alignment of conversion containing reads, and\nexperimental design decisions intended to make all of the above easier.\nI urge NR-seq users and developers to follow the roadmap laid out here\nso as to avoid misusing and misinterpreting their data.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}