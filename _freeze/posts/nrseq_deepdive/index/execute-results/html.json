{
  "hash": "b7ed5bcf2a3628019dd8bbed132c5672",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"NR-seq: an in-depth review\"\nauthor: \"Isaac Vock\"\ndate: \"2025-11-18\"\ncategories: [nrseq]\nformat: \n  html:\n    toc: true\nengine: knitr\neditor: \n  markdown: \n    wrap: 72\nimage: \"NRseq.png\"\n---\n\n## NR-seq: a review\n\nThis is a reproduction of Chapter 1 of [my\nthesis](https://github.com/isaacvock/Thesis), an in-depth technical\nreview of NR-seq data. This version is currently incomplete (figures and\ncitations aren't yet included). It is intended to be a living document\nthat evolves as the field evolves, and will thus be occasionally updated\n(edit time scale: \\~ monthly).\n\n## Abstract\n\nThe kinetics of gene expression are largely invisible to methods that\nonly probe steady-state RNA abundances. Nucleotide recoding RNA-seq\nmethods (TimeLapse-seq, SLAM-seq, TUC-seq, etc.) were developed to\novercome this limitation. These methods combine metabolic labeling with\nunique chemistries to track the dynamics of labeled and unlabeled RNA\nand resolve the kinetic ambiguities of vanilla RNA-seq. Despite their\npromise, analyzing NR-seq data presents several unique bioinformatic\nchallenges. While software packages exist that implement gold-standard\nanalysis strategies, misconceptions about how to properly analyze and\ninterpret NR-seq data persist. In some cases, this has led to the\nwidespread adoption of potentially flawed analysis paradigms. To address\nthis, I present a detailed overview of NR-seq analyses. I cover best\npractices, current software implementations, and optimal experimental\ndesign. I also discuss the landscape of NR-seq extensions, as these\nrepresent exciting areas with unique bioinformatic challenges. I hope\nthat this will be a useful resource to the community of NR-seq users and\ndevelopers.\n\n## Introduction\n\nFrom birth to death, an RNA's life cycle is tightly regulated. A key\naspect of this regulation is fine-tuning of the rate at whcih each stage\nprogresses, from transcription initiation to RNA degradation. Developing\na mechanistic understanding of gene expression regulation requires\nmethods to probe the kinetics of the RNA life cycle (transcription,\nprocessing, export, degradation, etc.).\n\nWhile standard RNA-seq begins to solve this problem, it provides limited\ninformation about the kinetics of the processes which determine an RNA's\nabundance. Nucleotide recoding RNA-seq (NR-seq; TimeLapse-seq, SLAM-seq,\nTUC-seq, etc.) overcomes these limitations. NR-seq relies on metabolic\nlabeling, which involves feeding cells a nucleotide analog that gets\nincorporated into RNA synthesized after the start of labeling. The rate\nat which old, unlabeled RNA degrades and new, labeled RNA accumulates\nprovides information about the kinetics of RNA metabolism (Figure 1). To\ndetermine how much of a given population of RNA is labeled, NR-seq\nrelies on novel chemistries that recode the hydrogen bond pattern of the\nmetabolic label to facilitate detection of labeled RNA via chemically\ninduced mutations in sequencing reads (Figure 1). NR-seq is thus a\npowerful method for resolving the kinetic ambiguities of standard\nRNA-seq.\n\n![**Figure 1:** Overview of a standard NR-seq analysis, adapted from (Vock and Simon 2023). NR-seq typically involves labeling cells with s4U and chemically recoding s4U to a cytosine. The data provided by this method is counts of sequencing reads containing a certain number of mutations and a certain number of mutable nucleotides. This data can be used to infer the abundance of labeled and unlabeled RNA, which can be modeled as a function of kinetic parameters (e.g., synthesis and degradation rate constants). An important quantity in this task is the fraction of reads from new RNA, or new-to-total ratio (NTR; denoted θ throughout this chapter) whose estimation is a key part of any NR-seq analysis.](NRseq.png)\n\n\nUnlocking the full potential of NR-seq data requires rigorous and\nwell-founded analysis strategies. While such strategies have been put\nforth and implemented in a number of bioinformatic tools,\nmisunderstanding regarding how to best interpret and analyze NR-seq data\nare common. Therefore, it is crucial to create gold-standard analysis\nguidelines for users and developers of NR-seq methods.\n\nTowards that end, I present a comprehensive overview of the analyses of\nNR-seq data. I will provide a combination of accessible big-picture\nsummaries of my main points, as well as rigorous mathematical formalism\nto back up key assertions. I also will conclude with a brief overview of\nexisting extensions of the original NR-seq methodology, their\napplications, and unique aspects of their analyses. Throughout, I will\npoint readers to bioinformatic tools implementing gold-standard analysis\nstrategies while also highlighting fundamental challenges posed by\nNR-seq analyses. I hope that this serves as a useful resource for the\nlarger community of NR-seq users and promotes best analysis practices in\nthis exciting and growing field.\n\n## Summary\n\nBelow I will briefly summarize the main takeaways that are expanded upon\nthroughout the remaining of this post. There are four unique challenges\nwhen analyzing NR-seq data: 1) estimating how many reads come from\nlabeled RNA; 2) inferring kinetic parameters from 1); 3) aligning and\nprocessing NR-seq reads; 4) optimizing the experimental design for\nachieving 1) and 2). I thus divide my advice into that pertaining to\neach of these points:\n\n**Inferring the number of reads that come from labeled RNA:**\n\n-   Mixture modeling is the most robust and accurate strategy by which\n    to assess the fraction of reads from a given mutational population.\n    This strategy is implemented in tools such as GRAND-SLAM, bakR,\n    Halfpipe, and EZbakR.\n-   Mutation content cutoffs (e.g., classifying reads as \"labeled\" if\n    they have a certain nubmer of T-to-C mutations) can yield highly\n    biased estimates of labeled RNA abundance.\n-   Mixture models necessarily make assumptions about the distribution\n    of mutations in reads from new and old RNA. It is important to\n    assess these assumptions when interpreting mixture model fits.\n-   Extending the simplest two-component mixture model is promising but\n    potentially fraught. Carefully validate improvements in model fit\n    and stability of parameter estimates. Consider using regularization\n    strategies to avoid unrealistic parameter estimates.\n\n**Inferring kinetic parameters:**\n\n-   Unless using a label time significantly shorter than the average\n    half-life of RNAs of interest (for example, this is around 4 hours\n    for mRNAs in mice an dhumans), the number of reads from labeled RNA\n    is reflective of both transcription and turnover kinetics, not just\n    the former.\n-   The relative abundances of reads from labeled and unlabeled RNA is\n    best modeled as a function of both transcription and degradation\n    rate constants. The functional form of this relationship depends on\n    your model for the dynamics of the RNA species you are probing.\n-   Assuming that the probed RNA populations are at steady-state during\n    labeling simplifies the task of kinetic parameter estimation. This\n    assumption can break down when labeling is done during or following\n    a perturbation. Analysis strategies exist which relax this\n    assumption, but they rely on having measurements of RNA abundances\n    at the start of labeling (i.e., no-label RNA-seq data from that time\n    point or a labeled NR-seq data in which RNA was extracted at that\n    point). grandR and EZbakR implement both of these analysis\n    strategies.\n-   Combining NR-seq with subcellular fractionation is a powerful way by\n    which to explore the kinetics of processes invisible to whole-cell\n    NR-seq. Analyzing this data requires a strategy to normalize read\n    counts and integrate information across all compartments. EZbakR\n    implements such strategies.\n\n**Processing NR-seq data:**\n\n-   Aligning NR-seq reads is difficult due to the chemically induced\n    T-to-C mismatches. While 3-base genome alignment strategies, popular\n    in analyses of bisfulite sequencing data, are a potential solution,\n    they often provide only minimal advantages over standard 4-base\n    alignment approaches while also suffering from their own unique\n    biases and limitations. This is in large part due to the fact that\n    in NR-seq reads, most Ts are not converted to Cs. Thus, the\n    downsides of aligning to a lower complexity genome may often nullify\n    the benefits of not penalizing T-to-C mismatches.\n-   Specialized alignment approaches may provide an improvement over\n    either a standard 3- or 4-base genome alignment approach.\n    grandRescue is a recently developed approach that implements 4-base\n    alignment followed by 3-base alignment of reads that fail to align\n    via the 4-base strategy. This approach can help recover high\n    mutation content sequencing reads.\n-   When processing NR-seq data, you have a choice to make about what\n    genomic features you want to assign reads to and perform analyses\n    on. Most tools support performing analyses at the gene- (or 3'-UTR\n    if using 3'-end sequencing) level, but fastq2EZbakR significantly\n    expands the set of features you can analyze to incldue transcript\n    isoforms, exon-exon junctions, etc.\n\n**Experimental design:**\n\n-   Label-free control samples (a.k.a. no-label controls) are crucial to\n    detect potential biases introduced by labeling (e.g., dorpout of\n    labeled RNA).\n-   No-label controls can be used to both assess and correct for these\n    biases in some cases. grandR and EZbakR provide strategies for this\n    task.\n-   Bias correction strategies make assumptions like any statistical\n    method, and these assumptions should be assessed when interpreting\n    the output of bias correction.\n-   While unique analysis strategies have been proposed that may be\n    strictly compatible with pulse-chases, pulse-chases suffer from the\n    following serious shortcomings:\n    -   Prolonged exposure to metabolic label, which can lead to adverse\n        effects\n    -   Increased variance in kinetic parameter estimation due to having\n        to compare the estimated fraction of reads that are labeled at\n        the end of the pulse to that at the end of the chase. Compare\n        this with a steady-state pulse-label analysis where the only\n        source of variance is that of the fraction labeled estimate for\n        the pulse.\n    -   The analysis is complicated by the potential for incomplete\n        competition of metabolic label with the chase nucleotide and\n        recycling of metabolic label from degraded RNA.\n    -   Higher cost due to the necessity of more samples (set of pulses\n        and set of chases).\n-   The best label time for standard NR-seq kinetic parameter inference\n    is around the median half-life of the RNAs you wish to probe the\n    kinetics of. It's better to undershoot than overshoot this target\n    though to avoid adverse effects of prolonged metabolic labeling.\n-   Accurate kinetic parameter estimates are obtainable with standard\n    RNA-seq sequencing depth. More depth can significantly improve these\n    analyses though. Sequencing depth is particularly important if\n    wanting ot perform analyses on the sub-gene (e.g., exon-exon\n    junction) level.\n\nAdditionally, an exciting advance in the NR-seq field is the number of\nunique extensions that have been developed to apply nucleotide recoding\nto the study of several aspects of RNA dynamics. This review concludes\nwith a summary of currently published NR-seq extensions.\n\n## A brief history of NR-seq\n\nOne of the first strategies to assess the kinetics of RNA synthesis and\ndegradation combined global transcriptional inhibition with RNA-seq. RNA\nlevels following a time coruse of inhibition could be fit to exponential\ndecay curves to assess turnover kinetics in high-throughput. Combining\nthis with pre-inhibition abundance information allows one to assess\nsynthesis kinetics. While this approach continues to be widelly used, it\nsuffers from a number of drawbacks. For one, rigorous normalization is\nneeded to track the decreasing absolute levels of RNA. As RNA-seq only\nprovides a relative measure of RNA abundance, this typically requires\nexogenous spike-ins to account for the global differences in RNA\nabundance between inhibition timepoints. This introduces additional\nexperimental complexity that requires optimization. In addition, global\ntranscription inhibition causes the cells to launch myriad stress\nresponses, many of which affect transcript stability. This represents a\nconfounder that complicates the interpretation of transcriptional\ninhibition data. Thus, a strategy to probe the kinetics of RNA without\nsignificant perturbation of the system was needed.\n\nMetabolic labeling with nucleotide analogs offers one such strategy.\nCells will incorporate these labels into nascent RNA, leading to the\nexistence of two distinct populations of RNA: unlabeled, old RNA that\nexisted at the start of labeling and new RNA that had the potential to\nincorporate metabolic label. Tracking the dynamics of these two\npopulations yields the information necessary to dissect transcription\nand degradation kinetics of each RNA in a population of cells.\nOriginally, doing so required biochemically separating the two\npopulations and sequencing each (or more specifically enriching for one\nand sequencing the enriched and input samples). This either relied on\nchemistries that conjugated biotin to incorproated labels so that\nlabeled RNA could be separated from unlabeled via a biotin-streptavidin\npull down, or immunoprecipitation via antibodies that specifically\nrecognize the metabolic label. While powerful, these enrichment-based\ntechniques require substantial amounts of starting RNA, introduce\nbiochemical biases during enrichment, and cannot distinguish the desired\nenriched RNAs from nontrivial levels of contamination. Labeled and\nunlabeled spike-ins have been proposed to quantify and account for some\nof these challenges, but hese introduce their own experimental\nchallenges and are unable to account for length biases in the\nenrichment. Further innovation was thus required to improve the\nrobustness of metabolic labeling strategies.\n\nSeveral labs (including the Simon lab, where I did my PhD) addressed\nthese shortcomings by developing nucleotide recoding RNA-seq methods\n(NR-seq). These techniques combine s4U metabolic labeling and nucleotide\nrecoding chemistry (TimeLapse, SLAM, TUC, etc.) to either convert or\ndisrupt the hydrogen bonding pattern of incorporated s4U. This yields\napparent T-to-C mutations in the RNA-seq data that indicate sites of s4U\nincorporation and can be used to estimate the fraction of extracted RNA\nthat was synthesized after the introduction of metabolic label. This\nadds kinetic information to the snapshot provided by RNA-seq while\neliminating the need for enrichment of labeled RNA. A simplified\nschematic of NR-seq data and its analysis is presented in Figure 1.\nT-to-C mutations in sequencing reads can be used to bioinformatically\nquantify the levels of labeled and unlabeled RNA, with the Simon lab\noriginally introducing the now gold-standard mixture modeling approach\nfor this task. Simple kinetic models of the trajectories of these two\nspecies relate the kinetic parameters of interest to the data obtained.\nThis is how NR-seq can quantify the kinetics of gene expression.\n\n## Analyzing NR-seq data\n\nHere I discuss how to analyze processed NR-seq data before discussing\nthe details of NR-seq data processing. This will allow me to better\nmotivate the importance of optimally processed NR-seq data. This means I\nwill assume that you have information about how many instances of a\ngiven read-vs-reference mismatch type (e.g., T-to-C mismatches) were in\neach read, and how many mutable nucleotides were in the reference\nsequence to which this read aligned (e.g., number of reference T's).\nThroughout these and later sections, my discussion will assume a\npulse-label (vs. a pule-chase) design was utilized, as the optimality of\nthis approach will be discussed later.\n\n### Modeling the mutational content of sequencing reads\n\nIn NR-seq data, two populations of sequencing reads exist: those\noriginating from RNA synthesized prior to metabolic labeling (the\nunlabeled RNA; old RNA in a pulse-label design), and those originating\nfrom RNA synthesized during metabolic labeling (the labeled RNA; new RNA\nin a pulse-label design). On average, the mutational content of the\nlatter will be higher than that of the former. Analyzing NR-seq dat\nrequires making use of this fact to infer the fraction of reads coming\nfrom each of these two populations for a given read.\n\nOriginally, two ideas were proposed. The simplest was to use a mutation\ncontent cutoff to classify reads as coming from labeled or unlabeled\nRNA. In other words, all reads with N or more (N usually being 1 or 2)\nmutations were classified as \"labeled\", and all other reads were\nclassified as \"unlabeled\". This strategy is intuitive and\ncomputationally efficient. Despite this, it suffers from some serious\nshortcomings. For one, the mutational content of reads from labeled RNA\nis largely a function of three factors: 1) the U-content of the region\nof the RNA from which the read is derived, 2) the metabolic label\nincorporation and chemical recoding efficiencies, and 3) the background\nmutation rate. While the background mutation rate is often fairly\nconstant across samples and RNAs, the other two factors are subject to\nlarge amounts of read-to-read and sample-to-sample variability. For\nexample, different RNAs can have very different average U-contents. In\naddition, perturbing cellular metabolism often decreases label\nincorporation rates. This latter point is an especially concerning batch\neffect, as it can cause the mutation content of reads from a given\nfeature to vary not because the amount of labeled RNA differs between\ntwo conditions, but because the incorporation rate is lower in one\ncondition versus the other. The result is that mutation content cutoffs\noften provide a simple but biased estimate for the amount of labeled RNA\nfrom a given feature.\n\nA more robust analysis strategy is mixture modeling. In this strategy,\nassumptions are made about the distributions that best describe the\nexpected mutational content from labeled and unlabeled RNA. For eample,\nthe number of mutations in reads from these two populations could be\nmodeled as following a Poisson distribution with some mean, a mean which\nis necessarily higher in the labeled RNA reads than the unlabeled RNA\nreads. Due to the high amounts of read-to-read variance in U-content\nthough, modeling the mutational content as a binomial distribution that\ntakes into account both the incorporation/recoding rate as well as the\nread's U-content is optimal. This strategy (or slight variants of it),\nknown as two-component binomial mixture modeling (TCBMM), was thus\nimplemened in analysis software such as GRAND-SLAM, bakR, HalfPipe, and\nEZbakR. Mixture modeling has been shown to provide unbiased estimates of\nlabeled RNA abundance, even in the face of relatively low\nincorporation/recoding rates (Figure 2).\n\n![**Figure 2:** (From (Vock et al. 2025)) Simple mutation cutoff analysis strategies are less accurate than mixture modeling. Comparison of accuracy of 3 analysis strategies on 3 simulated datasets. Each column represents analysis for a particular dataset. Datasets differ in their simulated plabeled (pnew in formalism presented here; 1%, 2.5%, and 5% mutation rates). Each row represents a particular analysis strategy. Top row: labeled reads are defined as those with at least 1 (> 0) mutation. This is the strategy implemented in SLAMDUNK by default. Middle row: labeled reads are defined as those with at least 2 (> 1) mutations. Represents another commonly used NR-seq analysis cutoff. Bottom row: EZbakR analysis with two-component mixture modeling. Points are colored by density in all plots. $\\theta$ = fraction of reads from labeled RNA. Red dotted lines represent perfect estimation.](Figure_S1.png)\n\n\nThe likelihood function for TCBMM (Figure 3) can be generalized as such:\n\n$$\n\\begin{gather}\nL(\\theta, p_{\\text{new}}, p_{\\text{old}}) = \\theta \\cdot \\text{Binomial}(\\text{nM}, \\text{nN}, p_{\\text{new}}) + (1 - \\theta) \\cdot \\text{Binomial}(\\text{nM}, \\text{nN}, p_{\\text{old}})  \\\\\n\\text{Binomial}(\\text{nM}, \\text{nN}, p_{\\text{old}}) =  \\binom{\\text{nN}}{\\text{nM}}p^{\\text{nM}}(1-p)^{\\text{nN} - \\text{nM}} \\\\ \n\\theta = \\text{fraction of reads from labeled RNA (aka the new-to-total ratio, or NTR)} \\\\\n\\text{nM} = \\text{number of mutations (e.g., T-to-C mutations in a standard }\\text{s}^{4}\\text{U} \\text{ NR-seq analysis)} \\\\\n\\text{nN} = \\text{number of mutable nucleotides (e.g., Ts in a standard }\\text{s}^{4}\\text{U} \\text{ NR-seq analysis)} \\\\\np_{\\text{new}}, p_{\\text{old}} = \\text{mutation probability in reads from new (labeled) and old (unlabeled) RNA}\n\\end{gather}\n$$\n\n![**Figure 3:** (Adapted from (Vock et al. 2023)) Two-component binomial mixture model. The counts of T-to-C mutations, given a number of mutable nucleotides, can be modeled as coming from a mixture of two binomial distribution. One of these (that describing the mutation content of reads from new RNA), has a higher probability of a “success” (mutation) than the other.](TCBMM.png)\n\n### Modifying TCBMM\n\nThe power of mixture modeling lies in both its robustness as well as its\nextensibility. TCBMM makes several assumptions about the mutational\ncontent of NR-seq reads; namely that:\n\n-   Every uridine in an RNA synthesized in the presence of label was\n    equally likely to get replaced with s^4^U. This is formalized above\n    by there being only one $p_{\\text{new}}$.\n-   Every sequenced uridine in an unlabeled RNA was equally likely to\n    give rise to a non-s^4^U-related mutation due to sequencing errors,\n    alignment errors, etc. This is formalized above by there being only\n    one $p_{\\text{old}}$.\n-   By default, all existing tools (GRAND-SLAM, bakR, HalfPipe, and\n    EZbakR) assume that the mutation rate in reads from labeled and\n    unlabeled RNA are sample-wide global parameters. That is, all RNAs\n    transcribed from all genes are assumed to have the same rate of\n    s^4^U incorporation, and reads from these RNAs are subject to the\n    same background mutation rate. Thus$p_{\\text{new}}$ and\n    $p_{\\text{old}}$ are assumed to be the same for all genes in the\n    above formalism.\n\nIf users find one or more of these assumptions to be violated, they can\nattempt to modify and extend this model. Towards thate end, several\nmodification of standard TCBMM have been proposed. These include:\n\n-   Three-component mixture modeling, where a second population of reads\n    from unlabeled RNA with a higher mutation rate (presumably due to\n    heightened alignment errors) is modeled.\n-   Overdisperse mixture modeling where an overdisperse binomial\n    distribution (e.g., a beta-binomial) replaces one or both of the\n    binomial distribution components, or where a different incorporation\n    rate parameter is estimated for fast and slow turnover RNA.\n-   Hierarchical mixture modelign where a sample-wide average\n    incorporation rate is inferred and used as a strongly regularizing\n    prior to estimate feature-specific incorporation rates.\n-   Modeling the transcription process, which at short label times leads\n    to an expected position-dependency in the incorporation rate.\n\nWhile all of these are theoretically promising, the challenge of fitting\nmore complex models is two-fold. 1) Their increased flexibility comes\nwith an increased risk of overfitting. This can lead to estimate\ninstability, where a better model fit yields extreme conclusions about\nRNA dynamics (e.g., unusualy high fraction new and thus unrealistically\nrapid turnover kinetics). 2) While an alternative model may capture one\naspect of the true data generating process unaccounted for by TCBMM, it\nmay amplify biases that arise from not accounting for some other aspect\nof the data generating process.\n\nTo illustrate point 1, consider the task of fitting a TCBMM with\nfeature-specific mutation rates. While in theory, it is straightforward\nto obtain maximum likelihood estimates for the parameters of such a\nmodel, model flexibility can make interpretation of maximum likelihood\nparameters fraught. Intuitively, this is because changing different\nparameters can have similar expected impacts on your data. A higher\nfraction new will yield more reads with high mutational content, but so\nwill a low fraction new combined with a higher background mutation rate.\nWhile with enough reads these two situations can be accurately\ndeconvolved, this analysis is highly uncertain for low coverage\nfeatures.\n\nTo illustrate point 2, consider the idea of three-component mixture\nmodeling. While this can capture certain types of overdispersion in\nmutation rates from old RNA reads, it can amplify biases from not\nmodeling overdispersion in mutation rates from new RNA reads. A\nthree-component mixture model will classify many moderate mutation rate\nreads as \"old\", when in fact they may represent a preponderance of low\nmutation rate new reads. This kind of overdispersion is made even more\nlikely by the fact that metabolic label availability will likeloy ramp\nup and down over time. Thus, reads from RNA synthesized at different\ntime points may have different true mutation rates.\n\nHow can one navigate building more complex models while avoiding some of\nthese problems? Point 1 can be addressed through regularization. From a\nBayesian perspective, this means using one's domain expertise or trends\nin these high-throughput datasets to craft informative priors that\nconstrain the parameter search space. For example, to fit a hierarchical\nmixture model in EZbakR, where each feature is allowed to have its own\nnew read mutation rate ($p_{\\text{new}}$), I crafted a strategy to infer\nstrongly regularizing priors from sample-wide trends. These priors were\ndesigned to be very conservative to limit estimate variance.\n\nPoint 2 represents the fundamental challenge of statistical modeling:\ncrafting a model of the data generating process that faithfully captures\nmost of the relevant sources of variance in one's data. This is\ndifficult, but several strategies exist to navigate this complexity.\nInformation criteria are a popular metric by which to compare fits of a\nmore complex model to that of a simpler model. These criteria are\ndesigned to penalize model complexity to avoid rewarding overfit models\nwith better metrics. While simple criteria like the Akaike information\ncriteria (AIC) are popular due to their implementation ease, more robust\nmetrics have been developed since the advent of AIC. For example, in the\ncontext of mixture modeling, the widely applicable information criteria\n(WAIC; a.k.a. the Watanabe-Akaike Information Criteria) may provide a\nnumber of advantages over AIC.\n\nInformation criteria are not panacea though. Information criteria are\nrigorous ways to assess if added model complexity is capture a\nsignificant amount of variance in your data that a simple model fails to\naccount for. Even if a more complex model is succeeding by this metric,\nit could still be providing biased estimates. Thus, when designing new\nNR-seq models, I suggest a multi-pronged approach that weighs several\nmetrics when deciding if a more complex model is worth using. If\nadopting a Bayesian approach, information criteria can be complemented\nwith posterior predictive checks, where data is simulated from the fit\nmodel and compared to the analyzed data. Serious discrepancies between\nsimulated and real data can reveal model mis-specifications and guide\nmodel improvement. We also suggest comparing results given by standard\nTCBMM with those provided by a more complex model. Extreme sample-wide\ndiscrepancies between the two may signify that the more complex model is\noverfitting or providing unstable estimates. Discrepancies should thus\nbe thoroughly explored and explained. Finally, we suggest assessing\nmodel robustness through simulations from a data generating process more\ncomplicated than that used for model fitting. If the bias introduced by\nthese true vs. assumed data generating process discrepancies is\namplified by use of a more complex model relative to TCBMM, we urge\ncautio nin adopting the more complex model. The simplicity and\nrobustness of TCBMM makes it an effective baseline with which to compare\nalternative models.\n\n### Transcript isoform analyses of short-read data\n\nTranscript isoforms are the RNA species whose synthesis and degradation\nkinetics are of biological significance. Despite this, quantifying the\nNTRs of individual transcript isoforms in a short-read NR-seq experiment\nis challenging. This is because most short reads cannot be unambiguously\nassigned to a specific transcript isoform. Strategies have been\ndeveloped to overcome these challenges in the context of quantifying the\nabundances of isoforms. I thus recently developd a similar approach to\nestimate isoform-specific NTRs in short read NR-seq data.\n\nThe approach, implemented in the EZbakR suite, combines standard NR-seq\nTCBMM with transcript isoform quantification. This approach estimates\nNTRs for each observed transcript equivalence class (TEC; i.e., the set\nof isoforms with which a read is compatible) and integrates this with\nestimates of transcript isoform abundances from standard tools for this\ntask. EZbakR is able to estimate isoform-specific NTRs by deconvolving\nTEC NTRs using a novel beta mixing model. I used this approach while in\nthe Simon lab, and in collaboration with Bobby Hogg's lab at the NIH, to\nstudy the synthesis and degradation kinetics of individual transcript\nisoforms, and to identify NMD sensitive isoforms.\n\nAccurate transcript isoform analyses require annotations of expressed\nisoforms. In our work presenting the isoform-NTR estimation strategy, we\nnoted that standard off-the-shelf references did not faithfully reflect\nour particular cell line's transcriptome. We thus explored strategies\nusing StringTie2 and custom filtering to build more accurate, bespoke\nannotations. We showed that this approach significantly improved the\naccuracy of transcript-isoform level analyses. While a powerful\napproach, the need to build custom annotations adds to the complexity of\nthe workflow. In addition, tools for building such annotations are not\nwithout their limitations, as ab initio assembly is a fundamentally\ndifficult task. Having matched, unlabeled, long read data can improve\nassembly, but presents its own challenges and shortcomings. Thus, while\nisoform-level analyses represent a poewrful new paradigm in analyses of\nNR-seq data, I urge users to carefully assess the potential of\nannotation-related biases in their analyses.\n\n### Modeling and correcting for dropout in NR-seq data\n\nLike all other RNA-seq based methods, NR-seq data can be plagued by\nvarious biases. The most prominent example of this is dropout, a\nphenomenon observed across many distinct datasets. Dropout is the\nunderrepresentation of reads from labeled RNA. While its origins are not\nfully understood, it has been proposed to be caused by a combination of\ndisproportionate loss of labeled RNA during RNA extraction and library\npreparation, loss of high mutation content reads during alignment, and\ntoxicity due to s^4^U labeling.\n\nDropout in NR-seq data can be detected and quantified with the help of\nno-label controls. A no-label control refers to data from samples not\nfed with a metabolic label. These are important controls that should be\nincluded in all NR-seq datasets. A simple model of dropout is that there\nexists a global rate at which label-containing RNA is lost relative to\nunlabeled RNA, referred to as the dropout rate. Thus, rapidly turned\nover RNA that are more highly labeled will be disproportionately\naffected compared to more stable, relatively unlabeled RNA. Comparing\nthe estimated turnover rate in the labeled samples to the no-label vs.\nlabeled read counts can thus reveal dropout. More specifically, dropout\nlooks like a strong correlation between these quantities. Plots like\nthose shown in Figure 4 can be easily made with both grandR and EZbakR.\n\n![**Figure 4**: Visualizing dropout. A simple model of dropout was simulated using EZbakR’s EZSimulate() function, where labeled RNA is lost relative to unlabeled RNA at a rate denoted above each pair of plots (pdo). grandR implements a visualization strategy that correlates the NTR rank (1 = smallest) vs. the log-difference in +label and -label read coverage (dropout). The strength of correlation between these is related to the rate of dropout. bakR implements a visualization strategy that allows for direct assessment of the fit of a simple dropout model on the data. The quantities plotted are similar to those in grandR, just on different model-relevant scales. EZbakR implements both of these visualization strategies.](Dropout_plots.png)\n\nIf your NR-seq data suffers from non-trivial amounts of dropout, you can\nemploy strategies to correct for dropout. grandR was the first tool to\nimplement such a strategy. It follows from a model alluded to above and\nassumes that there is a sample-wide rate (call it\n$\\text{p}_{\\text{do}}$) at which labeled RNA is disproportinately lost\nrelative to unlabeled RNA. If this is the case, then then true fraction\nof reads from labeled RNA ($\\theta_{\\text{true}}$) is related to the\ndropout-biased estimate ($\\theta_{\\text{do}}$) like so:\n\n$$\n\\begin{gather}\n\\theta_{\\text{true}} = \\frac{\\theta_{\\text{do}}\\cdot\\text{p}_{\\text{do}}}{\\theta_{\\text{do}}\\cdot\\text{p}_{\\text{do}} + (1-\\theta_{\\text{do}})}\n\\end{gather}\n$$\n\nSimilarly, a relationship exists between the true expected read counts\nfrom a given feature and the observed, labeled sample read count:\n\n$$\n\\begin{gather}\n\\text{R}_{\\text{true}} = \\text{R}_{\\text{do}} \\cdot \\frac{\\theta_{\\text{G}}\\cdot(1-\\text{p}_{\\text{do}}) + (1-\\theta_{\\text{G}})}{\\theta_{\\text{true}}\\cdot(1-\\text{p}_{\\text{do}}) + (1-\\theta_{\\text{G}})} \\\\\n\\theta_{\\text{G}} = \\frac{\\theta_{\\text{G,do}}}{(1-\\text{p}_{\\text{do}}) + \\theta_{\\text{G,do}} \\cdot \\text{p}_{\\text{do}}} \\\\\n\\theta_{\\text{G,do}} = \\frac{\\sum_{\\text{j=1}}^{\\text{NF}} \\theta_{\\text{do,j}} \\cdot \\text{R}_{\\text{j}}}}{\\sum_{\\text{j=1}}^{\\text{NF} \\text{R}_{\\text{j}}}\n\\end{gather}\n$$\n\nUsing these theoretical relationships, a dropout rate can be estimated\nthat, after correcting read counts, yields no correlation between the\nlabeled vs. no-label read count ratio and the turnover kinetics of an\nRNA. This strategy is implemented in grandR. bakR implements a similar\nstrategy by which the relationship between dropout and the NTR is\nmodeled and fit with the method of maximum likelihood. EZbakR implements\nboth of these strategies.\n\nDropout correction is a powerful addition to the NR-seq analysis\ntoolkit. Despite this, it is not without its limitations. The\nrequirement for matched no-label samples in all conditions tested adds\nto the experimental burden. In addition, even when no-label data is\ncolelcted, resource constraints often lead researchers to only collect a\nsingle replicate of this data, as it is not itself a useful NR-seq\nsample. This can make the dropout metric, the ratio of labeled:no-label\nread counts, noisy.\n\nTo address these limitations, EZbakR implements a strategy I refer to as\ndropout normalization. Dropout normalization does not require any\nno-label data and involves comparing internally normalized NTRs rather\nthan read counts across samples. The strategy starts by identifying the\nlowest dropout sample (e.g., that which provides the lowest median\nuncorrected half-life estimate) and estimating dropout in other samples\nrelative to this sample. This estimated relative dropout rate is then\nused to correct NTRs and read counts in all samples. This strategy has\nproven particularly useful as dropout rates often correlate with\nbiological conditions, which risks confounding comparative analyses if\nnot properly accounted for. The downside of dropout normalization is\nthat it tends to normalize out global differences in half-life\nestimates, even if tehse are biologically real and not solely the result\nof dropout. Dropout is thus similar in spirit to RNA-seq read count\nnormalization methods such as the median-of-ratios or TMM, as well as a\nsimpler median-kdeg normalization strategy implemented in grandR, which\nall effectively assume that there are no real global differences in RNA\nlevels/turnover kinetics across samples. Dropout normalization\nalternatively assumes that any global changes in RNA turnover kinetics\nare dropout driven. Users should thus be aware of this assumption when\nusing dropout normalization.\n\n## Kinetic parameter estimation with NR-seq data\n\nOne of the original motivations for developing NR-seq was to robustly\nestimate the kinetics of RNA synthesis and degradation. In this section,\nI discuss the modeling and assumptions necessary to make this possible.\n\n### Standard kinetic analyses\n\nVanilla, bulk NR-seq is a powerful method by which to quantify the\nkinetics of RNA synthesis and degradation. A typical kinetic analysis\nmeans modeling the NTR of reads from a feature (e.g., the union of exons\nat a gene) as a function of the RNA's synthesis and degradation rate\nconstants. The simplest identifiable model of this sort assumes that\nmature mRNA is synthesized at a rate $\\text{k}_{\\text{syn}}$ and\ndegraded with a rate constant $\\text{k}_{\\text{deg}}$. This model can be\nformalized via the following analytically tractable differential\nequation:\n\n$$\n\\begin{gather}\n\\frac{\\text{dR}}{\\text{dt}} = \\text{k}_{\\text{syn}} - \\text{k}_{\\text{deg}} \\cdot \\text{R} \\\\\n\\text{Solution (with R(0) = 0)}: \\text{R(t)} = \\frac{\\text{k}_{\\text{syn}}}{\\text{k}_{\\text{deg}}} \\cdot (1 - \\text{e}^{\\text{k}_{\\text{deg}} \\cdot \\text{t}}) \\\\\n\\end{gather}\n$$\n\nAt steady-state, the following relationships hold:\n\n$$\n\\begin{gather}\n\\frac{\\text{dR}}{\\text{dt}} = 0;\\text{ }\\text{R}_{\\text{ss}} = \\frac{\\text{k}_{\\text{syn}}}{\\text{k}_{\\text{deg}}} \\\\\n\\text{NTR} = \\frac{\\text{R(tl)}}{\\text{R}_{\\text{ss}}} = 1 - \\text{e}^{\\text{k}_{\\text{deg}} \\cdot \\text{t}} \\\\\n\\text{k}_{\\text{deg}} = \\frac{-\\text{ln}(1-\\text{NTR})}{\\text{t}_{\\text{label}}}\n\\end{gather}\n$$ This model makes several explicit assumptions:\n\n-   Steady-state: this means that during the labeling, RNA levels are\n    not changing. While individual cells may rarely ever be at\n    steady-state, e.g., as transcript levels are regulated throughout\n    the cell cycle, in bulk NR-seq this means assuming that the average\n    RNA levels across all cells assayed are constant.\n-   Zeroth-order synthesis kinetics (i.e., transcription is a Poisson\n    process): this means that there exists a single, constant rate of\n    transcription during the labeling. While once again often violated\n    in single cells due to phenomena such as transcriptional bursting,\n    it is likely a decent model of bulk transcriptional behavior.\n-   First-order degradation (i.e., exponential decay): this means that\n    RNAs have a characteristic half-life that does not change over\n    either the course of the labeling or their lifetime. This means\n    assuming that RNAs are \"ageless\" or that the probability an RNA\n    degrades in the next instance is independent of how long it has been\n    around. This assumption breaks down if multiple rate-limiting steps\n    separate an RNA's birth from its detah (e.g., if RNA export from the\n    nucleus is on a similar time-scale of cytoplasmic degradation), or\n    if there exist multiple sub-populations of RNA with different decay\n    kientics (e.g., if nuclear mRNA is degraded at a different rate than\n    cytoplasmic mRNA). While this assumption is inevitably violated to\n    some extent due to the complexity of the RNA life cycle, it has\n    consistently proven to be a reasonable approximation in a variety of\n    settings, while still providing a useful (if somewhat biased)\n    picture of general turnover kinetics when it is violated.\n\n### Non-steady-state modeling\n\nThe steady-state assumption makes analyzing and interpreting NR-seq data\neasy. Despite this, it is often violated in contexts where you are\napplying a perturbation to cells shortly before or during labeling.\nNarain et al. proposed a strategy to obtain unbiased estimates even in\nthis setting. The intuition behind how this approach works is that no\nmatter what, the dynamics of old (unlabeled) RNA is entirely a function\nof turnover kinetics. Thus, if you know the levels of an RNA at the\nstart of labeling, then that combined with its level at the end of\nlabeling tells you how much RNA decayed in that time frame. Formally:\n\n$$\n\\begin{gather}\n\\text{R}_{\\text{old}}(\\text{t}) = \\text{R}_{\\text{init}} \\cdot e^{-\\text{k}_{\\text{deg}} \\cdot \\text{t}} \\\\\n\\text{R}_{\\text{new}}(\\text{t}) = \\frac{\\text{k}_{\\text{syn}}}{\\text{k}_{\\text{deg}}} \\cdot (1 - e^{\\text{k}_{\\text{deg}} \\cdot \\text{t}}) \\\\\n-\\frac{\\text{ln}(\\frac{\\text{R}_{\\text{old}}(\\text{t}_{\\text{label}})}{\\text{R}_{\\text{init}}})}{\\text{t}_{\\text{label}}} = \\text{k}_{\\text{deg}} \\\\\n\\frac{\\text{R}_{\\text{new}}(\\text{t}_{\\text{label}})}{1-e^{\\text{k}_{\\text{deg}} \\cdot \\text{t}}}\\cdot \\text{k}_{\\text{deg}} = \\text{k}_{\\text{syn}}\n\\end{gather}\n$$\n\nAn estimate for $\\text{R}_{\\text{init}}$ typically comes from matched\nRNA-seq data collected at a time point equivalent to that at which\nlabeling was started for a given labeled sample. Note, the stability or\nsynthesis rate of an RNA could be changing during the labeling. The\n$\\text{k}_{\\text{deg}}$ and $\\text{k}_{\\text{syn}}$ estimates from this\nstrategy should thus be thought of as the time-averaged value of a\npotentially time-varying $\\text{k}_{\\text{deg}}(\\text{t})$ and\n$\\text{k}_{\\text{syn}}(\\text{t})$. Both grandR and EZbakR implement this\nanalysis strategy.\n\nWhile having an approach that yields theoretically unbiased estimates of\nturnover and synthesis kinetics even in the face of non-steady-state\ndynamics is powerful, it is not without its limitations. For one, a\nunique experimental design is required. If you are missing RNA-seq data\nfrom the start of any labeling period, this strategy cannot be applied.\nIn addition, even with the proper experimental design, this analysis\napproach has some weaknesses. Its statistical properties are suboptimal\nrelative to the steady-state analysis. In particular, the variance of\nthe $\\text{k}_{\\text{deg}}$ estimation is typically higher than that of\nthe steady-state estimate, especially at low NTRs (Figure 5).\nIntuitively, this is because the steady-state estimate is only a\nfunction of a single sample's NTR, while the non-steady-state estimate\nrequires comparing an NTR estimate combined with a normalized read count\nin one sample to a normalized read count in another sample. Thus, while\nthe estimate is technically unbiased, its mean-squared error is not\nnecessarily lower than that of the biased steady-state analysis. This\nproblem can partially be addressed through greater sequencing depth,\nlabel time optimization, and more replicates, but it is a challenge that\nusers should be aware of.\n\n![**Figure 5:** Comparison of steady-state and non-steady-state (NSS) analysis strategies. Steady-state data was simulated using EZSimulate() at 3 different label times (2, 4, and 6 hours). Both of these analysis strategies are appropriate in this setting; this comparison is meant to show differences in the best-case variance properties of these two approaches. 2 replicates of labeled and 2 replicates of no-label data were simulated. The simulated (true) and estimated degradation rate constants are compared in all plots. Points are colored by their local density. Red dotted line represents perfect estimation.](NSS.png)\n\n### Subcellular NR-seq\n\nSeveral studies have combined NR-seq with subcellular fractionation to\ninvesigate the kinetics of subcellular RNA trafficking. To date, such\nefforts have included sequencing chromatin-associated RNA, nuclear RNA,\ncytoplasmic RNA, ribosome-associated RNA, and membrane-associated RNA.\nThese data necessitate fitting more complex kinetic models to\nmulti-sample datasets. While Halfpipe was the first tool to support\nanalyses of one particular fractionation scheme, EZbakR is the first\ntool designed to support fitting any such model.\n\nEZbakR makes use of the fact that the most common identifiable models of\nRNA dynamics that subcellular NR-seq has been used to fit can be\nformalized as a linear system of ordinary differential equations (ODEs)\nof the form:\n\n$$\n\\begin{gather}\n\\textbf{R} = \\text{M} \\cdot \\dot{\\textbf{R}} + \\textbf{b} \\\\\n\\textbf{R} = \\text{vector of RNA species abundances of length N} \\\\\n\\text{M} = \\text{NxN matrix of first-order kinetic parameters} \\\\\n\\dot{\\textbf{R}} = \\text{time derivative of } \\textbf{R} \\\\\n\\textbf{b} = \\text{vector of zeroth-order rate constants (e.g., synthesis rate)}\n\\end{gather}\n$$\n\nAny such system of ODEs has an analytic solution inferrable from the\nmatrix M and forcing vecetor $\\textbf{b}$. EZbakR makes use of this fact\nto efficiently fit these models without needing to rely on numerical\nintegration. It also provides means for uncertainty quantification that\nis capable of flagging instances of practical unidentifiability, i.e.,\nwhen specific parameters of a model are outside of the dynamic range of\nwhat can be reasonably estimated given experimental details and noise.\n\nEZbakR also implements a novel strategy by which to normalize certain\nclasses of subcellular NR-seq. This strategy relies on having data from\nindividual fractions, as well as a sufficient set of combinations of\nfractions (e.g., nuclear, cytoplasmic, and whole cell). It uses the fact\nthat the total fraction of reads in each of the sub-compartments that\nare new (when adjusted for average length differences in the RNA\npopulations) is related to the same quantity in the combination\ncompartment. The scale factor that relates these quantities is precisely\nthe ratio of absolute abundances between the sub-compartments. For the\nwhole-cell, nuclear, and cytoplasmic example, this relationship looks\nlike:\n\n$$\n\\begin{gather}\n\\theta_{\\text{WC}} = \\frac{[\\text{C}]}{[\\text{C}] + [\\text{N}]}\\theta_{\\text{C}} + \\frac{[\\text{N}]}{[\\text{C}] + [\\text{N}]}\\theta_{\\text{N}} = s\\cdot\\theta_{\\text{C}} + (1-s)\\cdot\\theta_{\\text{N}} \\\\\n\\theta = \\text{total fraction of reads from new RNA in a given compartment} \\\\\n\\text{WC = whole-cell; C = cytoplasm; N = nucleus} \\\\\n[\\text{C}] = \\text{absolute levels of cytoplasmic RNA} \\\\\n[\\text{N}] = \\text{absolute levels of nuclear RNA}\n\\end{gather}\n$$\n\nAn appropriate RNA-seq read count normalization scale factor can be\ninferred from the value of $s$. While this strategy requires having\nNR-seq in all the necessary sub- and combination-compartments, recent\nwork suggests that it may be possible to relax this requirement and\ninstead only require at least standard RNA-seq data for all such\ncompartments.\n\n## Processing NR-seq data\n\nThe previous sections assumed that we had optimally processed NR-seq\ndata. In this section, I discuss how to obtain such data and the\nchallenges involved.\n\n### Aligning NR-seq data\n\nThe first data processing step with NR-seq-specific challenges is\naligning reads to the genome. The mutations introduced by recoding\nchemistries make this more challenging than in standard RNA-seq. These\nmismatches risk yielding inaccurate alignments or causing some reads to\nbe unmappable.\n\nSLAMDUNK, one of the first bioinformatic pipelines developed for\nprocessing NR-seq data, utilized the NextGenMap aligner to overcome this\nchallenge. NextGenMap is a non-splice aware aligner that provides an\nalternative mismatch scoring function designed for NR-seq data. This\nscoring function penalizes all but T-to-C mismatches, ameliorating the\nnegative impact of these mismatches on alignment accuracy. In addition,\nSLAMDUNK is optimized for 3'-end sequencing data. Because of this, it is\nable to reduce the sequence search space by aligning to the set of\nannotated 3'-UTRs rather than the entire genome. SLAMDUNK also\nimplements a unique multi-mapping read assignment strategy that\npreferentially assigns ambiguous reads to the more likely 3'-UTR.\nDespite being designed for 3'-end sequencing, some have used SLAMDUNK to\nprocess full-length NR-seq data. While NextGenMap's lack of splice\nawareness makes it impossible to accurately align reads from these\nlibraries to a genome, one can instead opt for alignment to a\ntranscriptome. This has a number of downsides (e.g., improper alignment\nof intron mapping reads) but can still provide reasonable results.\n\nFor full-length NR-seq data, standard splice-aware aligners like STAR\nand HISAT2 were originally preferred. Using these tools typically\nrequires relaxing the default mismatch penalization. Unlike in\nNextGenMap, no such aligner allows for this to be done in a mismatch\ntype-specific manner (technically BASAL, a recently developed aligner,\ncan also do this, but it cannot provide the unique MD bam file tag that\nis required for all existing NR-seq pipelines). This leads to a\nnecessary reduction in alignment accuracy to retain high mutation\ncontent reads. To address this challenge, HISAT-3N was developed, which\naligns reads to a 3-base genome, i.e. a genome where all Ts are\nconverted to Cs. By similarly convertins Ts i nall reads to Cs, reads\ncould be aligned without penalizing T-to-C mismatches. While this has\nbeen useful in recovering high mutation content reads, it is not without\nits own unique downsides. The lower complexity genome increases the\nlikelihood that a read does not uniquely map to a given locus, and can\nmore generally reduce the accuracy of read alignment. Recently, a\nrigorous simulation benchmark was performed to compare HISAT-3N and STAR\nalignment of NR-seq data. It concluded that neither is strictly superior\nto the other, with each having unique strengths and weaknesses. It\nremains to be seen if the same holds true for more recent 3N-aligners\nlike rmapalign3N. As STAR is more widely used and actively maintained\naligner than HISAT-3N, we and others typically opt for STAR when\naligning full-length RNA NR-seq data.\n\nIdeally, a strategy would exist to get the best of both 4- and 3-base\nalignment. Towards that end, grandRescue was developed. grandRescue\nfirst aligns reads with STAR to a 4-base genome. Then, reads that failed\nto align are aligned with STAR to a 3-base genome (while HISAT-3N makes\n3-base genome alignment convenient, it is technically possible with any\naligner. grandRescue provides the necessary helper functions to make\nthis user friendly). This allows grandRescue to outperform both STAR and\nHISAT-3N on the task of accurately aligning high mutation content reads\nfrom full-length NR-seq data. It also showed improved alignment accuracy\nover NextGenMap transcriptome alignment. While grandRescue has not yet\nreached STAR's level of bioinformatic maturity, it introduced a powerful\nparadigm for the accurate alignment of NR-seq reads.\n\n### Counting mutations\n\nOnce NR-seq reads have been aligned to a genome, many of the additional\nprocessing steps are shared with standard RNA-seq analyses. Despite\nthis, a unique aspect of NR-seq data processing post-alignment is\nmutation counting.\n\nNR-seq reads from labeled RNA will *on average* contain more mutations\nof a particular type than those from unlabeled reads. Thus, quantifying\nthe number of mutations of the relevant type (e.g., T-to-C mutations in\nstandard s^4^U NR-seq experiments) is a key compoennt of any NR-seq\npipeline. Unfortunately, there are no flexible, general-purpose\nbioinformatic tools specifically for this purpose. Instead, every NR-seq\npipeline typically reimplements a solution to this problem from scratch.\nSLAMDUNK is the closest exception, as it includes a mutation counting\nmodule that in theory could be slotted into other NR-seq pipelines.\nDespite this, SLAMDUNK's mutation counting module makes some rigid\nassumptions (e.g., single-end, forward stranded libraries) that make it\ndifficult to use in many situations. In addition, SLAMDUNK does not\ncurrently provide read-specific mutation counts and instead provides\nfeature-wide summarization of this information. This makes it\nincompatible with optimal downstream analyses (namely mixture modeling;\nsee the earlier section on \"Modeling the mutation content of sequencing\nreads\"). Finally, SLAMDUNK can only provide information about T-to-C\nmutational content, making it incompatible with s^6^G NR-seq datasets\n\nfastq2EZbakR and GRAND-SLAM lack the modularity of SLAMDUNK, but are\ncompatible with a wider array of NR-seq library types. Both can quantify\nT-to-C and G-to-A mutations. fastq2EZbakR is additionally able to count\nany combination of all mutation types. This allows these tools to\nsupport s^6^G-based NR-seq assays, and in the case of fastq2EZbakR, even\npotentially support the wider array of mutation-based chemical probing\nexperiments.\n\nA general feature of most NR-seq processing tools is the ability to mask\nsingle nucleotide polymorphisms (SNPs) when counting mutations. Despite\nits ubiquity, the relative efficacy of different mutational artifact\ncalling strategies in NR-seq data is understudied. Several approaches\nhave been suggested, including a simple cutoff of the fraction of times\na given nucleotide is identified as having a mismatch with the\nreference, or the use of established variant calling software like\nVarScan or BCFtools. Also, while SNP calling has been shown to\nsignificantly impact analyses relying on simple mutation-content cutoffs\nfor quantifying labeled read abundances, mixture modelign may be more\nrobust to high background mutatio nrates and uncalled SNPs. Despite\nthis, systematic, non-random mutations may violate the assumptions of\nindependence underlying popular mixture modeling strategies, especially\nin lower complexity libraries like those from 3'-end sequencing,\nhighlighting the potential importance of SNP-making and its continued\noptimization.\n\n### Feature assignment\n\nNR-seq reads often must be assigned to the annotated features (e.g.,\ngenes) to which they belong. Read-by-read feature assignment can then be\ncombined with mismatch calls to estimate the feature's NTR and metabolic\nkinetic parameters. While most NR-seq pipelines provide a limited set of\nfeatures to which this assignment can be done, fastq2EZbakR recently\nsignificantly expanded the flexibility of NR-seq feature assignment.\nfastq2EZbakR can assign reads to genes (exons and introns), exclusively\nexonic regions of genes, exonic bins, transcript equivalence classes,\nand exon-exon junctions. While some analyses of NR-seq only require\nstandard gene-level quantification, fastq2EZbakR's expanded set of\nfeature assignment strategies allows for a higher resolution dissection\nof RNA dynamics.\n\n### Processed data format\n\nWhat is the best way to store and format processed NR-seq data? For most\nanalyses of vanilla RNA-seq data, the answer to this question is a count\nmatrix. For each feature analyzed and each sample collected, a count\nmatrix notes the number of times a read came from that feature in that\nsample or tracks some more sophisticated measure of that feature's\nabundance. Count matrices are the ideal processed RNA-seq data format\nfor several reasons. Their size largely scales with the number of\nsamples, not the depth of sequencing. This is because the number of\nannotated features in a given organism (i.e., the number of rows of the\ncount matrix) is mostly fixed, and no matter how many reads you have in\na given sample, each feature will end up with a single number for each\nsample. This format also lends itself to highly efficient linear algebra\noperations and is widely used in a number of bioinformatic libraries.\nThus, one could argue that a similar procssed data format should be used\nfor NR-seq.\n\nTowards that end, several popular NR-seq tools provide processed data to\nusers in a form analagous to a count matrix. Both SLAMDUNK and\nGRAND-SLAM provide a table with a fixed set of data points for each\ngenes. SLAMDUNK provides users with information about the number of\nreads with conversions from a given feature, and GRAND-SLAM provides an\nNTR estimate for each feature from mixture modeling. The former is\nsuboptimal for reasons discussed in the section on \"Analyzing NR-seq\ndata\" but the latter represents the gold standard in NR-seq analysis\noutput. Thus, NR-seq tools should at least be able to provide NTR\nestimates and read counts for each feature in each sample collected.\n\nDespite this, I argue that there are a number of important limitations\nto providing users with this form of processed data. In my experience,\nwhen things go wrong with an NR-seq analysis, the problems start at the\nmixture modeling step. Thus, by only providing users mixture model\noutput, and thus limited strategies by which to interrogate model fit\nand identify potential model biases, this form of processed data risks\nobscuring issues in an NR-seq dataset. In addition, only providing NTR\nestimates limits innovation in NR-seq analyses. If a user wants to\ndevelop their own NTR estimation strategy, they must also develop their\nown pipeline to generate data in a form compatible with their modeling\nstrategy.\n\nFor this reason, fastq2EZbakR (and bam2bakR before it) provides user\nwith the processed data necessary to fit state-of-the-art models of\nNR-seq data. For each feature and sample, fastq2EZbakR reports the\nnumber of redas with a given number of mismatches of the relevant type\n(e.g., T-to-C in a single label, s^4^U-fed, NR-seq experiment) and a\ngiven number of mutable nucleotides (e.g., T's in the reference). The\nSimon lab termed this file format a \"counts binomial\", or cB, file. The\nrecently developed Halfpipe provides similar data though in a different\nformat. With this information, users can either make use of mixture\nmodel fitting strategies implemented in tools like EZbakR or develop\ntheir own unique analysis strategy.\n\nProcessed NR-seq data formats lik cB files significantly democratizes\nthe analysis of NR-seq data. Despite this, it is not without its\nlimitations. Unlike with GRAND-SLAM's output, the size of a cB file\nscales with the sequencing depth. While the scaling is sub-linear, as\ndata can be compressed by tracking the number of reads with identical\nmutation and mutable nucleotide content, the output is still often far\nmore unwieldy than GRAND-SLAM's simple table. To address this\nlimitation, fastq2EZbakR implements an alternative output type that\ninvolves creating separate files for separate smaples. While the size of\nthese individual files will still scale with the sequencing depth,\nsplitting the files in this manner supports analysis strategies that\neffectively load only a single sample into RAM at a time. EZbakR\nimplements such an analysis strategy, optimized with the help of the\nApache Arrow project and its R frontend, which has significantly\nincreased the scalability of its analyses.\n\nUnique decisions have to be made regarding how to format and distribute\nNR-seq data. While count matrix-like formats are familiar, scalable, and\nintuitive, they may significantly limit the types of analyses users can\nperform. Alternative data formats can address these issues. While more\ncompressed than read-level formats (e.g., BAM files), these alternatives\nstill have to sacrifice RAM scalability. We suggest that all NR-seq\npipelines should produce output of this latter type, while optionally\nproviding simpler, count matrix-like output for those looking to avoid\nthe extra hassles.\n\n## Optimal NR-seq experimental design\n\nNo matter how optimal your data processing and analysis pipeline is, bad\ndata can fundamentally limit obtainable insights. In this section, I\ndiscuss how to best design an NR-seq experiment to avoid these problems.\n\n### Choosing the population of RNA to sequence\n\nIn theory, NR-seq can be applied to a wide array of library preparation\nstrategies to probe the dynamics of your choice of RNA. SLAM-seq\noriginally made use of 3'-end sequencing, and a commercially available\nkit for this library preparation strategy was developed shortly after\nits initial publication. Combining NR-seq with 3'-end sequencing has\nseveral advantages. For one, it limits the amount of pre-mRNA in your\nlibrary. Pre-mRNA are typically rapidly turned over and can thus lead to\nslight overestimation in turnover kinetics if not accounted for in\ndownstream analyses. In addition, 3'-end sequencing allows for some\nlevel of isoform deconvolution, allowing analyses to distinguish the\nkinetics of alternative 3'-UTR isoforms. This is useful as 3'-UTRs are\nwell established hubs of post-transcriptional regulation.\n\nAlternatively, NR-seq can be combined with any standard full-length\nRNA-seq prep. TimeLapse-seq orginally made use of this strategy, and it\nhas been used with other recoding chemistries as well. While lacking\nsome of the unique advantages of 3'-end NR-seq, full-length NR-seq\nprovides more information. For one, the dynamics of pre-mRNA and mature\nmRNA can be assessed in the same sample with full-length RNA data. In\naddition, I recently developed strategies using full-length RNA NR-seq\ndata to infer the turnover and synthesis kinetics of individual\ntranscript isoforms. I worked with Bobby Hogg's lab at the NIH to apply\nthis approach to the study of NMD-sensitive isoforms, many of which are\nnot the result of alternative 3'-UTR usage and thus invisible to 3'-end\nNR-seq.\n\nDespite the common use cases, the poewr of NR-seq lies in its ability to\nbe combined with myriad library preparation strategies. For example,\nNR-seq has been combined with small RNA sequencing to probe the dynamics\nof miRNA. It has also been combined with a number of unique library\npreparation strategies to assess the dynamics of many different\nbiological processes (see section on extensions at the end). Thus, users\nhave copious flexibility when deciding what RNA they want to sequence in\nan NR-seq experiment.\n\n### Labeling and sequencing depth\n\nWhen designing and NR-seq experiment, there are 3 major\nconsiderations: 1) how deeply to sequence, 2) what concentration of\ns^4^U to use, and 3) how long to label with s^4^U.\n\nThe answer to the first is somewhat trivial: as deep as you can\ncomfortably afford. Sequencing coverage has a significant impact on the\naccuracy of NR-seq analyses, and thus maximizing coverage is beneficial\n(Figure 6). That being said, high quality, reproducible estimates of RNA\nhalf-lives have been obtained from a wide range of sequencing depths. If\nyou are looking to analyze specific feature classes though, like splice\njunctions, exonic bins, or transcript isoforms, depth can be an even\nmore important consideration. On the other hand, if performing 3'-end\nNR-seq, equally confident estimates can be obtained with less depth due\nto the lower complexity of these libraries.\n\nOptimization of s^4^U labeling conditions is a more subtle challenge.\nFirst, it is important to assess the extent to whcih s^4^U is getting\nincorporated into the system that you are studying. This can be done\nthrough low-throughput assays such as TAMRA dot blots, or small scale\nsequencing experiments. We have found there to be a significant amount\nof system-to-system variability in s^4^U incorporation rates. A standard\ns^4^U concentration of 100 uM has proven effective in a wide array of\nsettings. Despite this, some cell liens may take up s^4^U far more\nreadily, to the point that it is best to significantly decrease s^4^U\nconcentrations to avoid cytotoxic effects. Others may require much\nhigher concentrations to get appreciable incorporation rates. It is thus\nimportant to assess the s^4^U incorporation tendencies in any new system\nin which you have not previously performed NR-seq.\n\nThe length of time to label with s^4^U is the final major consideration\nfor an NR-seq experiment. Statistical arguments have been made in favor\nof a label time around the median half-life of the RNAs you are\ninterested in studying. For human mRNA, this is around 4 hours. This\nargument reflects the fact that the label time establishes the dynamic\nrange of your half-life estimation. RNAs with half-lives much longer\nthan your label time will have nearly undetectable levels of labeling,\nand those with half-lives much shorter than your label time will be\nalmost completely labeled. A naive estimate of the dynamic range can be\nmade by noting that in the most extreme case, you can claim that you\nhave a single labeled or unlabeled read, making the range of meaningful\nhalf-life estimates:\n\n$$\n\\begin{gather}\n\\text{Minimum} \\text{ t}_{1/2} \\text{ estimate} = \\frac{\\text{ln}(2)\\cdot \\text{tl}}{-\\text{ln}(1 - \\frac{\\text{N} - 1}{\\text{N}})} \\\\\n\\text{Maximum} \\text{ t}_{1/2} \\text{ estimate} = \\frac{\\text{ln}(2)\\cdot \\text{tl}}{-\\text{ln}(1 - \\frac{\\text{1} }{\\text{N}})} \\\\\n\\text{N} = \\text{number of reads}\n\\end{gather} \n$$\n\nIn practice, uncertainties in the half-life estimate get larger the\nfurther the true half-life is from the label time (Figure 6).\n\n![**Figure 6**: Estimate accuracy as a function of NTR. EZbakR’s SimulateOneRep() function was used to simulate data for 10,000 features, all of which were given 100, 1000, or 10000 reads. Features had a range of NTR values, evenly spaced from 0.01 to 0.99. NTRs were estimated with EZbakR. Top: comparison of EZbakR NTR estimates to the simulated truth (both on a logit-scale). Bottom: The logit(NTR) uncertainty as a function of NTR. The uncertainty is computed from the Hessian and can thus be thought of as how steeply the likelihood falls off around the maximum likelihood value. Larger values represent slow decreases in likelihood and high uncertainty (lots of logit(NTR) estimates explain the data reasonably well).](Uncertainties.png)\n\nDespite these theoretical arguments, shorter label times are usually\npreferable to longer ones. Long-term exposure to s^4^U can have\ncytotoxic effects, so minimizing this risk is advisable. In addition,\ndropout may be more likely the higher proportion of your RNA that is\nlabeled. Because of this, we suggest a slight deviation from theoretical\nstatistical optimality and propose that an optimal label time close to,\nbut shorter than, the median half-life of the RNA that you are\ninterested in probing.\n\nFinally, the best-case scenario is to have multiple different label\ntimes (e.g., 1 hour, 2 hour, 4 hours) in each biological condition.\nHaving multiple label times expands the dynamic rangeo f your half-life\nestimation. In addition, grandR implements a strategy that uses multiple\nlabel times to correct for biases that can arise due to gradual ramp up\nin s^4^U availability over time. It assumes that the longest label time\nis closest to be the ground-truth \"effective\" label time (this is\nbecause the s^4^U ramp-up time is the smallest percentage of that label\ntime), and identifies adjusted label times for the other time points\nthat yield, on average, estimates in agreement with the longest label\ntime. More generally, having multiple label times is useful for asessing\nthe extent to whcih models assumed in your NR-seq analysis accurately\ndescribe the dynamics of RNAs assayed.\n\n### Pulse-label vs. pulse-chase\n\nThere are two ways to perform the labeling in an NR-seq experiment: a\npulse-label and a pulse-chase (Figure 7). Pulse-labeling refers to\ntreating cells with metabolic label for a certain period of time, after\nwhich RNA is extracted, treated with nucleotide recoding chemistry, and\nsequenced. In a pulse-chase, labeling is typically done for a much\nlonger period of time, followed by a chase with the unmodified\nnucleotide (e.g., uridine). RNA is extracted after the chase.\n\nWhile technically equivalent in terms of the information provided, a\npulse-label design provides a number of advantages over a pulse-chase\ndesign. Pulse-chases necessitate extended exposure of cells to s^4^U.\nThis can have serious cytotoxic effects that risk biasing your analyses.\nIn addition, the metabolic label can be recycled from degraded RNA\nduring the chase and returned to the NTP pool, complicating analysis of\nthis data (this compounds the orthogonal problem of imperfect washout of\nmetabolic label during the chase). Finally, estimating kinetic\nparameters in a pulse-chase requires estimating the NTR after the pulse\nand the chase, and comparing these NTR values. For stable RNA, this can\nyield ambiguous estimates when the estimated NTR after the chase is\nlower than that after the pulse (e.g., due to experiment noise,\nestimation uncertainty, or experimental biases). In contrast,\npulse-label designs can limit a cell's exposure to s^4^U, aren't\nconfounded by label recycling, and can provide estimates of RNA turnover\nkinetics from NTR estimates in a single sample. Because of this, we\nstrongly suggest preferring a pulse-label to a pulse-chase design. This\nsuggestion is further backed up by a recent meta-analysis I performed of\npublished NR-seq datasets, where I found that estimates from pulse-label\nNR-seq experiments were far more consistent across cell lines, labs,\nmethod variations, etc. than those from pulse-chases.\n\n![**Figure 7:** Two NR-seq labeling strategies. Pulse-labeling refers to labeling cells for a certain amount of time, after which RNA is extracted. A pulse-chase refers to labeling cells for (typically) a much longer period of time, chasing with the regulate nucleotide (e.g., uridine), and extracting RNA after the chase. These two designs are symmetric (the unlabeled RNA in one behaves like the labeled RNA in another). Despite this, pulse-labels provide a number of technical advantages to a pulse-chase, and thus should be the default NR-seq labeling strategy (see text for discussion).](Labeling.png)\n\n## NR-seq extensions\n\nNucleotide recoding has not only been used to augment bulk RNA-seq. A\nvariety of other sequencing methods have been combined with metabolic\nlabeling and nucleotide recoding. These approaches are opening up\nexciting avenues for studying various aspects of RNA biology. In this\nsection, I briefly review the universe of current NR-seq extensions.\n\n### TT-NR-seq\n\nOriginally, s^4^U metabolic labeling was combined with biochemical\nenrichment of labeled RNA. While the limitations of this approach\ninspired the development of nucleotide recoding approaches, nucleotide\nrecoding also provides a unique route by which to improve these methods.\nThis is exciting as enrichment-based methods are still useful for\nstudying rapid processes, such as co-transcriptional splicing and\ncleavage and polyadenylation. A major challenge in analyzing these\nenrichment-based metabolic labeling data though is the inevitable\npresence of unlabeled RNA contamination in the enriched RNA population.\nThe magnitude of biases introduced by this contamination is greater in\nsettings where very short label times are required, precisely the\nsettings in which enrichment-based strategies are still needed. Applying\nnucleotide recoding to this data has the potential to allow users to\nbioinformatically filter out unlabeled RNA contamination.\n\nIn theory, standard mixture modeling can be applied to TT-NR-seq data so\nas to estimate the amount of reads from a given feature that come from\nunlabeled RNA contamination. In practice, the extent to which these\nmodels are fully appropriate in this setting is underexplored. Concerns\nabout overdispersion of mutation rate distributions are particularly\nvalid in settings with very short label times, as s^4^U concentrations\nare likely actively increasing during the labeling. Thus, regions of an\nRNA produced early in the labeling are likely exposed to lower\nconcentrations than those produced later in the labeling. This means\nthat instead of there being a single new read mutation rate, there is a\npotentially broad distribution of mutation rates. Strategies to model\nthis distribution could both address this problem and provide\ninteresting orthogonal information about the timing and kinetics of\nvarious processes.\n\n### STL-seq\n\nIn metazoans, transcription initiation at protein coding genes is often\nfollowed by promoter-proximal pausing, where RNA Pol II halts\ntranscription 20-60 nts downstream of the TSS and awaits further signal\nto continue into productive elongation or prematurely terminate. To\nprobe the steady-state levels of paused polymerase, Start-seq was\ndeveloped. This approach involves sequencing short, capped RNAs housed\nwithin the paused polymerase. To assess the kinetics of pause departure\nand initiation, the Simon lab combined this approach with nucleotide\nrecoding, in a method termed Start-TimeLapse-seq, or STL-seq.\n\nSTL-seq has provided unique insights regarding the kinetics of\npromoter-proximal pausing. There are number of analysis challenges and\nlimitations to consider when analyzing STL-seq data though. For one,\nunlike vanilla NR-seq's deconvolution of synthesis and degradation\nkinetics, STL-seq does not completely solve the problem of kinetic\nambiguities plaguing steady-state analyses of pausing. The paused\npolymerase has two distinct potential fates: release into productive\nelongation and premature termination. Thus, the pause site departure\nkinetics estimated by STL-seq represent a combination of the kinetics of\nthese two processes. Fully resolving the kinetics of pausing requires\ncombining STL-seq with inhibition of promoter-proximal pause release via\ndrugs like flavopiridol. This has the downside of being a harsh\nperturbation that can affect the kinetics of the processes being\nstudied, the very same downside that partially inspired the development\nof metabolic labeling methods in the first place.\n\nSTL-seq also presents some unique analysis challenges. For one, STL-seq\nreads are fairly short (\\< 80 nucleotides, as short as 20). Aligning\nthese short reads is made more challenging by the chemically induced\nT-to-C mismatches. 3-base alignment was originally shown to\nsignificantly improve the recovery of high mutation content reads.\nDespite this, rigorous benchmarks of different alignment strategies for\nSTL-seq data have not yet been developed. Drawing inspiration from\nSLAMDUNK's 3'-UTR alignment strategy, we suspect that a strategy using\nno-label data to infer TSS sequences and aligning s^4^U labeled data to\nthis constrained sequence space may provide some advantages. In\naddition, using NextGenMap and its custom T-to-C mismatch penalization\nfunction may provide further advantages.\n\nIn addition, mixture modeling of STL-seq data may suffer from the same\nchallenges faced by mixture modeling of TT-NR-seq data, due to the\nnecessarily short label times (\\~5 minutes). The discusion in that\nsection about the potential of modeling the distribution of new read\nmutation rates hold here as well. In summary, further innovation of the\nprocessing and analysis of STL-seq data may provide significant\ndividends.\n\n### Dual-labeling and TILAC\n\nWhile NR-seq experiments typically make use of s4U, s6G has also shown\nto be compatible with these methods. This has opened the door for\ndual-label designs. While EZbakR and GRAND-SLAM implement strategies to\nfit mixture models to this general class of methods, the potential use\ncases of dual-labeling are currently underexplored.\n\nTo my knowledge, the only existing specialized dual-labeling method\ncurrently published is TILAC. TILAC draws inspiration from the\nproteomics methods SILAC to make use of dual-labeling for rigorous\nnormalization of complex sequencing experiments. TILAC involves mixing\ns^4^U labeled cells with s6G labeled cells. The idea is that these two\npopulations represent distinct biological conditions that you are\ncomparing gene expression levels in. The relative abundances of reads\nfrom s^4^U and s^6^G labeled RNA from a given gene in this experiment\nprovides information about the relative expression of that gene in the\ntwo conditions. This allows for rigorous normalization without the need\nfor spike-ins or statistical assumptions.\n\nTILAC is an exciting paradigm for the normalization of complex\nsequencing experiments. Despite this, it and other potential dual\nlabeling approaches are not without their unique challenges. s6G is more\ncytotoxic than s^4^U, raising the concern of adverse effects due to\nlabeling. In addition, s^6^G may not be readily incorporated into the\nNTP pool to the same extent that s4U is. Finally, for true dual labeling\napproaches (i.e., labeling the same population of RNA with both s^4^U\nand s^6^G), we have found that incorporation of these two labels may\ninterfere with one another. This combined with the unique bioinformatic\nchallenge of aligning reads with high levels of both T-to-C and G-to-A\nconversions makes generating high quality, dual labeled NR-seq data\nchallenging. Despite this, we suspect that strategies to address these\nchallenges could open novel and exciting routes for finer-grained\ndissection of RNA kinetics (e.g., investigation of RNA aging dynamics).\n\n### NascentRibo-seq\n\nApproaches like Ribo-seq are commonly used to infer the efficiency of\ntranslation initiation. Despite this, these analyses are fundamentally\nplagued by kinetic ambiguities that require strong assumptions to\ninterpret in this way. Thus, a method termed NascentRibo-seq was\ndeveloped, which combines nucleotide recoding with Ribo-seq. This\napproach allowed the authors to assess the kinetics of polysome\nassembly. This approach proved particularly useful in settings where\ncells are actively responding to a perturbation, as the active\nregulation of mRNA levels would bias standard Ribo-seq analyses of\nribosome loading efficiency.\n\n### scNR-seq\n\nOne of the most active areas of NR-seq extensions is its application to\nsingle-cell RNA-seq. A number of distinct approaches have been developed\nthat combine scRNA-seq approaches with s^4^U metabolic labeling and\nnucleotide recoding (Erhard et al. 2019a; Hendriks et al. 2019; Cao et\nal. 2020b; Qiu et al. 2020b; Lin et al. 2023; Maizels et al. 2024). This\nincludes work to further optimize the scalability,\nease-of-implementation, and conversion efficiency of these protocols.\n\nIn addition, scNR-seq has seen a number of exciting bioinformatic\ndevelopments. Work from Jonathan Weissman’s lab showed that metabolic\nlabeling scRNA-seq approaches have the potential to significantly\nimprove analyses of RNA velocity. This work also introduced a widely\nused pipeline for processing scNR-seq data, dynast (GRAND-SLAM is the\nonly other existing tool that provides support for single-cell specific\nprocessing tasks). This has spurred several groups to improve upon their\ninitial metabolic labeling-based velocity analyses. This route is\nparticularly promising, as it theoretically eliminates the need to rely\non low and biased coverage for intronic regions of genes in most\nscRNA-seq libraries.\n\nscNR-seq is also opening up brand new classes of analyses. Recently,\nFlorian Erhard’s group developed a combined experimental and\nbioinformatic approach making use of scNR-seq to infer cell\ntype-specific responses to perturbations. The idea is to feed cells the\nmetabolic label at the same time that they are treated with a drug or\nperturbed in some other way. By inferring the degradation kinetics of\nRNA in a given cell (or group of similar cells) after the perturbation,\nthe starting levels of each RNA (i.e., before application of the\nperturbation) can be inferred. This allowed them to identify how each\ncell type present in the starting population uniquely responded to the\ntreatment. Using a non-linear causal inference strategy, they were also\nable to identify gene’s whose regulation were driving the larger changes\nseen.\n\nThis approach, implemented in the new R package HetSeq, showcases how\ninnovative experimental design and data analysis can make use of\nnucleotide recoding to unlock new avenues for biological discovery.\nDespite the theoretical promise of this approach, inferring the starting\nlevels of RNA in these experiments is incredibly challenging. The\nsteady-state assumption is almost inevitably violated. While we have\ndiscussed approaches to achieve unbiased parameter estimation in\nnon-steady-state contexts, HetSeq requires an experimental design\nfundamentally incompatible these strategies (HetSeq estimates the\nquantity that needs to be measured for non-steady-state analyses, the\ninitial RNA levels). In addition, single cell data is notably sparse,\nmaking accurate estimation of cell type specific NTRs (and thus\ndegradation rate constants) difficult. While GRAND-SLAM and HetSeq\nimplement a number of strategies to mitigate these challenges, further\ndenoising of the analysis may be achievable through collection of\nmatched, unperturbed scRNA-seq data. This data could be used to generate\nan atlas of initial cell states that the noisy inferred cell states are\nmapped onto.\n\n### PerturbSci-Kinetics\n\nMany biologists using NR-seq methods are not necessarily interested in\nhow the kinetics of RNA synthesis and degradation change when cells are\nperturbed. Approaches like Perturb-seq have provided an unprecedented,\nhigh-throughput look at the impact of genetic perturbations on gene\nexpression. To dissect how regulation of transcription and RNA decay\nunderlies these changes, Perturb-seq was recently combined with\nnucleotide recoding, in an approach termed PerturbSci-Kinetics. While\nthe method and its bioinformatic toolkit is in its nascence, it\nrepresents a paradigm shift in the scale at which we can investigate the\nmechanisms of gene expression regulation.\n\n### Tether-seq\n\nThe Simon lab recently developed an approach that combines a\nTT-NR-seq-style experiment with single-nucleotide bioinformatic analyses\nto identify sites of small molecule binding across the transcriptome.\nThis approach, termed Tether-seq, relies on treating cells with small\nmolecule drugs containing a disulfide moiety, and using this handle to\ncovalently bind the small molecule to RNAs with which it associates.\nNucleotide recoding is then used to identify the exact sites to which\nthe small molecule tethered. This approach facilitates the\nidentification of weak binders, while also establishing an exciting\nparadigm for single-nucleotide NR-seq analyses.\n\n### Long read NR-seq\n\nCombining nucleotide recoding with long read sequencing technologies\npresents a promising avenue by which to dissect the isoform-specific\nkinetics of gene expression. While it is possible to glean insights\nabout the synthesis and degradation kinetics of individual transcript\nisoforms from short read NR-seq, long read technologies can open up\ninvestigation of isoform-specific dynamics largely invisible to short\nread data (e.g., coupling of TSS and TES usage). There is currently only\none described attempt at long read NR-seq, in which TUC chemistry has\nbeen combined with PacBio long read sequencing.\n\nDespite this, some long read sequencing technologies provide avenues by\nwhich to perform enrichment-free metabolic labeling analyses without\nnucleotide recoding chemistry. In particular, nanopore direct RNA\nsequencing has been used by several groups to directly distinguish\nlabeled from unlabeled RNA. These approaches have utilized a range of\nmachine learning architectures, often relying on convolutional neural\nnetworks, to achieve this end. While distinguishing labels like s4U and\n5-EU in nanopore current signal is challenging, these approaches are\npromising avenues towards high fidelity dissection of isoform-specific\nkinetics. This approach was even recently combined with subcellular\nfractionation to probe the kinetics of more than just bulk RNA synthesis\nand degradation. Thus, long read NR-seq and direct RNA sequencing of\nmetabolically labeled samples will likely be a powerful tool in\ndissecting the dynamics of transcript isoforms.\n\n## Conclusions\n\nNR-seq represents a powerful paradigm shift in the study of RNA\ndynamics. Making full effective use of these methods requires rigorous\nanalysis strategies robust to technical and biological variance as well\nas biases that can plague these experiments. While a number of tools\nhave been developed to implement these strategies, new NR-seq extensions\noften demand unique solutions to these problems. Thus, it is important\nthat users and developers of NR-seq technologies are aware of the dos\nand don’ts of NR-seq data processing and analysis.\n\nIn this review, I have laid out the tenets of a solid NR-seq analysis\nfoundation. This includes rigorous modeling of the mutational data in\nNR-seq reads, awareness regarding biases such as dropout that can plague\nthese data, accurate kinetic modeling of the labeled and unlabeled RNA\nabundances, proper alignment of conversion containing reads, and\nexperimental design decisions intended to make all of the above easier.\nI urge NR-seq users and developers to follow the roadmap laid out here\nso as to avoid misusing and misinterpreting their data.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}